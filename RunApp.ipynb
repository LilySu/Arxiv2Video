{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e445fe3-e5e0-4080-9a78-6570bfae7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip, AudioFileClip, concatenate_videoclips\n",
    "from pymilvus import Milvus, MilvusClient, IndexType, connections, utility\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from pdf2image.exceptions import PDFPageCountError, PDFSyntaxError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from moviepy.editor import concatenate_videoclips, ImageClip\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from moviepy.config import change_settings\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pdf2image import convert_from_path\n",
    "from milvus import default_server\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import feedparser\n",
    "import requests\n",
    "import imageio\n",
    "import base64\n",
    "import pprint\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b35aae-ddeb-4c2b-acbe-4529e441a6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:19531\n"
     ]
    }
   ],
   "source": [
    "change_settings({\"FFMPEG_BINARY\": \"/opt/homebrew/bin/ffmpeg\", \"DYLD_LIBRARY_PATH\":\"/opt/homebrew/bin/convert\"})\n",
    "# Set up a Milvus client\n",
    "default_server.start()\n",
    "host=\"127.0.0.1\"\n",
    "connections.connect(host=host, port=default_server.listen_port)\n",
    "port=default_server.listen_port\n",
    "my_uri = \"http://localhost:\" + str(port)\n",
    "print(my_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8361215-27c0-4617-bba1-d4e41bcd7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_variables():\n",
    "    \"\"\"Fetch all necessary configurations from environment variables.\"\"\"\n",
    "    return {\n",
    "        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),\n",
    "        'ELEVEN_LABS_API_KEY': os.getenv('ELEVEN_LABS_API_KEY')\n",
    "    }\n",
    "\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"The folder '{folder_name}' has been created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{folder_name}' already exists.\")\n",
    "\n",
    "\n",
    "def arxiv_id_from_url(url):\n",
    "    # Extract the arXiv ID from the URL using a regular expression\n",
    "    match = re.search(r'arxiv\\.org/pdf/(\\d+\\.\\d+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def download_and_save_pdf(url, folder_pdfs):\n",
    "    \"\"\"\n",
    "    Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The arXiv.org URL of the paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: ArXiv ID of the downloaded paper if successful, or an error message.\n",
    "    \"\"\"\n",
    "    # Extract arXiv ID from the URL\n",
    "    arxiv_id = arxiv_id_from_url(url)\n",
    "\n",
    "    arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "    pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "    create_folder(pdf_path)\n",
    "\n",
    "    # Check if a valid arXiv ID was extracted\n",
    "    if arxiv_id:\n",
    "        try:\n",
    "            # Make a request to the arXiv API\n",
    "            feed = feedparser.parse(f'http://export.arxiv.org/api/query?id_list={arxiv_id}')\n",
    "\n",
    "            # Check if the response contains entries\n",
    "            if 'entries' in feed:\n",
    "                # Iterate over each entry (paper) in the feed\n",
    "                for entry in feed.entries:\n",
    "                    # Extract the PDF link from the entry\n",
    "                    pdf_link = entry.link.replace('/abs/', '/pdf/') + '.pdf'\n",
    "\n",
    "                    # Download the PDF\n",
    "                    response = requests.get(pdf_link)\n",
    "\n",
    "                    # Save the PDF in the local directory with the name based on the arXiv ID\n",
    "                    with open(f'{pdf_path}/{arxiv_name}.pdf', 'wb') as pdf_file:\n",
    "                        pdf_file.write(response.content)\n",
    "\n",
    "                    print(f\"\\nPDF downloaded and saved as {arxiv_name}.pdf\")\n",
    "                    return arxiv_id\n",
    "\n",
    "            else:\n",
    "                return f\"\\nNo entries found for arXiv ID {arxiv_id}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"\\nError extracting information: {e}\"\n",
    "    else:\n",
    "        return \"Invalid arXiv PDF URL format. Please enter a valid URL.\"\n",
    "\n",
    "\n",
    "def download_and_initialize_embedding_model(model_name=\"WhereIsAI/UAE-Large-V1\", device=None):\n",
    "    \"\"\"\n",
    "    Download and initialize the Sentence Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the Sentence Transformer model to download.\n",
    "    - device (str or torch.device): The device to use for the model (e.g., 'cuda:3' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - encoder (SentenceTransformer): The initialized Sentence Transformer model.\n",
    "    - EMBEDDING_DIM (int): The embedding dimension of the model.\n",
    "    - MAX_SEQ_LENGTH (int): The maximum sequence length.\n",
    "\n",
    "    Example usage:\n",
    "    encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "    \"\"\"\n",
    "    # Initialize torch settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    DEVICE = torch.device(device) if device else torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\ndevice: {DEVICE}\")\n",
    "\n",
    "    # Load the model from the Hugging Face model hub\n",
    "    encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "    print(f\"\\nDatatype of SentenceTransformer encoded object{type(encoder)}\\n\")\n",
    "    print(f\"\\nWhat the encoder object looks like: {encoder}\\n\")\n",
    "\n",
    "    # Get the model parameters and save for later\n",
    "    EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "    try:\n",
    "        MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_MAX_SEQ_LENGTH()\n",
    "    except AttributeError:\n",
    "        MAX_SEQ_LENGTH_IN_TOKENS = 512\n",
    "    # Assume tokens are 3 characters long\n",
    "    # MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "    # HF_EOS_TOKEN_LENGTH = 1 * 3\n",
    "    # Test with 512 sequence length\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "    HF_EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "    # Inspect model parameters\n",
    "    print(f\"\\nmodel_name: {model_name}\")\n",
    "    print(f\"\\nEMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "    print(f\"\\nMAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "    return encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH\n",
    "\n",
    "\n",
    "def create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, uri=my_uri):\n",
    "    \"\"\"\n",
    "    Create a no-schema Milvus collection and define the database index.\n",
    "\n",
    "    Parameters:\n",
    "    - uri (str): The URI of the Milvus server.\n",
    "    - COLLECTION_NAME (str): The name of the Milvus collection.\n",
    "    - EMBEDDING_DIM (int): The dimension of the embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "    - milvus_client (Milvus): The Milvus client instance.\n",
    "\n",
    "\n",
    "    Example usage:\n",
    "    my_uri = \"tcp://127.0.0.1:19530\"\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    my_EMBEDDING_DIM = 1024\n",
    "    \n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, uri=my_uri)\n",
    "    \"\"\"\n",
    "\n",
    "    # For vector similarity search applications that require perfect accuracy and depend \n",
    "    # on relatively small (million-scale) datasets, the FLAT index is a good choice. \n",
    "    # FLAT does not compress vectors, and is the only index that can guarantee exact \n",
    "    # search results. Results from FLAT can also be used as a point of comparison for \n",
    "    # results produced by other indexes that have less than 100% recall.\n",
    "\n",
    "    # FLAT is accurate because it takes an exhaustive approach to search, which means for \n",
    "    # each query the target input is compared to every set of vectors in a dataset. This \n",
    "    # makes FLAT the slowest index on our list, and poorly suited for querying massive\n",
    "    # vector data. There are no parameters required for the FLAT index in Milvus, and \n",
    "    # using it does not need data training.\n",
    "\n",
    "    index_params = {\n",
    "        \"index_type\": IndexType.FLAT,\n",
    "        \"metric_type\": \"COSINE\",\n",
    "    }\n",
    "\n",
    "    # Use no-schema Milvus client using flexible json key:value format.\n",
    "    milvus_client = MilvusClient(uri=my_uri)\n",
    "\n",
    "    # Check if collection already exists, if so drop it.\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        print(f\"\\nCollection had previously been created, dropping previous collection to initialize anew: `{COLLECTION_NAME}`\")\n",
    "\n",
    "    # Create the collection.\n",
    "    milvus_client.create_collection(COLLECTION_NAME, EMBEDDING_DIM,\n",
    "                                    consistency_level=\"Eventually\",\n",
    "                                    auto_id=True,\n",
    "                                    overwrite=True,\n",
    "                                    params=index_params)\n",
    "\n",
    "    print(f\"\\nSuccessfully created collection: `{COLLECTION_NAME}`\")\n",
    "    print(milvus_client.describe_collection(COLLECTION_NAME))\n",
    "\n",
    "    return milvus_client\n",
    "\n",
    "\n",
    "def split_documents_to_chunks(docs, MAX_SEQ_LENGTH, hf_eos_token_length):\n",
    "    \"\"\"\n",
    "    Split documents into smaller recursive chunks using Sentence Transformers' RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be split.\n",
    "    - MAX_SEQ_LENGTH (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list): List of chunks.\n",
    "\n",
    "    Example usage:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    docs = [\"Document 1 text.\", \"Document 2 text.\", \"Document 3 text.\"]\n",
    "    \n",
    "    resulting_chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    print(resulting_chunks)\n",
    "    \"\"\"\n",
    "    # Calculate chunk size and overlap\n",
    "    chunk_size = MAX_SEQ_LENGTH - hf_eos_token_length\n",
    "    chunk_overlap = int(round(chunk_size * 0.10, 0))\n",
    "\n",
    "    # Create an instance of the RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Split the documents further into smaller, recursive chunks.\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, hf_eos_token_length):\n",
    "    \"\"\"\n",
    "    Insert document chunks into a Milvus collection.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be inserted.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - encoder (SentenceTransformer): SentenceTransformer model for generating embeddings.\n",
    "    - milvus_client (Milvus): Milvus client instance.\n",
    "    - MAX_SEQ_LENGTH (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - insert_time (float): Time taken for the insertion process.\n",
    "\n",
    "    Example Usage assuming 'chunks' is a list of dictionaries with 'page_content' and 'metadata' keys:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    ENCODER_MODEL_NAME = \"WhereIsAI/UAE-Large-V1\"\n",
    "    # Initialize Milvus client\n",
    "    # Initialize encoder model\n",
    "    \n",
    "    resulting_insert_time = insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \"\"\"\n",
    "    # Convert chunks to a list of dictionaries.\n",
    "    chunk_list = []\n",
    "    for chunk in chunks:\n",
    "    \n",
    "        # Generate embeddings using encoder from HuggingFace.\n",
    "        embeddings = torch.tensor(encoder.encode([chunk.page_content]))\n",
    "        # embeddings = F.normalize(embeddings, p=2, dim=1) #use torch\n",
    "        embeddings = np.array(embeddings / np.linalg.norm(embeddings)) #use numpy\n",
    "        converted_values = list(map(np.float32, embeddings))[0]\n",
    "        \n",
    "        # Assemble embedding vector, original text chunk, metadata.\n",
    "        chunk_dict = {\n",
    "            'vector': converted_values,\n",
    "            'chunk': chunk.page_content,\n",
    "            'source': chunk.metadata['page']\n",
    "        }\n",
    "        chunk_list.append(chunk_dict)\n",
    "\n",
    "    # Insert data into the Milvus collection.\n",
    "    print(\"Start inserting entities\")\n",
    "\n",
    "    inserted_chunks = milvus_client.insert(\n",
    "        COLLECTION_NAME,\n",
    "        data=chunk_list,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    print(\"Finished inserting entities\")\n",
    "\n",
    "    # After the final entity is inserted, call flush to stop growing segments left in memory.\n",
    "    milvus_client.flush(COLLECTION_NAME)\n",
    "\n",
    "    return print(f\"\\nNumber of chunks inserted into Milvus database: {len(inserted_chunks)} with chunk id starting at number: {inserted_chunks[0]}\\n\")\n",
    "\n",
    "\n",
    "def client_assemble_retrieved_context(retrieved_top_k, metadata_fields=[], num_shot_answers=3):\n",
    "    \"\"\" \n",
    "    For each question, assemble the context and metadata from the retrieved_top_k chunks.\n",
    "    retrieved_top_k: list of dicts\n",
    "\n",
    "    Example Usage:\n",
    "    formatted_results, context, context_metadata = client_assemble_retrieved_context(results, metadata_fields=metadata_fields, num_shot_answers=top_k)\n",
    "    \"\"\"\n",
    "    # Assemble the context as a stuffed string.\n",
    "    distances = []\n",
    "    context = []\n",
    "    context_metadata = []\n",
    "    i = 1\n",
    "    for r in retrieved_top_k[0]:\n",
    "        distances.append(r['distance'])\n",
    "        if i <= num_shot_answers:\n",
    "            if len(metadata_fields) > 0:\n",
    "                metadata = {}\n",
    "                for field in metadata_fields:\n",
    "                    metadata[field] = r['entity'][field]\n",
    "                context_metadata.append(metadata)\n",
    "            context.append(r['entity']['chunk'])\n",
    "        i += 1\n",
    "\n",
    "    # Assemble formatted results in a zipped list.\n",
    "    formatted_results = list(zip(distances, context, context_metadata))\n",
    "    # Return all the things for convenience.\n",
    "    return formatted_results, context, context_metadata\n",
    "    \n",
    "\n",
    "def search_and_generate_response(docs, LLM_NAME, key):\n",
    "    \"\"\"\n",
    "    Extracts text before the \"Abstract\" section from a document, then generates various responses and edits related to the academic paper based on the extracted text.\n",
    "\n",
    "    Args:\n",
    "    - docs (list): Lanchain document object, the first one of the object grabbed for page title of academic paper.\n",
    "    - LLM_NAME (str): Name of the OpenAI language model to be used.\n",
    "    - key (dict): A dictionary containing the OpenAI API key.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the generated responses and edits related to the academic paper.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract text before the \"Abstract\" section therefore always grabbing paper title, authors and universities for search and reference.\n",
    "    text_before_abstract = re.search(r'^(.*?)\\bAbstract\\b', docs[0].page_content, re.DOTALL)\n",
    "\n",
    "    dish_name = text_before_abstract.group(1)\n",
    "\n",
    "    # Initialize OpenAI language model\n",
    "    llm = ChatOpenAI(model_name=LLM_NAME, temperature=0, openai_api_key=key['OPENAI_API_KEY']) \n",
    "\n",
    "    # This is an LLMChain to define prompt templates for different stages\n",
    "    prompt_outline = PromptTemplate.from_template(\n",
    "        template= \"\"\"\n",
    "        - Use only the the paper {dish_name} to answer the following: \n",
    "        - Pick the top 5 most occurring terminology in {dish_name} and define the terms.\n",
    "        - Write 2 sentences to give the definition of each of the 5 most occurring terminology in {dish_name}, for a total of 10 sentences.\n",
    "        - Answer what are the key contributions of the paper {dish_name} in one sentence?\n",
    "        - Answer what are the evluation metrics or new approaches proposed in {dish_name}? \n",
    "    \"\"\")\n",
    "    # This is an LLMChain to define prompt templates for different stages\n",
    "    prompt_summary = PromptTemplate.from_template(\n",
    "        template=\"\"\" Assume the role of a NeurIPS Paper Evaluation Committee Member who evaluates Use only is evluating the {dish_name} paper based on various criteria such as novelty, significance, and technical soundness to pitch to a stakeholder.\n",
    "        - The output text should more than 1000 characters:\n",
    "        - Write 3 sentences to explain in detail the key contributions answered in {outline} of the paper {dish_name}.\n",
    "        - Expand upon the evluation metrics or new approaches from {outline} by summarizing each and every paragraph in {dish_name} into one sentence. If the paper has 20 paragraphs, there this prompt needs to output 20 sentences.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # This is an LLMChain to define prompt templates for different stages\n",
    "    prompt_edit = PromptTemplate.from_template(\n",
    "        template=\"\"\"Edit the entire script {summary} for good flow, professionalism, redundancy removal in a voiceover text format.\n",
    "        - Assume the role of a NeurIPS Paper Evaluation Committee Member who evaluates Use only is evluating the {dish_name} paper based on various criteria such as novelty, significance, and technical soundness to pitch to a stakeholder.\n",
    "        - It is of utmost importance to be as specific as possible, going into the technical details, using technical terminology \n",
    "        - Make the output longer than 2500 characters.\n",
    "        - Keep the detail and specificity from {summary} where {dish_name} where every paragraph was summarized into one sentence\n",
    "        - Make sure that it is obviously stated to the video viewer the related work that the paper is built on, use the terminology and definitions from the {summary} or {outline}.\n",
    "        - In the edit, ensure that the voiceover text states the following specifics, if not, go back to the {dish_name} paper to extract more specific details:\n",
    "        - what research was previously known in the past that the paper is based upon that the paper advances previously known information, what are the technologies that were previously known involved\n",
    "        - specific methods used,\n",
    "        - how is the technique, actions or methods performed advanced the field, or challenges the status-quo\n",
    "        - what are the major contributions, spend 3 sentences to elaborate the details, using terminology from {outline} \n",
    "        - What are the key metrics that define the success of the work\n",
    "        - make sure to always reference the paper {dish_name}  what are the technologies that were previously known involved\n",
    "        - for each of the technologies involved, if the technologies are from the 5 most occurring terminology in {outline}, edit that part of the {outline} into a complete sentence without further edits.\n",
    "        - for each of the technologies involved, further elaborate the technology by citing how the {dish_name} has introduced these technologies by looking at the terminology in the paper and summarizing the words before and after. \n",
    "        - what are future directions that lie ahead. Cite the grounding sources. \n",
    "        - Always start by stating the title and authors of the paper as the first few words.\n",
    "        - Make sure the output is at least 2500 characters or tokens in length, if not reference the paper {dish_name} to answer the previous points more in depth\n",
    "        \"\"\"\n",
    "    )\n",
    "    # This is an LLMChain to define prompt templates for different stages\n",
    "    prompt_metrics = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "        - What are the technologies that were previously known involved from the paper {dish_name}?\n",
    "        - If you cannot find the paper {dish_name} and are going to output Unfortunately, as an AI language model, I do not have access to specific papers or their content\n",
    "        - Do not say you do not have access, keep trying to reference the paper {dish_name}. \n",
    "        - For each of the technologies involved, if the technologies are from the 5 most occurring terminology in {outline}, edit that part of the {outline} into a complete sentence without further edits. Further elaborate the technology by citing how the {dish_name} has introduced these technologies by looking at the terminology in the paper and summarizing the words before and after. \n",
    "        - What are the key metrics that define the success of the work from the paper? Name specific evlauation metrics used in the research?\n",
    "        - What was the datset was used?\n",
    "        - How was the research structured?\n",
    "        - Describe the results and what metrics the paper {dish_name} claims to have beaten? What scores does the paper claim to have gotten compared to previous work?\n",
    "        - List the previous methods that the paper beat. \n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create LLM chains for different tasks\n",
    "    chain_outline = LLMChain(llm=llm, prompt=prompt_outline, output_key=\"outline\")\n",
    "    chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "    chain_edit = LLMChain(llm=llm, prompt=prompt_edit, output_key=\"edit\")\n",
    "    chain_metrics = LLMChain(llm=llm, prompt=prompt_metrics, output_key=\"metrics\")\n",
    "\n",
    "    # Assemble overall chain\n",
    "    overall_chain = SequentialChain(\n",
    "        chains=[chain_outline, chain_summary, chain_edit, chain_metrics],\n",
    "        input_variables=[\"dish_name\"],\n",
    "        output_variables=[\"outline\",\"summary\", \"edit\", \"metrics\"],\n",
    "    )\n",
    "    # Execute overall chain with input data    \n",
    "    result = overall_chain({\"dish_name\": text_before_abstract})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_transcript(response_choices, folder_transcripts, arxiv_name):\n",
    "    \"\"\"\n",
    "    Save the first element of response_choices into a text file in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - response_choices (list): A list of choices where the first element is the transcript content.\n",
    "    - folder_transcripts (str): The directory path where the transcript file will be saved.\n",
    "    - arxiv_name (str): The name used for generating the transcript file.\n",
    "\n",
    "    Returns:\n",
    "    None: The function saves the transcript content to a text file.\n",
    "\n",
    "    Example:\n",
    "    save_transcript([\"This is the transcript content.\"], \"transcripts\", \"example_arxiv\")\n",
    "    \"\"\"\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(folder_transcripts):\n",
    "        os.makedirs(folder_transcripts)\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(folder_transcripts, f\"{arxiv_name}.txt\")\n",
    "\n",
    "    # Save response_choices[0] to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(response_choices[0])\n",
    "\n",
    "    print(f\"\\nTranscript saved in: {file_path}\")\n",
    "\n",
    "\n",
    "def text_to_speech(text_for_TTS, arxiv_name, folder_audio):\n",
    "\n",
    "    ELEVEN_LABS_API_KEY = os.environ.get(\"ELEVEN_LABS_API_KEY\")\n",
    "\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/bVMeCyTHy58xNoL34h3p\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVEN_LABS_API_KEY\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"text\": text_for_TTS,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Generate a unique filename based on timestamp\n",
    "    filename = f\"output_{arxiv_name}.mp3\"\n",
    "    target_path = os.path.join(folder_audio, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"Recording file {filename} already exists in {folder_audio}. Skipping download.\")\n",
    "        return target_path\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Save the recording to the unique file\n",
    "        with open(target_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"\\nRecording saved in {target_path}\")\n",
    "        return target_path\n",
    "    else:\n",
    "        print(f\"\\n Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "def convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name):\n",
    "    try:\n",
    "        # Create a folder for storing the PNGs\n",
    "        sub_folder_name = os.path.splitext(os.path.basename(pdf_file_path))[0] + \"_pngs\"\n",
    "        full_path = os.path.join(folder_images, sub_folder_name)\n",
    "        if not os.path.exists(full_path):\n",
    "            os.makedirs(full_path)\n",
    "    \n",
    "        # Convert each page of the PDF to PNG\n",
    "        images = convert_from_path(pdf_file_path, output_folder=full_path)\n",
    "        # arxiv_name = sub_folder_name.replace(\"_pngs\", \"\")\n",
    "    \n",
    "        # Save each image as a separate PNG file\n",
    "        for i, image in enumerate(images):\n",
    "            png_path = os.path.join(full_path, f\"{arxiv_name}_page_{i + 1}.png\")\n",
    "            image.save(png_path, \"PNG\")\n",
    "    \n",
    "        print(f\"\\nAll pages converted and saved in the folder: {full_path}\")\n",
    "    \n",
    "        # Clean up: Delete the .ppm files and uncropped files\n",
    "        for filename in os.listdir(full_path):\n",
    "            if filename.endswith(\".ppm\"):\n",
    "                file_to_remove_path = os.path.join(full_path, filename)\n",
    "                os.remove(file_to_remove_path)\n",
    "    \n",
    "        print(f\"\\n.ppm artifacts deleted in the folder: {full_path}\")\n",
    "    except (PDFPageCountError, PDFSyntaxError, PermissionError) as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(f\"Skipping processing of {pdf_file_path}\")\n",
    "        if isinstance(e, PdfReadError):\n",
    "            print(\"PdfReadError: Unable to read PDF file.\")\n",
    "        elif isinstance(e, PermissionError):\n",
    "            print(\"PermissionError: Permission issue while processing the PDF file.\")\n",
    "\n",
    "\n",
    "def cut_pngs_in_half(image_folder):\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\"\\nError: Directory '{image_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for file_name in files:\n",
    "        # Check if the file is a PNG and does not contain 'cropped' in the name\n",
    "        if file_name.lower().endswith('.png') and 'cropped' not in file_name.lower():\n",
    "            image_path = os.path.join(image_folder, file_name)\n",
    "\n",
    "            # Open the image\n",
    "            with Image.open(image_path) as img:\n",
    "                # Get the dimensions of the image\n",
    "                width, height = img.size\n",
    "\n",
    "                # Cut the image in half (top and bottom)\n",
    "                top_half = img.crop((0, 0, width, height // 2))\n",
    "                bottom_half = img.crop((0, height // 2, width, height))\n",
    "\n",
    "                # Save the top and bottom halves with \"_cropped_1\" and \"_cropped_2\" suffixes\n",
    "                top_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_1.png\"), 'PNG')\n",
    "                bottom_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_2.png\"), 'PNG')\n",
    "\n",
    "                print(f\"\\nImages saved: {file_name}_cropped_1.png (top) and {file_name}_cropped_2.png (bottom)\")\n",
    "        else:\n",
    "            print(f\"\\nSkipping processing for {file_name} as it contains 'cropped' in the file name.\")\n",
    "\n",
    "\n",
    "def analyze_mp3_length(mp3_path):\n",
    "    audio = AudioSegment.from_file(mp3_path)\n",
    "    return len(audio) / 1000.0  # Length in seconds\n",
    "\n",
    "def fetch_cropped_images(image_folder):\n",
    "    # List all images in the folder\n",
    "    all_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Identify files to keep (those with the word \"cropped\" in their filenames)\n",
    "    cropped_images = [image for image in all_images if image.lower().endswith('.png') and 'cropped' in image.lower()]\n",
    "    \n",
    "    # Delete files that do not contain the word \"cropped\"\n",
    "    for image in all_images:\n",
    "        if image not in cropped_images:\n",
    "            os.remove(os.path.join(image_folder, image))\n",
    "    \n",
    "    # List the remaining images after deletion\n",
    "    remaining_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Sort the cropped images based on numeric values in their filenames\n",
    "    sorted_images = sorted(remaining_images, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    return sorted_images\n",
    "\n",
    "\n",
    "def move_uncropped_files(image_folder):\n",
    "    try:\n",
    "        # Create a new folder if it doesn't exist\n",
    "        uncropped_folder = os.path.join(image_folder, \"uncropped_pngs\")\n",
    "        if not os.path.exists(uncropped_folder):\n",
    "            os.makedirs(uncropped_folder)\n",
    "\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            file_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            # Check if the file name contains the word \"cropped\"\n",
    "            if \"cropped\" not in filename:\n",
    "                # Move the file to the uncropped folder\n",
    "                new_path = os.path.join(uncropped_folder, filename)\n",
    "\n",
    "                try:\n",
    "                    shutil.move(file_path, new_path)\n",
    "                    print(f\"File moved to uncropped folder: {filename}\")\n",
    "                except Exception as move_error:\n",
    "                    print(f\"Error moving file {filename}: {move_error}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"All non-cropped files moved to the folder: {uncropped_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def create_video(mp3_path, image_folder, output_path):\n",
    "    try:\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            file_path = os.path.join(image_folder, filename)\n",
    "            \n",
    "            # Check if the file name contains the word \"cropped\"\n",
    "            if \"cropped\" not in filename:\n",
    "                # Remove the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"File removed: {filename}\")\n",
    "                \n",
    "        print(f\"All non-cropped files removed in the folder: {image_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Sort the images by converting the numeric parts of filenames into integers\n",
    "    image_files = sorted([file for file in os.listdir(image_folder) if 'cropped' in file and file.lower().endswith('.png')],\n",
    "                         key=lambda x: [int(part) if part.isdigit() else part for part in re.split(r'(\\d+)', x)])\n",
    "    audio_clip = AudioFileClip(mp3_path)\n",
    "    \n",
    "    # Calculate the duration of each image based on the total duration of the audio and the number of images\n",
    "    image_duration = audio_clip.duration / len(image_files)\n",
    "    \n",
    "    clips = []\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        # Load each image using imageio\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = imageio.imread(image_path)\n",
    "    \n",
    "        if image.sum() == 0:\n",
    "            print(f\"Image {image_file} is blank. Skipping...\")\n",
    "            os.remove(image_file)\n",
    "            continue\n",
    "                \n",
    "        # Create a clip from the image and set its duration\n",
    "        image_clip = ImageClip(image).set_duration(image_duration)\n",
    "    \n",
    "        # Add the image clip to the list of clips\n",
    "        clips.append(image_clip)\n",
    "    \n",
    "    # Concatenate the image clips to create the final video\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_clip = final_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video with audio\n",
    "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24, verbose=True)\n",
    "    print(f\"\\nFinal video saved at: {output_path}.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c6321b-abd9-447c-b48e-bdc9047f02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(url):\n",
    "    \n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    LLM_NAME = \"gpt-3.5-turbo\"\n",
    "    \n",
    "    folder_pdfs = \"pdfs\"\n",
    "    folder_images = \"images\"\n",
    "    folder_final_videos = \"final_videos\"\n",
    "    folder_audio = \"audio_voiceovers\"\n",
    "    folder_transcripts = \"transcripts\"\n",
    "    \n",
    "    # Call the function to create the folder\n",
    "    create_folder(folder_pdfs)\n",
    "    create_folder(folder_images)\n",
    "    create_folder(folder_audio)\n",
    "    create_folder(folder_final_videos)\n",
    "    create_folder(folder_transcripts)\n",
    "    \n",
    "    # Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "    arxiv_id = download_and_save_pdf(url, folder_pdfs)\n",
    "    \n",
    "    arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "    pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "    pdf_file_path = os.path.join(pdf_path, f\"{arxiv_name}.pdf\")\n",
    "    image_folder = f\"{folder_images}/{arxiv_name}_pngs\" \n",
    "    mp3_path = f\"{folder_audio}/output_{arxiv_name}.mp3\"\n",
    "    output_path = f\"{folder_final_videos}/{arxiv_name}.mp4\" \n",
    "    \n",
    "    # Download open source embedding model \"WhereIsAI/UAE-Large-V1\" via Huggingface's Sentence Transformers\n",
    "    encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "    \n",
    "    # Create a no-schema milvus collection and define the database index\n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, my_uri)\n",
    "    \n",
    "    # Load PDF's into a PDF object using LangChain's PyPDFLoader\n",
    "    loader = PyPDFLoader(f\"{pdf_path}/{arxiv_name}.pdf\")\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Cut text from PDF's into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "    chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \n",
    "    # Insert text chunks into Milvus vector database using index type HNSW Indexing and Cosine Distance\n",
    "    insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "\n",
    "        \n",
    "    load_dotenv()\n",
    "    key = get_env_variables()\n",
    "    \n",
    "    # Generate transcript using OpenAI based on the cosine distance search of the document then using gpt-3.5-turbo's chat completions\n",
    "    text_for_TTS_dict = search_and_generate_response(docs, LLM_NAME, key)\n",
    "    \n",
    "    # Save LLM-generated voiceover script to directory\n",
    "    save_transcript(text_for_TTS_dict, folder_transcripts, arxiv_name)\n",
    "    \n",
    "    # convert text to speech with Elevenlabs\n",
    "    audio_path = text_to_speech(text_for_TTS_list[0], arxiv_name, folder_audio)\n",
    "    \n",
    "    # convert each pdf to a png\n",
    "    convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name)\n",
    "    \n",
    "    # cut png's in half\n",
    "    cut_pngs_in_half(image_folder)\n",
    "\n",
    "    move_uncropped_files(image_folder)\n",
    "    \n",
    "    # combine png's with audio to generate an mp4\n",
    "    create_video(mp3_path, image_folder, output_path)\n",
    "    milvus_client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "    return folder_final_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b87f57-84ff-43ba-ae44-7b887466b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = [\"2402.13254\", \"2403.07874\"] # \"2403.07872\", \"2403.07870\",\"2403.07869\"]\n",
    "# paper_list = [\"2403.07867\",\"2308.08079\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d75ab254-dc96-47fe-82cc-5533180a82ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs' already exists.\n",
      "The folder 'images' already exists.\n",
      "The folder 'audio_voiceovers' already exists.\n",
      "The folder 'final_videos' already exists.\n",
      "The folder 'transcripts' already exists.\n",
      "The folder 'pdfs/2402_13254' already exists.\n",
      "\n",
      "PDF downloaded and saved as 2402_13254.pdf\n",
      "\n",
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name WhereIsAI/UAE-Large-V1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datatype of SentenceTransformer encoded object<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "\n",
      "\n",
      "What the encoder object looks like: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "\n",
      "\n",
      "model_name: WhereIsAI/UAE-Large-V1\n",
      "\n",
      "EMBEDDING_DIM: 1024\n",
      "\n",
      "MAX_SEQ_LENGTH: 512\n",
      "\n",
      "Collection had previously been created, dropping previous collection to initialize anew: `MilvusDocs`\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 1024}, 'element_type': 0}], 'aliases': [], 'collection_id': 448390687018071175, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "Start inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 27.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of chunks inserted into Milvus database: 122 with chunk id starting at number: 448390687017670728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_for_TTS_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m paper_list:\n\u001b[1;32m      3\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://arxiv.org/pdf/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mprocess_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36mprocess_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     51\u001b[0m text_for_TTS_dict \u001b[38;5;241m=\u001b[39m search_and_generate_response(docs, LLM_NAME, key)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Save LLM-generated voiceover script to directory\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m save_transcript(\u001b[43mtext_for_TTS_list\u001b[49m, folder_transcripts, arxiv_name)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# convert text to speech with Elevenlabs\u001b[39;00m\n\u001b[1;32m     57\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m text_to_speech(text_for_TTS_list[\u001b[38;5;241m0\u001b[39m], arxiv_name, folder_audio)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_for_TTS_list' is not defined"
     ]
    }
   ],
   "source": [
    "for paper in paper_list:\n",
    "\n",
    "    url = f\"https://arxiv.org/pdf/{paper}.pdf\"\n",
    "    process_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d92e0-24b3-4dd3-834b-cf42ef7396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_url,\n",
    "    inputs=gr.Textbox(placeholder=\"Enter arXiv PDF URL\"),\n",
    "    outputs=gr.Video(),\n",
    "    live=True,\n",
    "    theme=\"sky\",\n",
    "    flagging_options=None,  # Disable the flag button\n",
    "    title=\"Arxiv2Video\",\n",
    ")\n",
    "\n",
    "# Add a submit button\n",
    "submit_button = gr.Button()\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23e133-4b96-4ddb-ab6f-b50851a71a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7b5e5-f046-403f-970e-5d1bb9e99ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c18507a8-242a-4f68-b551-a7212c325155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs' already exists.\n",
      "The folder 'images' already exists.\n",
      "The folder 'audio_voiceovers' already exists.\n",
      "The folder 'final_videos' already exists.\n",
      "The folder 'transcripts' already exists.\n",
      "The folder 'pdfs/2402_13254' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name WhereIsAI/UAE-Large-V1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PDF downloaded and saved as 2402_13254.pdf\n",
      "\n",
      "device: cpu\n",
      "\n",
      "Datatype of SentenceTransformer encoded object<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "\n",
      "\n",
      "What the encoder object looks like: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "\n",
      "\n",
      "model_name: WhereIsAI/UAE-Large-V1\n",
      "\n",
      "EMBEDDING_DIM: 1024\n",
      "\n",
      "MAX_SEQ_LENGTH: 512\n",
      "\n",
      "Collection had previously been created, dropping previous collection to initialize anew: `MilvusDocs`\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 1024}, 'element_type': 0}], 'aliases': [], 'collection_id': 448390687018072209, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "Start inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 27.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of chunks inserted into Milvus database: 122 with chunk id starting at number: 448390687017670974\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The paper \"CounterCurate: Enhancing Physical and Semantic Visibility of Cultural Heritage Artifacts\" by [Authors] introduces a novel approach to improving the visibility and understanding of cultural heritage artifacts through the concept of CounterCurate.\\n\\nUnfortunately, as an AI language model, I do not have access to specific papers or their content.. It builds upon existing research on physical and semantic visibility, emphasizing the importance of making cultural heritage accessible and meaningful to the public. The paper advances the field by proposing the use of user engagement metrics, such as visitor feedback and interaction data, to evaluate the effectiveness of CounterCurate in enhancing both physical and semantic qualities of cultural heritage artifacts. \\n\\nIn terms of technology, CounterCurate incorporates digital tools and technology to enhance the visibility of cultural heritage objects. It leverages user engagement metrics to measure the impact of these enhancements and improve the overall experience for viewers. The paper also explores the potential collaborations between cultural heritage experts, technologists, and community members in implementing CounterCurate, highlighting the interdisciplinary nature of the approach.\\n\\nMoving forward, future directions for research could involve further exploring the scalability of CounterCurate across different cultural heritage contexts and settings. By continuing to evaluate the long-term impact of CounterCurate on cultural heritage preservation, researchers can gain valuable insights into the effectiveness of the approach. Additionally, the role of storytelling and narrative in enhancing the semantic visibility of cultural heritage artifacts could be further investigated to deepen the connection between viewers and cultural heritage objects. \\n\\nIn conclusion, the paper \"CounterCurate: Enhancing Physical and Semantic Visibility of Cultural Heritage Artifacts\" presents a significant contribution to the field by introducing a new approach to enhancing the visibility and understanding of cultural heritage artifacts. By incorporating user engagement metrics and technology, CounterCurate offers a promising method for preserving and promoting cultural heritage for future generations.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f\"https://arxiv.org/pdf/2402.13254.pdf\"\n",
    "\n",
    "key = get_env_variables()\n",
    "\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "HF_EOS_TOKEN_LENGTH = 3\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "folder_pdfs = \"pdfs\"\n",
    "folder_images = \"images\"\n",
    "folder_final_videos = \"final_videos\"\n",
    "folder_audio = \"audio_voiceovers\"\n",
    "folder_transcripts = \"transcripts\"\n",
    "\n",
    "# Call the function to create the folder\n",
    "create_folder(folder_pdfs)\n",
    "create_folder(folder_images)\n",
    "create_folder(folder_audio)\n",
    "create_folder(folder_final_videos)\n",
    "create_folder(folder_transcripts)\n",
    "\n",
    "# Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "arxiv_id = download_and_save_pdf(url, folder_pdfs)\n",
    "\n",
    "arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "pdf_file_path = os.path.join(pdf_path, f\"{arxiv_name}.pdf\")\n",
    "image_folder = f\"{folder_images}/{arxiv_name}_pngs\" \n",
    "mp3_path = f\"{folder_audio}/output_{arxiv_name}.mp3\"\n",
    "output_path = f\"{folder_final_videos}/{arxiv_name}.mp4\" \n",
    "\n",
    "# Download open source embedding model \"WhereIsAI/UAE-Large-V1\" via Huggingface's Sentence Transformers\n",
    "encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "\n",
    "# Create a no-schema milvus collection and define the database index\n",
    "milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, my_uri)\n",
    "\n",
    "# Load PDF's into a PDF object using LangChain's PyPDFLoader\n",
    "loader = PyPDFLoader(f\"{pdf_path}/{arxiv_name}.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Cut text from PDF's into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "\n",
    "# Insert text chunks into Milvus vector database using index type HNSW Indexing and Cosine Distance\n",
    "insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "\n",
    "# # Generate transcript using OpenAI based on the cosine distance search of the document then using gpt-3.5-turbo's chat completions\n",
    "result = search_and_generate_response(docs, LLM_NAME, key)\n",
    "\n",
    "# Split the first sentence of the edit response\n",
    "edit_sentences = result['edit'].split('. ')\n",
    "first_sentence_edit = edit_sentences[0]\n",
    "\n",
    "# Insert the metrics information into the first sentence\n",
    "first_sentence_edit += \".\\n\\n\" + result['metrics']\n",
    "\n",
    "# Update the edit response with the modified first sentence\n",
    "transcript = first_sentence_edit + '. ' + '. '.join(edit_sentences[1:])\n",
    "transcript\n",
    "\n",
    "\n",
    "\n",
    "# text_for_TTS_list = search_and_generate_response(\n",
    "#     milvus_client, # Running session via docker container on port http://localhost:19531\n",
    "#     encoder, # Sentence Transformer WhereIsAI/UAE-Large-V1\n",
    "#     COLLECTION_NAME, # MilvusDocs by default\n",
    "#     LLM_NAME,\n",
    "#     TEMPERATURE,\n",
    "#     RANDOM_SEED,\n",
    "#     M)\n",
    "\n",
    "# # Save LLM-generated voiceover script to directory\n",
    "# save_transcript(text_for_TTS_list, folder_transcripts, arxiv_name)\n",
    "\n",
    "# # convert text to speech with Elevenlabs\n",
    "# audio_path = text_to_speech(text_for_TTS_list[0], arxiv_name, folder_audio)\n",
    "\n",
    "# # convert each pdf to a png\n",
    "# convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name)\n",
    "\n",
    "# # cut png's in half\n",
    "# cut_pngs_in_half(image_folder)\n",
    "\n",
    "# move_uncropped_files(image_folder)\n",
    "\n",
    "# # combine png's with audio to generate an mp4\n",
    "# create_video(mp3_path, image_folder, output_path)\n",
    "# milvus_client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "# return folder_final_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc27d40e-d654-4d7f-88cd-be9507d2d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2212d9f1-f313-49c9-8eb8-f3d61993a3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae Lee1\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\n1University of Wisconsin–Madison2Microsoft Research\\nhttps://countercurate.github.io\\n',\n",
       " 'outline': \"Top 8 most occurring terminology in the paper:\\n\\n1. Counterfactual Examples: Examples that are slightly modified from the original data to test the model's robustness and reasoning capabilities.\\n2. Visio-Linguistic Compositional Reasoning: The ability to reason about visual and linguistic information in a compositional manner.\\n3. Physical Reasoning: The ability to understand and reason about physical properties and interactions in the environment.\\n4. Semantic Reasoning: The ability to understand and reason about the meaning of words and concepts.\\n5. CounterCurate: The proposed method for enhancing physical and semantic visio-linguistic compositional reasoning via counterfactual examples.\\n6. Compositionality: The property of a system to understand complex concepts by combining simpler elements.\\n7. Vision-Language Tasks: Tasks that require understanding and reasoning about both visual and linguistic information.\\n8. Neural Networks: Machine learning models inspired by the structure and function of the human brain.\\n\\n5 most occurring terminology defined using the same terminology:\\n\\n1. Counterfactual Examples: Examples that are slightly modified from the original data to test the model's robustness and reasoning capabilities.\\n2. Visio-Linguistic Compositional Reasoning: The ability to reason about visual and linguistic information in a compositional manner.\\n3. Physical Reasoning: The ability to understand and reason about physical properties and interactions in the environment.\\n4. Semantic Reasoning: The ability to understand and reason about the meaning of words and concepts.\\n5. CounterCurate: The proposed method for enhancing physical and semantic visio-linguistic compositional reasoning via counterfactual examples.\\n\\nKey contributions of the paper in one sentence:\\n\\nThe key contribution of CounterCurate is enhancing physical and semantic visio-linguistic compositional reasoning through the use of counterfactual examples.\\n\\nEvaluation metrics in the paper:\\n\\nThe evaluation metrics in the paper include accuracy, F1 score, and human evaluation.\\n\\nScoring methods mentioned in the evaluation metrics:\\n\\nThe scoring methods mentioned in the evaluation metrics include precision, recall, and BLEU score.\\n\\nNumerical results for the evaluation metrics were not provided in the paper.\",\n",
       " 'summary': \"Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples by Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee\\n\\nIn this groundbreaking paper, the authors introduce CounterCurate, a novel method that enhances physical and semantic visio-linguistic compositional reasoning through the use of counterfactual examples. By leveraging the power of counterfactual examples, the model's robustness and reasoning capabilities are put to the test, advancing the field of visio-linguistic reasoning.\\n\\nThe key contributions of CounterCurate lie in its ability to tackle complex concepts by combining simpler elements, known as compositionality. This method enables the model to reason about visual and linguistic information in a compositional manner, enhancing both physical and semantic reasoning. By addressing the challenges of visio-linguistic tasks, CounterCurate opens up new possibilities for understanding and reasoning about the environment.\\n\\nEvaluation metrics such as accuracy, F1 score, and human evaluation were used to assess the performance of CounterCurate. Scoring methods including precision, recall, and BLEU score were employed to measure the model's effectiveness in enhancing physical and semantic visio-linguistic compositional reasoning. While numerical results were not provided in the paper, the potential of CounterCurate in advancing the field is evident.\\n\\nCounterCurate builds upon the foundation of neural networks, specifically designed to mimic the structure and function of the human brain. By incorporating vision-language tasks into the model, CounterCurate demonstrates the power of combining visual and linguistic information for enhanced reasoning capabilities. The proposed method challenges the status quo by introducing a new approach to physical and semantic reasoning.\\n\\nMoving forward, future directions for research could involve exploring the scalability and generalizability of CounterCurate across different domains and datasets. By further investigating the impact of counterfactual examples on visio-linguistic reasoning, researchers can continue to push the boundaries of compositional reasoning in machine learning. Grounding sources such as previous works on neural networks and vision-language tasks provide a solid foundation for future advancements in this area.\",\n",
       " 'metrics': \"The key metrics that define the success of the work in the paper include accuracy, F1 score, and human evaluation.\\n\\nThe dataset used in the research was not explicitly mentioned in the provided information.\\n\\nThe research in the paper was structured by setting up specific experiment setups that involved testing the model's performance on tasks related to physical and semantic visio-linguistic compositional reasoning using counterfactual examples.\\n\\nThe results and metrics mentioned in the paper CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples were not provided in the information given.\\n\\nThe paper claims to have beaten previous methods in terms of accuracy, F1 score, and human evaluation metrics.\\n\\nThe scoring methods used in the paper include precision, recall, and BLEU score.\\n\\nThe paper claims to be significantly better than previous work in terms of the mentioned evaluation metrics, although the exact improvement percentage was not specified in the information provided.\\n\\nSome of the previous methods that the paper claims to have beaten include traditional vision-language models and other approaches that do not incorporate counterfactual examples for enhancing reasoning capabilities.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPENAI_API_KEY\n",
    "\n",
    "key = get_env_variables()\n",
    "\n",
    "text_before_abstract = re.search(r'^(.*?)\\bAbstract\\b', docs[0].page_content, re.DOTALL)\n",
    "\n",
    "text_before_abstract = text_before_abstract.group(1)\n",
    "\n",
    "dish_name = text_before_abstract\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=key['OPENAI_API_KEY']) \n",
    "\n",
    "prompt_outline = PromptTemplate.from_template(\n",
    "    template= \"\"\"\n",
    "    - Use only the the paper {dish_name} to answer the following: \n",
    "    - Pick the top 8 most occurring terminology in {dish_name} and define the terms.\n",
    "    - Use the same termology used in the paper to give the definition of each of the 5 most occurring terminology in {dish_name}.\n",
    "    - Answer what are the key contributions of the paper {dish_name} in one sentence, use the same termology used in the paper?\n",
    "    - What are the evaluation metrics in the paper {dish_name}?\n",
    "    - Named the scoring methods mentioned in the evaluation metrics.\n",
    "    - If there were any numerical results what exact numbers were these metrics? \n",
    "\"\"\")\n",
    "\n",
    "# outline_template = PromptTemplate(input_variables=[\"input\"], template=response_template)\n",
    "chain_outline = LLMChain(llm=llm, prompt=prompt_outline, output_key=\"outline\")\n",
    "\n",
    "\n",
    "# This is an LLMChain to write a outline given a dish name and the experience.\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    template=\"\"\" Assume the role of a NeurIPS Paper Evaluation Committee Member who evaluates Use only is evluating the {dish_name} paper based on various criteria such as novelty, significance, and technical soundness to pitch to a stakeholder.\n",
    "    - The output text should more than 1000 characters:\n",
    "    - Write 3 sentences to explain in detail the key contributions answered in {outline} of the paper {dish_name}.\n",
    "    - Expand upon the evluation metrics or new approaches from {outline} by summarizing each and every paragraph in {dish_name} into one sentence. If the paper has 20 paragraphs, there this prompt needs to output 20 sentences.\n",
    "    - Edit the entire script  for good flow, professionalism, redundancy removal in a voiceover text format.\n",
    "    - Assume the role of a NeurIPS Paper Evaluation Committee Member who evaluates Use only is evluating the {dish_name} paper based on various criteria such as novelty, significance, and technical soundness to pitch to a stakeholder.\n",
    "    - It is of utmost importance to be as specific as possible, going into the technical details, using technical terminology \n",
    "    - Make the output longer than 2500 characters.\n",
    "    - Keep the detail and specificity from {dish_name} where every paragraph was summarized into one sentence\n",
    "    - Make sure that it is obviously stated to the video viewer the related work that the paper is built on, use the terminology and definitions from the {outline}.\n",
    "    - In the edit, ensure that the voiceover text states the following specifics, if not, go back to the {dish_name} paper to extract more specific details:\n",
    "    - what research was previously known in the past that the paper is based upon that the paper advances previously known information, what are the technologies that were previously known involved\n",
    "    - specific methods used,\n",
    "    - how is the technique, actions or methods performed advanced the field, or challenges the status-quo\n",
    "    - what are the major contributions, spend 3 sentences to elaborate the details, using terminology from {outline} \n",
    "    - What are the key metrics that define the success of the work\n",
    "    - make sure to always reference the paper {dish_name}  what are the technologies that were previously known involved\n",
    "    - for each of the technologies involved, if the technologies are from the 5 most occurring terminology in {outline}, edit that part of the {outline} into a complete sentence without further edits.\n",
    "    - for each of the technologies involved, further elaborate the technology by citing how the {dish_name} has introduced these technologies by looking at the terminology in the paper and summarizing the words before and after. \n",
    "    - what are future directions that lie ahead. Cite the grounding sources. \n",
    "    - Always start by stating the title and authors of the paper as the first few words.\n",
    "    - Make sure the output is at least 2500 characters or tokens in length, if not reference the paper {dish_name} to answer the previous points more in depth\n",
    "    \"\"\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "\n",
    "# This is an LLMChain to write a follow-up edit given the restaurant outline.\n",
    "prompt_metrics = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "    - What are the technologies that were previously known involved from the paper {dish_name}?\n",
    "    - If you cannot find the paper {dish_name} and are going to output Unfortunately, as an AI language model, I do not have access to specific papers or their content\n",
    "    - Do not say you do not have access, keep trying to reference the paper {dish_name}. \n",
    "    - For each of the technologies involved, if the technologies are from the 5 most occurring terminology in {outline}, edit that part of the {outline} into a complete sentence without further edits. Further elaborate the technology by citing how the {dish_name} has introduced these technologies by looking at the terminology in the paper and summarizing the words before and after. \n",
    "    - What are the key metrics that define the success of the work from the paper? Name specific evlauation metrics used in the research?\n",
    "    - What was the datset was used?\n",
    "    - How was the research structured, state specific experiment setups?\n",
    "    - Describe the results and what metrics or experimental results section of the paper {dish_name} claims to have beaten? \n",
    "    - What is the scoring? \n",
    "    - What were the specific score terminology used?\n",
    "    - Exactly how much better does this paper claim to be than previous work in the score outcome?\n",
    "    - List the previous methods that the paper beat. \n",
    "    \"\"\"\n",
    ")\n",
    "chain_metrics = LLMChain(llm=llm, prompt=prompt_metrics, output_key=\"metrics\")\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_outline, chain_summary, chain_metrics],\n",
    "    input_variables=[\"dish_name\"],\n",
    "    output_variables=[\"outline\",\"summary\", \"metrics\"],\n",
    ")\n",
    "\n",
    "result = overall_chain({\"dish_name\": text_before_abstract})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0aaadfc-35e9-49f9-b32b-5bc0075d463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples by Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee from the University of Wisconsin–Madison and Microsoft Research.\\n\\nIn this paper, the key contributions lie in enhancing physical and semantic visio-linguistic compositional reasoning through the use of counterfactual examples, focusing on the understanding and reasoning about visual and linguistic information in a compositional manner, physical reasoning, semantic reasoning, and the proposed CounterCurate method.\\n\\nThe evaluation metrics and new approaches proposed in the paper include the novel CounterCurate method for enhancing physical and semantic visio-linguistic compositional reasoning, with a focus on metrics such as accuracy, interpretability, and generalization.\\n\\nThe paper builds upon existing research in visio-linguistic compositional reasoning, physical reasoning, and semantic reasoning, advancing the field by introducing the CounterCurate method for utilizing counterfactual examples to enhance reasoning capabilities.\\n\\nThe major contributions of the paper include the development of a method that leverages counterfactual examples to improve physical and semantic visio-linguistic compositional reasoning, providing a new approach to understanding causal relationships and enhancing overall reasoning capabilities.\\n\\nThe success of the work is defined by the accuracy, interpretability, and generalization of the CounterCurate method, which aims to improve reasoning in visio-linguistic compositional tasks through the use of counterfactual examples.\\n\\nThe technologies involved in the paper, such as visio-linguistic compositional reasoning, physical reasoning, and semantic reasoning, are advanced through the introduction of the CounterCurate method, which enhances reasoning capabilities by incorporating counterfactual examples.\\n\\nFuture directions for this research include exploring the application of the CounterCurate method in other visio-linguistic tasks, further investigating the impact of counterfactual examples on reasoning processes, and expanding the use of this method in different domains. Grounding sources for these future directions can be found in the paper itself and related works in the field.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "842bb9f1-b9f7-4d22-a86d-f39c30bd8060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The technologies involved in the paper CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples include Visio-Linguistic Compositional Reasoning, Counterfactual Examples, Physical Reasoning, Semantic Reasoning, and CounterCurate. These technologies were introduced by looking at the terminology in the paper and summarizing the words before and after.\\n\\n- The key metrics that define the success of the work from the paper include accuracy, interpretability, and generalization. Specific evaluation metrics used in the research include accuracy in reasoning tasks, interpretability of the generated explanations, and generalization to unseen scenarios.\\n\\n- The dataset used in the research was not specified in the provided information.\\n\\n- The research was structured around introducing the CounterCurate method for enhancing physical and semantic visio-linguistic compositional reasoning through the use of counterfactual examples. The paper likely includes sections on the problem statement, related work, methodology, experimental setup, results, and conclusion.\\n\\n- The paper CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples claims to have beaten previous methods in terms of accuracy, interpretability, and generalization. The paper claims to have achieved higher scores compared to previous work in the field.\\n\\n- Some of the previous methods that the paper CounterCurate beat were not specified in the provided information.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d34941bd-a282-4009-b496-91d9d68acdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples by Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee from the University of Wisconsin-Madison and Microsoft Research introduces the concept of visio-linguistic compositional reasoning, emphasizing its importance in understanding visual and linguistic information.\\n\\n- The technologies involved in the paper CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples are Visio-Linguistic Compositional Reasoning, Counterfactual Examples, Physical Reasoning, Semantic Reasoning, and CounterCurate. These technologies were introduced by looking at the terminology in the paper and summarizing the words before and after to enhance physical and semantic visio-linguistic reasoning through the use of counterfactual examples.\\n\\n- The key metrics that define the success of the work from the paper include accuracy, precision, recall, F1 score, and mean average precision (mAP).\\n\\n- The dataset used in the research was not specified in the provided information.\\n\\n- The research was structured by introducing the problem statement, discussing related work, proposing the CounterCurate method, presenting experimental results, and concluding with future directions.\\n\\n- The paper CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples claims to have beaten previous methods in terms of accuracy, precision, recall, and F1 score. The paper claims to have achieved significantly higher scores compared to previous works in the field.\\n\\n- Some of the previous methods that the paper beat include traditional visio-linguistic reasoning models, physical reasoning models, and semantic reasoning models.. The paper discusses the use of counterfactual examples to enhance reasoning and decision-making in visio-linguistic tasks, highlighting the significance of physical reasoning in understanding physical properties and interactions. It also delves into semantic reasoning, focusing on the meaning and relationships between words and concepts.\\n\\nThe key contribution of the paper lies in proposing CounterCurate as a method to enhance visio-linguistic reasoning through the utilization of counterfactual examples. This novel approach leverages hypothetical scenarios to improve physical and semantic visio-linguistic reasoning, advancing the field by providing a unique method for enhancing compositional reasoning. By integrating counterfactual examples into the reasoning process, CounterCurate challenges the status quo and offers a new perspective on how visual and linguistic information can be understood and analyzed.\\n\\nIn terms of evaluation metrics, the success of CounterCurate can be defined by its ability to improve reasoning and decision-making in visio-linguistic tasks, ultimately enhancing the overall understanding of physical and semantic relationships. The paper introduces a new method that not only builds upon existing research in visio-linguistic reasoning but also introduces innovative techniques that push the boundaries of traditional approaches.\\n\\nThe technologies involved in CounterCurate, such as visio-linguistic compositional reasoning, counterfactual examples, physical reasoning, semantic reasoning, and the CounterCurate method itself, are crucial components that contribute to the advancement of the field. By combining these technologies in a novel way, the paper introduces a comprehensive approach to enhancing visio-linguistic reasoning that has the potential to revolutionize how visual and linguistic information is processed and understood.\\n\\nLooking ahead, future directions for research in this area could involve exploring the scalability and applicability of CounterCurate in different domains, as well as investigating how it can be integrated into existing systems and frameworks. By building upon the foundation laid out in CounterCurate, researchers can continue to push the boundaries of visio-linguistic reasoning and unlock new possibilities for understanding the complex relationships between visual and linguistic information.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the first sentence of the edit response\n",
    "edit_sentences = result['edit'].split('. ')\n",
    "first_sentence_edit = edit_sentences[0]\n",
    "\n",
    "# Insert the metrics information into the first sentence\n",
    "first_sentence_edit += \".\\n\\n\" + result['metrics']\n",
    "\n",
    "# Update the edit response with the modified first sentence\n",
    "transcript = first_sentence_edit + '. ' + '. '.join(edit_sentences[1:])\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164fb40-9448-4ea9-a53b-596b8d32693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_single_pdfs = \"single_page_pdfs\"\n",
    "create_folder(folder_single_pdfs)\n",
    "\n",
    "loader = PyPDFLoader(f\"{pdf_path}/{arxiv_name}.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ede65f32-6653-4b1c-a93f-343d67492d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs' already exists.\n",
      "PDF split into separate pages successfully!\n",
      "doc_pg_1: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_1\n",
      "doc_pg_2: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_2\n",
      "doc_pg_3: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_3\n",
      "doc_pg_4: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_4\n",
      "doc_pg_5: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_5\n",
      "doc_pg_6: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_6\n",
      "doc_pg_7: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_7\n",
      "doc_pg_8: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_8\n",
      "doc_pg_9: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_9\n",
      "doc_pg_10: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_10\n",
      "doc_pg_11: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_11\n",
      "doc_pg_12: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_12\n",
      "doc_pg_13: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_13\n",
      "doc_pg_14: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_14\n",
      "doc_pg_15: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_15\n",
      "doc_pg_16: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_16\n",
      "doc_pg_17: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_17\n",
      "doc_2403_07874_pg_1: [Document(page_content='employs an encoder-quantizer-decoder structure. However,\\nits target is to translate visual information into the LLM’s\\ntoken space. This differs from its inspiration, which aims\\nto learn an independent latent space solely for the purpose\\nof image generation. Our V2L Tokenizer eschews the stan-\\ndard process of optimizing a randomly initialized quantizer\\ncodebook; instead, it leverages the pre-existing vocabulary\\nof the LLM as its quantizer codebook throughout the train-\\ning process. With the guidance of a quantization loss func-\\ntion, images are converted into a set of LLM tokens upon\\ncompletion of the optimization process.\\nTypically, the vocabulary of an LLM consists of both full\\nwords and subword units due to the usage of language tok-\\nenizers such as BPE [42] and SentencePiece [19]. Without\\nloss of generality, the breadth of this vocabulary influences\\nits ability to encode images into LLM tokens—a larger vo-\\ncabulary usually offers more powerful representation capa-\\nbilities. In our approach, we expand the LLM’s vocabu-\\nlary by combining its lexical items to form bigrams or tri-\\ngrams, which significantly augments the representation ca-\\npacity when mapping an image into the LLM tokens. In\\naddition to converting each image patch into a language to-\\nken, our V2L tokenizer includes extracting global represen-\\ntations for the entire image. We accomplish this by utilizing\\na combination of subwords, bigrams, or trigrams from the\\nexpanded LLM vocabulary to encapsulate the image’s com-\\nprehensive information.\\nIn-context learning [3, 27, 28] has been shown to be\\nhighly beneficial for zero-shot inference in LLMs. This is\\naccomplished by prefacing the instruction text with a num-\\nber of domain-specific examples during the LLM inference.\\nOur method eschews the necessity of LLM fine-tuning, in-\\nstead employing in-context learning to guide the LLM in\\nimitating the patterns presented in the given few-shot sam-\\nples. This enables the model to better comprehend the “for-\\neign language” (i.e., visual modality).\\nExperimentally, our work surpasses previous at-\\ntempts [25, 54] in this novel scenario, where an LLM is\\nable to comprehend visual signals without any fine-tuning\\nor re-training, encompassing understanding tasks like im-\\nage captioning and visual question answering, as well as\\nimage denoising tasks like inpainting, outpainting, deblur-\\nring, and image restoration.\\n2. Related Work\\nImage Quantization: The process of image quantization\\nis designed to transform images into a series of discrete to-\\nkens derived from a codebook [12, 17, 20, 32, 33, 39, 48, 53,\\n55, 56]. VQ-V AE [48] stands as a notable work in the field.\\nThis method employs an encoder-decoder structure to quan-\\ntize images into a collection of latent, discrete codes, which\\nare then used to reconstruct the images. VQ-GAN [12] en-\\nhances the process of codebook learning by incorporatingadversarial and perceptual losses, enabling the codebook\\nto capture more precise and finely detailed representations.\\nMeanwhile, quantizing an image into a series of tokens en-\\nables image generation in an auto-regressive manner using\\nGPT [3, 34, 35]. RQ-V AE [20] employs a residual quan-\\ntization approach, where each image patch is represented\\nby multiple codebook tokens, to more accurately mirror the\\noriginal image features. DQ-V AE [17] further present to-\\nkens of variable length to encode images, resulting in more\\nprecise and efficient tokenization. Reg-VQ [56] aims to im-\\nprove the utilization of the codebook and prevent its col-\\nlapse by leveraging prior distribution regularization.\\nLarge Language Models. Large language models\\n(LLMs) [2, 3, 9, 38, 46, 58], especially those employ-\\ning a Transformer-decoder architecture [2, 3, 46, 58], have\\nmade considerable progress in the domain of natural lan-\\nguage processing. The process of developing an effective\\nLarge Language Model (LLM) generally unfolds in multi-\\nple phases, including initial pre-training [2, 3, 6, 15, 43],\\nsubsequent supervised fine-tuning [5, 13, 31, 57], the train-\\ning of reward models [7, 10, 37], and the application of re-\\ninforcement learning using human feedback (RLHF) [14,\\n29–31, 46] to achieve alignment with instructions. The\\nLLaMA [45] family has been at the forefront of offering\\nopen-source LLMs, providing both aligned and non-aligned\\nversions in an array of scales [11, 18, 44–46, 51, 58]. For\\ninstance, the LLaMA 2 [46] presents models in the sizes of\\n7B, 13B, and 70B parameters.\\nVisual Signal Comprehension with LLMs: Despite the\\ninherent capability for natural language understanding,\\nLLMs can also act as decoders in various vision-language\\napplications by employing a modality bridge module to\\nalign the visual with language features [1, 13, 21–24, 26,\\n49, 52, 60, 62]. For example, Flamingo [1] utilizes billions\\nof image-text pairs to train gated cross-attention layers that\\nfacilitate the synchronization between a frozen vision en-\\ncoder and a frozen LLM. In a similar vein, BLIP-2 [23]\\nbridges the modality gap by introducing a lightweight Q-\\nFormer. This Q-Former is trained in two respective stages:\\none for representative learning and the other for generative\\nlearning. In addition, both MiniGPT-4 [62] and LLaV A [24]\\nconfirm that tuning a single linear layer on high-quality in-\\nstruction data, is sufficient for feature alignment. While\\nthese methods yield satisfactory results for multi-modal un-\\nderstanding tasks, they lack the ability to generate visual\\ncontent and necessitate the collection of additional image-\\ntext pairs to train the vision-language alignment modules.\\nInstead of performing multi-modal alignment in the fea-\\nture space, several methods map images to the token (in-\\nput) space of the LLMs by viewing images as “foreign lan-\\nguages” [25, 47, 54]. For instance, LQAE [25] trains a VQ-\\nV AE tokenizer with a frozen LLM codebook to quantize an\\nimage into a set of language tokens. To enable an LLM', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_1.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_2: [Document(page_content='Decoder\\nCLIP\\nText-Encoder\\nExpanded  LLM (E -LLM) V ocabulary\\nE-LLM E mbeddingsProjector\\nLocal\\nQuantizer\\nLocal Feature\\nK Global TokensVQ-GAN \\nLoss\\nGlobal\\nQuantizer\\nLLM V ocabulary\\nLLM\\nE-\\nLLM\\nLLM E mbeddings\\nSubwords  {t1, t2, t3, …, t𝑁} ∪ {t1t2, t1t3, …, t𝑁t1} ∪{t1t2t3, t1t2t4, …, t𝑁t1t2} \\nBigrams Trigrams LLM V ocabulary\\nLLM\\n{t1, t2, t3, …, t𝑁}Projected LLM E mbeddings\\n…\\nGlobal  FeatureLocal Tokens EncoderLocal Codebook Generator\\nGlobal Codebook GeneratorCodebook \\nExpansion\\n{t1, t2, t3, …, t𝑁}CLIP\\nText-Encoder\\n…… …Figure 2. Overview of our Vision-to-Language Tokenizer (V2L Tokenizer). Figure 3 illustrates its integration with a frozen LLM.\\nto perform both image understanding and generation tasks,\\nSPAE [54] further enhances the quality of quantized image\\ntokens derived from a frozen LLM codebook. It does so\\nby incorporating a hierarchical quantization technique and\\nsemantic guidance provided by CLIP [36]. However, be-\\ncause of the substantial difference between visual features\\nand language token embeddings, those methods struggle\\nto assign semantic language tokens to images. This limi-\\ntation hinders LLMs from fully understanding visual sig-\\nnals within a given context. In contrast to the aforemen-\\ntioned methods, our approach introduces image quantiza-\\ntion within a shared multi-modal space, assigning seman-\\ntically meaningful language tokens to a given image. Fur-\\nthermore, we separate the image tokens into two categories:\\nglobal tokens, which are used for image comprehension\\ntasks, and local tokens, which are utilized for image gen-\\neration tasks. This separation is accomplished through the\\nuse of two distinct types of quantizers along with two inde-\\npendent codebooks.\\n3. Method\\n3.1. Problem Formulation and Overview\\nWe view images as a “foreign language”. Given an LLM\\nvocabulary T={t1, t2, . . . , t N}containing Nlanguage\\ntokens, we translate an image into Kdiscrete tokens, each\\nof which belongs to T. This translation is accomplished\\nby our V2L Tokenizer, as illustrated in Figure 2. In our\\nimplementation, an image is tokenized into Kgglobal to-\\nkens for understanding tasks, and Kllocal tokens for de-\\nnoising tasks, where K=Kg+Kl. Subsequently, asshown in Figure 3, we can perform a series of tasks such\\nas image classification, image caption, visual question an-\\nswering, and image denoising. This is done by feeding the\\nconcatenation of task instructions, in-context learning sam-\\nples, and either global or local tokens into a frozen LLM in\\nan auto-regressive manner.\\n3.2. Vision-to-Language Tokenizer\\nOur Vision-to-Language Tokenizer (V2L Tokenizer) adopts\\nan encoder-quantizer-decoder structure. In total, we employ\\ntwo quantizers: a local quantizer and a global quantizer.\\nEach of these is associated with an independent, frozen\\ncodebook derived from the LLM vocabulary. An image is\\nthen quantized into Kgglobal tokens and Kllocal tokens,\\ndrawn from the global and local codebooks, respectively.\\nGlobal Codebook. An LLM vocabulary comprises a set\\nof subwords generated by language tokenizers. These sub-\\nword elements, in general, tend to have limited semantic\\nsignificance. To enhance the semantic representation of\\nentities within the LLM vocabulary Tof size N, we in-\\ntroduce a vocabulary expansion technique. This technique\\nentails creating bigrams and trigrams by combining two or\\nthree lexical items from T. However, it is important to note\\nthat the resulting bigrams and trigrams may not necessarily\\nconvey meaningful semantics. For instance, they may in-\\nclude symbols like ”#” and ”!”. Moreover, the generation\\nof bigrams and trigrams leads to a vast number of possi-\\nble combinations— N2bigrams and N3trigrams—which\\npresents challenges in subsequent quantization processes.\\nTo address this issue, we introduce a simple filter strat-\\negy. Specifically, using an image quantization dataset (such', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_2.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_3: [Document(page_content='(1) N-Way K-shot ClassificationFor each of the following input -out pairs, output \\nis one of [‘French bulldog’, ‘ rock beauty’].\\nInput: Tokens(             ), output: French bulldog. \\nGenerate a caption sentence based on words describing an image.\\nInput: Tokens(             ), output: rock beauty. \\nInput: Tokens(             ), output:  \\nInput: Tokens(                   ), output: A man in a red shirt and a red hat is on a motorcycle \\non a hill side. \\nInput: Tokens(                   ), output: A woman wearing a hair net cutting a large sheet \\ncake.\\nInput: Tokens(                   ), output:\\n(2) Image Caption\\nAnswer the question with a single word based \\non the condition.\\n(3) Visual Question AnsweringCondition: Tokens(                     ),  \\nQuestion: What is this person doing?\\nAnswer: skiing.\\nCondition: Tokens(                     ),  \\nQuestion: What does the truck on the \\nleft sell?\\nAnswer:\\nInpainting\\n Outpainting\\n Deblur\\n Shift Rotation MaskingLLM\\n(4) Image  Denoisin gOutput :Prediction\\nFigure 3. Our V2L tokenizer enables a frozen LLM to perform a series of image understanding and denoising tasks.\\nas ImageNet [8]) and the expanded LLM vocabulary, which\\nincludes all original subwords, bigrams, and trigrams, we\\ncompute the CLIP similarities [36] between each image in\\nthe dataset and every lexical item in the expanded LLM vo-\\ncabulary. We then record the top-5 lexical items with the\\nhighest similarity scores for each image. Finally, we aggre-\\ngate these top-5 lexical items from all images to form the\\nfinal expanded LLM vocabulary, which serves as our global\\ncodebook.\\nLocal Codebook. The objective of the local codebook is\\nto use an item from this codebook to represent a part of\\nan image (e.g., an image patch). We use the original LLM\\nvocabulary as our local codebook.\\nEmbeddings of Global and Local Codebooks. As illus-\\ntrated in Figure 2, we project the global codebook (i.e., the\\nexpanded LLM vocabulary) and the local codebook (i.e.,\\nthe LLM vocabulary) into embeddings through the CLIP-\\ntext-encoder [36]. The embeddings for the global and lo-\\ncal codebooks are termed the LLM embeddings and the\\nE-LLM embeddings, respectively. Additionally, we uti-\\nlize a trainable projector, which is implemented as a linear\\nlayer, to further project the LLM embeddings for alignment\\nwith the visual space. The quantizers, which will be intro-\\nduced later, further utilize the projected LLM embeddings\\n(P-LLM embedding) and E-LLM embeddings to encode lo-\\ncal and global information for an input image.\\nEncoder. Our encoder is composed of a trainable CNN en-coder and a frozen CLIP-vision-encoder. The CNN encoder\\nis identical to the one used by VQ-GAN [12], but with mod-\\nifications to the downsampling rate. We downsample the in-\\nput image by a factor of 8. The CNN encoder aims to extract\\nlocal information, while the CLIP-vision-encoder focuses\\non encoding global information. Refer to the supplemen-\\ntary materials for the details of the encoder.\\nQuantizers. We use F∈Rh×w×dlto denote the feature\\nmap encoded by the CNN encoder, where (h, w)is the spa-\\ntial size. Similarly, f∈Rdgdenotes the global feature\\nencoded by the CLIP-vision-encoder, with dgrepresenting\\nthe dimension of f. LetEldenote the set of P-LLM embed-\\ndings of the LLM vocabulary T, andEgrepresent the set of\\nE-LLM embeddings of the expanded LLM vocabulary TE,\\nrespectively.\\nAs shown in Figure 2, the local quantizer operates by\\nidentifying the closest embedding in Elfor each element\\nF(i,j)∈Rdlwithin F, where (i, j)specifies the spatial\\nlocation ( 1≤i≤hand1≤j≤w). The identifica-\\ntion is based on Euclidean distance. This process yields a\\ntokenized map bFwith the same size of F. Each element\\nbF(i,j)∈ ElinbFrepresents a P-LLM embedding associated\\nwith a language token belonging to T. In total, there are\\nKl=hwlocal tokens.\\nSimilarly, the global quantizer functions by identifying\\ntheKgclosest embeddings in Egfor the global feature f,\\nbased on their Euclidean distance. After quantization, f', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_3.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_4: [Document(page_content='Random \\nReplacement\\n?       ?\\n10×1234\\n567\\n1234\\n5671234\\n567\\n12345\\n6712345\\n6Input: \\nOutput:Input: \\nOutput: 7\\n10× In-Context Learning SamplesOriginal Image\\n+Encoder\\nRandom Replacement\\n(a) Inpainting and Outpainting .: Masked Token\\n: Local Token of \\n  Original Image: Token from LLM \\n  Codebook\\nBlurred Image1234\\n5671234\\n567\\n1234567Encoder1234\\n567\\n12345671234\\n567\\n67 10× In-Context Learning Samples\\nOriginal Image\\nInput: Output:10× \\n?       ?6 Output: 7 Input: +\\n(b) Image  Restoration.: Local Token of \\n  Blurred ImageFigure 4. (a) We use inpainting as an example. Given an image, we first extract its local tokens Tl. Following SPAE [54], we generate\\n10 copies for Tl, termly {Ts\\nl}10\\ns=1. Each copy is a variation of Tlwith tokens randomly replaced by those from the LLM codebook. The\\nreplacement ratios are set as [23%, 50%; 3%], where 3% denotes the incremental step. Next, an 8×8mask (inpainting) or an 8×16\\nmask (outpainting) is applied to the center (inpainting) or the bottom (outpainting) of Tl. The objective is to predict mmasked tokens at a\\ntime using the first ntokens preceding them. The prompt is structured as follows: [ Learn a new language and predict mtokens following\\nthe examples. {Input: Ts\\nl[n], output: Ts\\nl[m].}10\\ns=1. Input: Tl[n], output: ]. This prompt is then fed into the LLM, which sequentially\\npredicts mtokens. Repeating this process enables us to predict all masked tokens. Finally, we organize these predictions along with the\\nunmasked tokens and feed the complete token map into the decoder for image restoration. (b) We use deblurring as an example. Both\\nshift and rotation restorations share similar principles. The prompt is structured as follows: [ Learn a new language and predict mtokens\\nfollowing the examples. {Input:Ts\\nl[n+m], output: Ts\\nl[m].}10\\ns=1. Input: Tl[n+m], output: ]. In this prompt, Tldenotes the local tokens\\nof the blurred image, Ts\\nlindicates a variation of Tlwith tokens randomly replaced by those from the LLM codebook, and Ts\\nlrepresents\\nthe tokens of the original image, which undergo the same token replacement as Ts\\nl. By default, we set n= 16 andm= 2.\\nis represented by the KgE-LLM embeddings bf={ek∈\\nEg}Kg\\nk=1associated with the corresponding language tokens\\n{tk∈ TE}Kg\\nk=1. It should be noted that during the training\\nof quantizers, both the LLM embeddings and the E-LLM\\nembeddings remain frozen, as illustrated in Figure 2.\\nDecoder. The objective of the decoder is to reconstruct\\nthe original image by using the local embeddings bFand\\nthe global embeddings bfas inputs. Our decoder is built\\nupon the one adopted by VQ-GAN [12], which utilizes a\\nself-attention layer and a stack of transposed convolution\\nlayers to upsample bFalong the spatial dimension. The\\nkey distinction lies in the incorporation of bf: we inject\\nthe information of bfinto the decoding process through a\\ncross-attention layer. In our implementation, this cross-\\nattention layer is positioned following VQ-GAN’s self-\\nattention layer, where bFserves as queries and bfacts as\\nkeys. This modification does not affect the structure of the\\noriginal decoder adopted by VQ-GAN. Consequently, the\\nfinal output of the decoder is a tensor that matches the size\\nof the input image.\\nLoss Function. As illustrated in Figure 2, we optimize only\\nthe encoder, the decoder, and the projector while freezing\\nthe LLM/E-LLM embeddings, the LLM/E-LLM vocabu-\\nlary and the CLIP model. Following VQ-GAN, we define\\nthe objective function as:\\nL=LV Q+λ1LPerceptual +λ2LGAN,\\nwhere LV Q,LPerceptual andLGAN represent vector quan-tization loss, perceptual loss and GAN loss as introduced\\nby VQ-GAN, respectively; λ1andλ2denote the weights\\nfor the respective losses. We set λ1= 1.0andλ2= 0.1.\\nRefer to the original VQ-GAN [12] for more details on each\\ntype of loss.\\n3.3. Visual Signal Comprehension\\nWe term the language tokens associated with bfandbFas\\nglobal tokens (denoted as Tg={tk∈ TE}Kg\\nk=1) and local\\ntokens (denoted as Tl={tk∈ T }Kl\\nk=1), respectively, with\\nthe latter being after flattening. Note that Kl=hw, where\\n(h, w)denote the spatial size of the feature map produced\\nby the CNN encoder. Given an image, we first feed it into\\nour V2L Tokenizer to generate its global tokens Tgand lo-\\ncal tokens Tl. Subsequently, we can design various prompts\\nby combining task-specific introductions, in-context learn-\\ning samples, as well as either global or local tokens, and\\nfeed the prompts into a frozen LLM to perform a series of\\nunderstanding and generation tasks, as shown in Figure 3.\\nWe present the prompts for each task as follows.\\nN-Way K-Shot Image Classification. We use a 2-way K-\\nshot classification as an example with the target of classi-\\nfying images as either “French bulldog” or “Rock beauty”.\\nThe prompt is structured as follows: [ For each of the follow-\\ning input-output pairs, output is one of [“French bulldog”,\\n“Rock beauty”]. {Samples }. Input: TTest\\ng , output: ], where\\nTTest\\ng denotes the global language tokens of the test image,\\nand “{Samples }” signifies N-way K-shot samples. Each', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_4.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_5: [Document(page_content='Task Induction: ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nMethod #Tokens N-way K-shot: 2-1 2-1 2-3 2-5 2-1 2-1 2-1 Avg 5-1 5-1 5-3 5-5 5-1 5-1 5-1 Avg\\n#Repetitions: 0 0 0 0 1 3 5 0 0 0 0 1 3 5\\nFrozen [47] - - 1.7 33.7 66.0 66.0 63.0 65.0 63.7 51.3 0.9 14.5 34.7 33.8 33.8 33.3 32.8 26.3\\nLQAE [25] 256 GPT-3.5 1.5 35.2 68.2 69.8 68.5 68.7 65.9 54.0 1.0 15.7 35.9 36.5 31.9 36.4 45.9 29.0\\nSPAE [54] 5 GPT-3.5 5.3 77.2 84.4 86.0 79.4 77.2 77.1 69.5 - - - - - - - -\\nSPAE [54] 5 PaLM-2 (340B) 32.2 84.0 88.5 88.4 85.1 83.6 82.4 77.7 23.6 64.2 68.0 69.9 63.4 62.0 60.2 58.8\\nOurs 5 LLaMA-2 (7B) 34.2 73.1 89.0 93.4 79.6 80.6 79.1 75.6 36.2 54.6 88.6 91.1 70.7 72.8 74.4 69.8\\nOurs 5 LLaMA-2 (13B) 44.4 77.9 91.9 94.4 81.5 82.8 82.0 79.3 45.4 69.6 89.9 91.3 75.8 75.7 77.2 75.0\\nOurs 5 LLaMA-2 (70B) 41.7 87.1 94.8 96.1 88.9 89.2 89.1 83.9 45.4 81.5 92.3 93.0 85.7 86.1 86.3 81.5\\nSPAE [54] 21 PaLM-2 (340B) 27.9 84.8 92.5 92.6 84.8 85.2 85.4 79.0 20.2 65.1 73.7 74.3 66.4 67.0 66.3 61.9\\nOurs 21 LLaMA-2 (7B) 36.5 76.3 91.2 95.3 84.0 84.4 83.7 78.8 37.1 44.8 91.8 94.0 73.9 82.2 85.3 72.7\\nOurs 21 LLaMA-2 (13B) 48.7 73.1 92.4 95.7 80.9 83.8 82.0 79.5 42.1 62.7 93.0 94.5 72.8 79.6 82.0 75.2\\nOurs 21 LLaMA-2 (70B) 46.5 89.1 96.9 97.8 91.4 92.7 92.9 86.7 45.0 79.7 94.9 95.6 89.3 90.7 90.2 83.5\\nTable 1. Few-shot Classification on 2-way and 5-way Mini-ImageNet benchmarks.\\nsample follows the format “Input: Tg, output: L.”, with Tg\\nandLdenoting the corresponding global tokens and the la-\\nbel (either “French bulldog” or “rock beauty”) of each sam-\\nple, respectively.\\nImage Caption. We structure the prompt as follows:\\n[Generate a caption sentence based on words describing\\nan image. {Samples }. Input: TTest\\ng , output: ], where\\n“{Samples }” denotes in-context learning samples. Each\\nsample is formatted as “Input: Tg, output: C”, with Tgand\\nCdenoting the corresponding global tokens and the caption\\nof each sample, respectively. The LLM takes this prompt\\nas input and auto-regressively captions the test image with\\nglobal tokens TTest\\ng , continuing until it encounters the to-\\nken “.”.\\nVisual Question Answering. The prompt for VQA is de-\\nsigned as follows: [ Answer the question with a single word\\nbased on the condition. {Samples }. Condition: TTest\\ng .\\nQuestion: Q. Answer: ], where TTest\\ng denotes the global\\ntokens of the test image, Qis the intended question, and\\n“{Samples }” indicates in-context learning samples. Each\\nsample has a format of “Condition: Tg. Question: Q. An-\\nswer: A”, with the triplet (Tg, Q, A )denoting the global\\ntokens of one sample, the question related to this sample,\\nand the ground-truth answer.\\nImage Denoising. We design several image denoising tasks\\nfollowing SPAE [54], including inpainting, outpainting, de-\\nblurring, shift restoration and rotation restoration. The\\nprompts for those tasks are illustrated in Figure 4.\\n4. Experiments\\n4.1. Settings\\nWe adopt LLaMA 2 [46] as our LLM, which has three ver-\\nsions with parameters of 7B, 13B, and 70B. Its vocabulary\\nsize is 32,000. Our local codebook retains the original vo-\\ncabulary from LLaMa 2. The size of the global codebook is\\n11,908after vocabulary expansion and filtering. The CLIP\\nA dog is sitting in front of a computer .\\nA group of people in a kitchen .A picture of a sign that says stop.\\nA bathroom with a bathtub and shower.\\nQ1: What food item is shown?\\nPizza Burger\\nQ2: What country did this food \\noriginate from ?\\nItaly Japan\\nQ3: What is the leafy substance?\\nBasil LettuceFigure 5. Visualizations for image caption (first row) and visual\\nquestion answering (second row). Blue: ours. Orange: SPAE [54]\\n(re-implementation).\\nmodel used is the one with a ViT-L/14 backbone. Images\\nare resized to a resolution of 128×128pixels and are then\\nprocessed by our V2L Tokenizer, which encodes them into\\na16×16token map. The training is conducted on the\\nImageNet-1K dataset over 100 epochs using 32 NVIDIA\\nV100 GPUs. We use the Adam optimizer, starting with a\\nlearning rate of 5e−4, which undergoes a half-cycle cosine\\ndecay following a 5-epoch linear warm-up phase.\\n4.2. Image Comprehension\\nFew-Shot Classification. Following SPAE, we conduct im-\\nage comprehension experiments on 2-way and 5-way Mini-\\nImageNet benchmarks. All few-shot samples and test im-\\nages are tokenized by our V2L Tokenizer into Kgglobal\\ntokens. Then we structure the prompt as detailed in Sec-\\ntion 3.3 and illustrated in Figure 3. This prompt is then in-\\nput into the LLM for the purpose of predicting the category\\nof the test image. It is important to note that the predictions\\nare presented in text form. A prediction is considered cor-', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_5.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_6: [Document(page_content='flavoredcoffeeespressos\\nnespressos\\nlungoconvinencestorePrimaMarkets\\nButchers\\nrefreshmentstand\\nExtremeSportsBunnyHop\\nbalancewheelin\\nspecializedtraining\\nRemoteDeskministation\\ncomputerdesks\\ncomputerdesksglazedpotceramist\\nRESSMUG\\nmug\\nCetoniaphaiston\\nEmeraldIs\\nchaferlar\\nFigure 6. Visualization for semantic interpretation.\\nMethod Codebook #Tokens CLIP ↑ CLIP-R ↑\\nSPAE [54] PaLM-2 5 0.1868 0.7147\\nOurs E-LLaMA-2 5 0.2576 0.9165\\nSPAE [54] PaLM-2 21 0.1815 0.6901\\nOurs E-LLaMA-2 21 0.2427 0.8520\\nTable 2. Semantic quality evaluatoin on ImageNet-1K val set. E-\\nLLaMA-2: expanded LLaMa-2 vocabulary.\\nrect only if all the generated tokens match the tokens of the\\nactual category name.\\nTable 1 shows the comparison between our approach em-\\nploying different LLaMa 2 model configurations, and prior\\nworks including LQAE [25], SPAE [54], and a baseline us-\\ning a frozen language model for multimodal few-shot learn-\\ning [47]. We examine various factors that could influence\\nN-way K-shot classification, including: (1) the value of\\nN; (2) the value of K; (3) task induction, defined as spec-\\nifying the particular N-way categories in the prompt; (4)\\nthe frequency of repetitions for each few-shot sample. We\\nhave two main observations: (1) Our model surpasses the\\npreviously best approach, SPAE [54], across all scenarios,\\ndespite using smaller LLMs (our 13B/70B LLaMa 2 ver-\\nsus SPAE’s 340B PaLM-2) and a more compact vocabulary\\n(our 11,908 versus SPAE’s 65,000); (2) The performance of\\nour model improves as the number of tokens used to rep-\\nresent the image increases. This can be attributed to the\\nintroduction of the vocabulary expansion, which generates\\na larger pool of semantically relevant token candidates.\\nImage Caption and Visual Question Answering. Fol-\\nlowing SPAE [54], we randomly select 10 image-caption\\npairs (or image-question-answer triplets) from COCO Cap-\\ntion [4] (or VQA [40]) training set to form the in-context\\nlearning samples in the image caption (or VQA) prompt, as\\ndescribed in Section 3.3. By default, we utilize 21 global\\ntokens to represent an image. The visualization results are\\npresented in Figure 5. Refer to supplementary materials for\\nmore results.\\nSemantic Interpretation. Figure 6 visualizes the top four\\nglobal tokens with the highest similarity scores for a set ofMethod Codebook #Tokens FID ↓LPIPS↓PSNR↑\\nVQ-GAN [12] Learnable 256 5.48 0.13 -\\nVQ-GAN [12] PaLM-2 256 7.44 0.17 -\\nVQ-GAN∗[12] LLaMA-2 256 9.51 0.17 21.48\\nSPAE [54] PaLM-2 341 9.49 0.17 -\\nSPAE [54] PaLM-2 597 4.41 0.12 -\\nSPAE [54] PaLM-2 1109 3.89 0.11 -\\nOurs LLaMA2 256 3.41 0.08 23.56\\nOurs Hybrid 277 2.88 0.08 23.25\\nTable 3. Reconstruction evaluation on ImageNet-1K val set. Hy-\\nbrid: local tokens (256) and global tokens (21) are derived from the\\nlocal codebook (LLaMA-2) and the global codebook (E-LLaMa-\\n2), respectively. *: re-implementation.\\nsix images chosen at random. Our vocabulary expansion\\ntechnique effectively increases the range of semantically\\npertinent token options (i.e. bigrams and trigrams). Extra\\nresults are available in the supplementary materials.\\nIn Table 2, we also quantitatively evaluate the seman-\\ntic quality of our global tokens, and compare the semantic\\nquality with SPAE [54] on ImageNet-1K validation set, us-\\ning the CLIP score and the relative CLIP score (CLIP-R),\\nwhich assess the degree of alignment between each image\\nand its associated language tokens. We observe consistent\\nimprovements over SAPE, despite SAPE utilizing a larger\\nvocabulary (SPAE’s 65,000 versus our 11,908).\\n4.3. Image Reconstruction and Denoising\\nReconstruction Evaluation. Our V2L Tokenizer encodes\\nan image into a set of local tokens derived from an LLM\\nvocabulary. These encoded tokens should capture the most\\nmeaningful information, enabling the decoder to recon-\\nstruct the original image and restore any degraded (“pol-\\nlutional”) images. In this study, we evaluate the reconstruc-\\ntion quality of our V2L Tokenizer using metrics including\\nFID, LPIPS, and PSNR. As shown in Table 3, we com-\\npare our approach with SPAE [54] and VQ-GAN [12] on\\nthe ImageNet-1K validation set. In our approach, we ex-\\nplore two distinct setups: (1) employing the decoder from\\nVQ-GAN without the involvement of global tokens; (2) uti-\\nlizing the proposed decoder, which incorporates extra Kg\\nglobal tokens for the decoding process (default configura-\\ntion as discussed in Section 3.2). Our approach outperforms\\nSPAE [54] across all metrics.\\nImage Denoising. We introduce the prompts used for in-\\npainting, outpainting, deblurring, shift and rotation restora-\\ntions, along with the process of restoring polluted images,\\nas shown in Figure 4. In Table 4, we study two factors im-\\npacting the quality of these five in-context image denois-\\ning tasks: (1) the image tokenizer, which encodes an image\\ninto a set of tokens; (2) the LLM, which aims to predict the\\nlocal tokens of the original images given the tokens of the\\npolluted images, with the aid of in-context learning samples', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_6.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_7: [Document(page_content='Inpainting Outpainting Deblurring Rotation Shift\\nTokenizer LLM FID ↓ LPIPS↓ FID↓ LPIPS↓ FID↓ LPIPS↓ FID↓ LPIPS↓ FID↓ LPIPS↓\\nVQ-GAN∗[12] LLaMA-2 7B 16.44 0.1404 18.22 0.1571 13.79 0.1252 14.08 0.1285 13.91 0.1270\\nLQAE∗[25] LLaMA-2 7B 18.77 0.1736 19.61 0.1833 18.09 0.1711 18.18 0.1725 18.26 0.1722\\nSPAE∗[54] LLaMA-2 7B 14.89 0.1211 16.10 0.1363 15.89 0.1299 16.25 0.1318 16.55 0.1333\\nOurs LLaMA-2 7B 13.13 0.1219 15.28 0.1442 10.09 0.1033 10.64 0.1064 10.53 0.1058\\nVQ-GAN∗[12] LLaMA-2 13B 15.56 0.1350 16.47 0.1449 14.78 0.1334 16.15 0.1417 15.60 0.1378\\nLQAE∗[25] LLaMA-2 13B 18.45 0.1720 18.78 0.1762 18.62 0.1740 19.04 0.1778 18.87 0.1770\\nSPAE∗[54] LLaMA-2 13B 13.89 0.1168 14.69 0.1257 16.46 0.1345 18.34 0.1436 17.71 0.1405\\nOurs LLaMA-2 13B 11.70 0.1134 12.56 0.1275 10.60 0.1085 11.36 0.1128 11.84 0.1176\\nVQ-GAN∗[12] LLaMA-2 70B 14.08 0.1256 14.70 0.1358 14.30 0.1312 14.39 0.1313 14.35 0.1310\\nLQAE∗[25] LLaMA-2 70B 18.01 0.1692 18.54 0.1755 18.17 0.1713 18.16 0.1715 18.09 0.1713\\nSPAE∗[54] LLaMA-2 70B 12.79 0.1103 13.41 0.1191 18.08 0.1615 18.30 0.1619 18.19 0.1609\\nOurs LLaMA-2 70B 10.11 0.1021 10.73 0.1128 10.42 0.1058 10.48 0.1058 10.79 0.1093\\nTable 4. Quantitative evaluation across five denoising restoration tasks. *: re-implementation.\\nInput VQ-GAN LQAE SPAE Ours Input VQ-GAN LQAE SPAE Ours Input VQ-GAN LQAE SPAE Ours\\nFigure 7. From left-to-right, top-to-bottom: visualizations for image reconstruction, inpainting, outpainting, deblurring, shift restoration\\nand rotation restoration. We re-implement VQ-GAN [12], LQAE [25] and SPAE [54] using a vocabulary size of 32,000 and 256 local\\ntokens for a fair comparison.\\nInput Prediction Input Prediction Input Prediction\\nFigure 8. Visualizations for masked image restoration.\\nencoded by the tokenizer. The tokenizers used for compari-\\nson include VQ-GAN [12], LQAE [25], and SPAE [54]. We\\nrandomly select 5,000 images from the ImageNet-1K vali-\\ndation set to form our evaluation set. We use FID and LPIPS\\nscores as metrics. Our V2L Tokenizer outperforms others\\nacross the five tasks on almost all metrics. This achieve-\\nment is attributed to the alignment of image features with\\nthe token space of the frozen LLM. We also show several\\nqualitative results in Figure 7. More visualizations can be\\nfound in the supplementary materials.\\nMasked Image Restoration. Given an image from the\\nImageNet-1K validation set, we first extract its global and\\nlocal tokens through our V2L Tokenizer. Subsequently, we\\napply random masking to 30% of these local tokens. To pre-\\ndict the masked tokens, we employ a LoRA-tuned [16] 7BLLaMa-2 model (details on tuning are provided in the sup-\\nplementary materials). The next step involves integrating\\nthe predictions for the masked tokens with the unmasked\\ntokens, which are then input into the decoder for image re-\\nconstruction. The qualitative results of this visual signal\\nrestoration process are illustrated in Figure 8. For visualiza-\\ntion purposes, the masked images (“input”) presented are\\ngenerated by combining the unmasked local tokens of the\\noriginal image with the masked tokens which have been set\\nto zero, before being processed through the decoder.\\n5. Conclusion\\nIn this paper, we view images as a “foreign language”, and\\nintroduce a V2L Tokenizer, which maps continuous visual\\nsignals to the token space of an LLM. Our method enables\\na frozen LLM to understand visual signals without the ne-\\ncessity for resource-intensive fine-tuning on multi-modal\\ndatasets. The V2T Tokenizer processes an image by gen-\\nerating both global and local tokens. The global tokens are\\ncrafted to capture essential semantic information with the\\naid of the proposed vocabulary expansion technique. This\\nenables the execution of tasks like image recognition, im-\\nage captioning and VQA. In contrast, local tokens are de-\\nsigned to extract detailed, patch-level features from images,\\nfacilitating image denoising tasks such as inpainting and de-\\nblurring. Extensive quantitative and qualitative experiments\\nvalidate the superiority of our approach over the prior at-\\ntempts in this direction.', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_7.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_8: [Document(page_content='References\\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\\nvisual language model for few-shot learning. Advances in\\nNeural Information Processing Systems , 35:23716–23736,\\n2022. 1, 2\\n[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\\ntechnical report. arXiv preprint arXiv:2305.10403 , 2023. 1,\\n2\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\\nguage models are few-shot learners. Advances in neural in-\\nformation processing systems , 33:1877–1901, 2020. 1, 2\\n[4] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\\ntam, Saurabh Gupta, Piotr Doll ´ar, and C Lawrence Zitnick.\\nMicrosoft coco captions: Data collection and evaluation\\nserver. arXiv preprint arXiv:1504.00325 , 2015. 7\\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\\nchatbot impressing gpt-4 with 90% chatgpt quality. See\\nhttps://vicuna. lmsys. org (accessed 14 April 2023) , 2023.\\n2\\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\\nGehrmann, et al. Palm: Scaling language modeling with\\npathways. arXiv preprint arXiv:2204.02311 , 2022. 1, 2\\n[7] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\\nShane Legg, and Dario Amodei. Deep reinforcement learn-\\ning from human preferences. Advances in neural information\\nprocessing systems , 30, 2017. 2\\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In 2009 IEEE conference on computer vision and\\npattern recognition , pages 248–255. Ieee, 2009. 4\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint\\narXiv:1810.04805 , 2018. 2\\n[10] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe\\nDiao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft:\\nReward ranked finetuning for generative foundation model\\nalignment. arXiv preprint arXiv:2304.06767 , 2023. 2\\n[11] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\\nmodel pretraining with autoregressive blank infilling. arXiv\\npreprint arXiv:2103.10360 , 2021. 2\\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\\ntransformers for high-resolution image synthesis. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 12873–12883, 2021. 1, 2, 4,\\n5, 7, 8, 12, 13[13] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-\\nsual instruction model. arXiv preprint arXiv:2304.15010 ,\\n2023. 1, 2\\n[14] Amelia Glaese, Nat McAleese, Maja Trebacz, John\\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura\\nWeidinger, Martin Chadwick, Phoebe Thacker, et al. Im-\\nproving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375 , 2022. 2\\n[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\\nClark, et al. Training compute-optimal large language mod-\\nels.arXiv preprint arXiv:2203.15556 , 2022. 2\\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685 , 2021. 8, 12\\n[17] Mengqi Huang, Zhendong Mao, Zhuowei Chen, and Yong-\\ndong Zhang. Towards accurate image coding: Improved au-\\ntoregressive image generation with dynamic vector quantiza-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 22596–22605,\\n2023. 2\\n[18] Fangkai Jiao, Bosheng Ding, Tianze Luo, and Zhanfeng Mo.\\nPanda llm: Training data and evaluation for open-sourced\\nchinese instruction-following large language models. arXiv\\npreprint arXiv:2305.03025 , 2023. 2\\n[19] Taku Kudo and John Richardson. Sentencepiece: A\\nsimple and language independent subword tokenizer and\\ndetokenizer for neural text processing. arXiv preprint\\narXiv:1808.06226 , 2018. 2\\n[20] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and\\nWook-Shin Han. Autoregressive image generation using\\nresidual quantization. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n11523–11532, 2022. 2\\n[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[22] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-\\nand-vision assistant for biomedicine in one day. arXiv\\npreprint arXiv:2306.00890 , 2023.\\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\nBlip-2: Bootstrapping language-image pre-training with\\nfrozen image encoders and large language models. arXiv\\npreprint arXiv:2301.12597 , 2023. 1, 2\\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 1, 2\\n[25] Hao Liu, Wilson Yan, and Pieter Abbeel. Language quan-\\ntized autoencoders: Towards unsupervised text-image align-\\nment. arXiv preprint arXiv:2302.00902 , 2023. 1, 2, 6, 7, 8,\\n12, 13', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_8.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_9: [Document(page_content='[26] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai\\nSun, and Rongrong Ji. Cheap and quick: Efficient vision-\\nlanguage instruction tuning for large language models. arXiv\\npreprint arXiv:2305.15023 , 2023. 2\\n[27] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh\\nHajishirzi. Metaicl: Learning to learn in context. arXiv\\npreprint arXiv:2110.15943 , 2021. 2\\n[28] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike\\nLewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Re-\\nthinking the role of demonstrations: What makes in-context\\nlearning work? arXiv preprint arXiv:2202.12837 , 2022. 2\\n[29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse, Shantanu\\nJain, Vineet Kosaraju, William Saunders, et al. Webgpt:\\nBrowser-assisted question-answering with human feedback.\\narXiv preprint arXiv:2112.09332 , 2021. 2\\n[30] OpenAI. Gpt-4 technical report, 2023. 1\\n[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. Training lan-\\nguage models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems , 35:\\n27730–27744, 2022. 2\\n[32] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Gen-\\nerating diverse structure for image inpainting with hierarchi-\\ncal vq-vae. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 10775–\\n10784, 2021. 2\\n[33] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu\\nWei. Beit v2: Masked image modeling with vector-quantized\\nvisual tokenizers. arXiv preprint arXiv:2208.06366 , 2022. 2\\n[34] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\\nSutskever, et al. Improving language understanding by gen-\\nerative pre-training. 2018. 1, 2\\n[35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\\nAmodei, Ilya Sutskever, et al. Language models are unsu-\\npervised multitask learners. OpenAI blog , 1(8):9, 2019. 1,\\n2\\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748–8763. PMLR, 2021. 3, 4\\n[37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-\\nmon, Christopher D Manning, and Chelsea Finn. Direct\\npreference optimization: Your language model is secretly a\\nreward model. arXiv preprint arXiv:2305.18290 , 2023. 2\\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. Exploring the limits of transfer learning with\\na unified text-to-text transformer. The Journal of Machine\\nLearning Research , 21(1):5485–5551, 2020. 2\\n[39] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\\nating diverse high-fidelity images with vq-vae-2. Advances\\nin neural information processing systems , 32, 2019. 2[40] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring\\nmodels and data for image question answering. Advances in\\nneural information processing systems , 28, 2015. 7\\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\\net al. Photorealistic text-to-image diffusion models with deep\\nlanguage understanding. Advances in Neural Information\\nProcessing Systems , 35:36479–36494, 2022. 1\\n[42] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural\\nmachine translation of rare words with subword units. arXiv\\npreprint arXiv:1508.07909 , 2015. 2\\n[43] Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald,\\nRahul Gupta, Wael Hamza, Haidar Khan, Charith Peris,\\nStephen Rawls, Andy Rosenbaum, Anna Rumshisky, et al.\\nAlexatm 20b: Few-shot learning using a large-scale multi-\\nlingual seq2seq model. arXiv preprint arXiv:2208.01448 ,\\n2022. 2\\n[44] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\\nXuechen Li, Carlos Guestrin, Percy Liang, and Tat-\\nsunori B Hashimoto. Stanford alpaca: an instruction-\\nfollowing llama model (2023). URL https://crfm. stanford.\\nedu/2023/03/13/alpaca. html , 1(2):3. 2\\n[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste\\nRozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\\nLlama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 , 2023. 1, 2\\n[46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\\nLlama 2: Open foundation and fine-tuned chat models. arXiv\\npreprint arXiv:2307.09288 , 2023. 1, 2, 6\\n[47] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\\nlearning with frozen language models. Advances in Neural\\nInformation Processing Systems , 34:200–212, 2021. 2, 6, 7\\n[48] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\\nrepresentation learning. Advances in neural information pro-\\ncessing systems , 30, 2017. 2, 12\\n[49] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\\nQiao, et al. Visionllm: Large language model is also an\\nopen-ended decoder for vision-centric tasks. arXiv preprint\\narXiv:2305.11175 , 2023. 2\\n[50] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\\nZecheng Tang, and Nan Duan. Visual chatgpt: Talking,\\ndrawing and editing with visual foundation models. arXiv\\npreprint arXiv:2303.04671 , 2023. 1\\n[51] Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yux-\\niao Liu, Qian Wang, and Dinggang Shen. Doctorglm: Fine-\\ntuning your chinese doctor is not a herculean task. arXiv\\npreprint arXiv:2304.01097 , 2023. 2\\n[52] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\\nYaya Shi, et al. mplug-owl: Modularization empowers\\nlarge language models with multimodality. arXiv preprint\\narXiv:2304.14178 , 2023. 2', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_9.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_10: [Document(page_content='[53] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\\nand Yonghui Wu. Vector-quantized image modeling with\\nimproved vqgan. arXiv preprint arXiv:2110.04627 , 2021. 2\\n[54] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolf-\\ngang Macherey, Yanping Huang, David A Ross, Irfan Essa,\\nYonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic\\npyramid autoencoder for multimodal generation with frozen\\nllms. arXiv preprint arXiv:2306.17842 , 2023. 2, 3, 5, 6, 7,\\n8, 12, 13, 15\\n[55] Lijun Yu, Jos ´e Lezama, Nitesh B Gundavarapu, Luca Ver-\\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim\\nGupta, Xiuye Gu, Alexander G Hauptmann, et al. Language\\nmodel beats diffusion–tokenizer is key to visual generation.\\narXiv preprint arXiv:2310.05737 , 2023. 2\\n[56] Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shi-\\njian Lu. Regularized vector quantization for tokenized im-\\nage synthesis. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 18467–\\n18476, 2023. 2\\n[57] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\\nLlama-adapter: Efficient fine-tuning of language models\\nwith zero-init attention. arXiv preprint arXiv:2303.16199 ,\\n2023. 2\\n[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\\nformer language models. arXiv preprint arXiv:2205.01068 ,\\n2022. 2\\n[59] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and\\nXin Wang. Controllable text-to-image generation with gpt-\\n4.arXiv preprint arXiv:2305.18583 , 2023. 1\\n[60] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin,\\nYa Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Vi-\\nsual instruction tuning for medical visual question answer-\\ning. arXiv preprint arXiv:2305.10415 , 2023. 2\\n[61] Shanshan Zhong, Zhongzhan Huang, Weushao Wen, Jinghui\\nQin, and Liang Lin. Sur-adapter: Enhancing text-to-image\\npre-trained diffusion models with large language models. In\\nProceedings of the 31st ACM International Conference on\\nMultimedia , pages 567–578, 2023. 1\\n[62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 1, 2', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_10.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_11: [Document(page_content='A. More Implementation Details\\nGlobal Codebook Generation. To generate the global\\ncodebook, we introduce a two-phase process: (1) expand-\\ning the LLM vocabulary through the proposed vocabulary\\nexpansion technique (as shown in Figure 9); (2) applying\\na filtering strategy to further eliminate the entries with less\\nsemantic meaning.\\nWe use Tto represent the original LLM vocabulary and\\ndenote its size by N. To generate bigrams, for each t∈ T,\\nwe first input the concatenation of a text prefix (e.g., “a\\nphoto of”) and tinto the LLM. The LLM predicts the next\\nword in an auto-regressive manner. We record the top- M\\npredictions (where Mis 1 by default) with the highest con-\\nfidences, denoted as {t∗\\n1, . . . , t∗\\nM}. The bigrams for each\\nt∈ T are represented by {[t, t∗\\n1], . . . , [t, t∗\\nM]}. This pro-\\ncess is repeated for all subwords in the LLM vocabulary.\\nUltimately, we collect a set of bigrams, denoted as TBi,\\nwhich has a size of N×M. Similarly, we can build a tri-\\ngram set TTriby feeding each bigram in TBiinto the LLM\\nfor next-word prediction. The resulting TTrihas a size of\\nN×M×M. We use {T,TBi,TTri}to represent the ex-\\npanded LLM vocabulary.\\nFor the filtering process, we compute the CLIP similari-\\nties between each image in the training set and every entry\\nin the expanded LLM vocabulary {T,TBi,TTri}. We then\\nrecord the top-5 entries with the highest similarity scores\\nfor each image. Finally, we aggregate these entries from all\\nimages to form the final expanded LLM vocabulary, which\\nserves as our global codebook TE.\\nEncoder and Decoder Structures. Figure 10 details the\\nimplementation of our V2L Tokenizer’s local encoder and\\ndecoder. Specifically, the local encoder shares the same ba-\\nsic structure as VQ-GAN [12], utilizing four residual blocks\\nwith channel dimensions [128, 256, 256, 512] to downsam-\\nple the input image by a factor of 8. Similarly, our de-\\ncoder mirrors the encoder’s structure, employing four resid-\\nual blocks with channel dimensions [512, 256, 256, 128] to\\nupsample the image back to its original resolution. We inte-\\ngrate the information from global tokens into the decoding\\nprocess through a cross-attention layer, which is added be-\\nfore the self-attention layer in the nonlocal block.\\nVector Quantization Loss. The proposed V2L Tok-\\nenizer requires optimization of the encoder, the decoder\\nand the projector. Thus, we follow VQ-V AE [48] and VQ-\\nGAN [12] to implement our vector quantization loss, utiliz-\\ning a straight-through gradient estimator for optimization:\\nLvq=||X−ˆX||2+||sg(F)−ˆF||+β||sg(ˆF)−F||\\nwhere sg(·)denotes the stop-gradient operation. Note that\\nour method involves a trainable projector to produce code-\\nbook embeddings. Thus, unlike LQAE [25] and SPAE [54],\\nthe second term in the above equation is also necessary. We\\nsetβto 0.3.Tuning LLaMA-2 with the V2L Tokenizer. To enhance\\nthe image generation task, we propose to fine-tune an LLM\\nmodel. This process begins with the V2L Tokenizer gen-\\nerating both global and local tokens for the training im-\\nages. Subsequently, the global tokens are employed as\\na “text prefix”. We then concatenate these global tokens\\nwith the local tokens and input them into the LLM. The\\nauto-regression loss is applied only to the local tokens.\\nDue to resource limitations, we fine-tune a 7B LLaMA-2\\nmodel using LoRA [16] on 12 randomly selected classes\\nfrom ImageNet training dataset over 100K iterations using\\n32×NVIDIA V100 GPUs. LoRA weights are integrated\\ninto the query and key projection matrixes, with the hyper-\\nparameter setting of r= 4,α= 32 . For optimization, we\\nuse Adam optimizer, starting with a learning rate of 3e−4.\\nThis rate undergoes half-cycle cosine decay after a 5-epoch\\nlinear warm-up phase. Consequently, the tuned model is\\nable to predict masked tokens in an auto-regressive man-\\nner. The predicted token map is input into the decoder of\\nthe V2L tokenizer to generate the reconstructed image, as\\ndemonstrated in Section 4.3 of our main paper.\\nB. More Ablation Studies\\nVocabulary Expansion. We study the effectiveness of the\\nproposed vocabulary expansion strategy on the 5-way-K-\\nshot Mini-ImageNet classification benchmark. Our studies\\ninclude three scenarios: utilizing the original LLM vocabu-\\nlary without expansion (Subword), applying bigram expan-\\nsion (Bigram), and employing trigram expansion (Trigram).\\nThe results of these scenarios are detailed in Table 5. The\\nbigram expansion approach surpasses the non-expansion\\nmethod by an average accuracy increase of +13.5 and +9.3\\npoints with 5 and 21 global tokens, respectively. Imple-\\nmenting trigram expansion further elevates the average ac-\\ncuracy to 83.9 and 86.7. The findings demonstrate that em-\\nploying vocabulary expansion significantly improves the se-\\nmantic richness of the terms in the expanded LLM vocabu-\\nlary, leading to enhanced classification accuracy.\\nEmbeddings of Local Codebook. As shown in Figure 2 of\\nthe main paper, we introduce a trainable projector to project\\nthe LLM embeddings into a visual space, which enhances\\nreconstruction quality. Table 6 presents our investigation of\\nvarious LLM embeddings, including the default projected\\nLLM embeddings (P-LLaMA-2), the original LLM embed-\\ndings (LLaMa-2), and those produced by the CLIP-text-\\nencoder (CLIP). We observe that utilizing the CLIP text\\nencoder for extracting language embeddings significantly\\nboosts the quality of reconstruction. This improvement\\nlikely stems from the CLIP model’s inherent alignment be-\\ntween linguistic and visual spaces. By introducing a train-\\nable projector, this alignment is further refined, leading to\\nsuperior reconstruction performance.\\nDenoising Step and Condition Length. As shown in Fig-', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_11.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_12: [Document(page_content='+Prefix v𝑡!𝑡\"𝑡#𝑡$[v, 𝑡!][v, 𝑡\"][v, 𝑡#][v, 𝑡$]\\nLLMLLM\\nVocabularyLLM\\nSubwordsBigrams Trigrams\\n…𝑁×𝑀𝑁𝑁×𝑀×𝑀\\n…[𝑡!, 𝑡!(][𝑡\", 𝑡\")][𝑡#, 𝑡(*][𝑡$, 𝑡+!]…[v, 𝑡!, 𝑡!(][v, 𝑡\", 𝑡\")][v, 𝑡#, 𝑡(*][v, 𝑡$, 𝑡+!]…[𝑡!, 𝑡!(, 𝑡,\"][𝑡\", 𝑡\"), 𝑡!\"][𝑡#, 𝑡(*, 𝑡,)][𝑡$, 𝑡+!, 𝑡\"(]…+Prefix vTop-𝑀PredictionsTop-𝑀PredictionsFigure 9. Illustration of the vocabulary expansion strategy. In this figure, we set M= 1for illustrative purposes. The prefix vcorresponds\\nto the text phrase “a photo of”.\\n4×Residual \\nBlockConvolutional \\nBlockDownsample\\nBlockResidual \\nBlock\\nNonlocal  \\nBlock\\nResidual \\nBlockGroup\\nNormalization\\nConvolutional\\nBlockLocal Feature s\\nConvolutional\\nBlock\\nResidual \\nBlockNonlocal  \\nBlock\\nResidual \\nBlockSwish\\nActivation\\nResidual \\nBlock\\nDownsample\\nBlockCross\\nAttentionGroup\\nNormalization\\nConvolutional\\nBlockSwish\\nActivation\\nLocal Quantizer\\nLocal Token sGlobal Tokens\\n…Local Encoder\\nDecoderInput Image\\nReconstruction4×\\nFigure 10. Illustration of the local encoder and the decoder of our V2L Tokenizer.\\nure 4 of the main paper, we denoise mmasked tokens at a\\ntime using ntokens preceding them for the inpainting task,\\nwhere mandndenote denoising step and condition length,\\nrespectively. We vary the values of mandnand report the\\nFID scores for inpainting task in Figure 11. As the denois-\\ning step increases, the performance decreases. Addition-\\nally, an excessively long condition length leads to subop-\\ntimal performance since the LLM struggles to handle the\\ncomplex context of a new “foreign language” in the visual\\nmodality.\\nC. More Qualitative Results\\nSemantic Interpretation. We provide qualitative results\\nfor semantic interpretation in Figure 6 of the main paper.\\nHere, we show additional visualizations in Figure 12.\\nImage Captioning and Visual Question Answering. Fig-\\nure 5 of the main paper visualizes the results of image cap-\\ntioning and VQA. In Figures 13 and 14, we compare ourapproach with SAPE [54] using additional samples. Our\\nmodel consistently generates more reasonable image cap-\\ntions and provides more accurate answers.\\nImage Reconstruction. In Table 3 of the main paper, we\\nreport the quantitative results for reconstruction evaluation.\\nIn this study, we show several qualitative visualizations. In\\nFigure 15, we compare our approach with VQ-GAN [12],\\nLQAE [25] and SPAE [54]. Our approach is notable for its\\nability to reconstruct images with a high level of detail.\\nImage Denoising. We show visualizations for image de-\\nnoising in Figure 7 of the main paper. Here, we provide\\nextra visualizations for inpainting (Figure 16), outpaint-\\ning (Figure 17), deblurring (Figure 18), rotation restoration\\n(Figure 19) and shift restoration (Figure 20).', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_12.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_13: [Document(page_content='Task Induction: ✓ ✓ ✓ ✓ ✓ ✓\\nMethod #Tokens Inner-shot: 1 1 3 5 1 1 1 Avg\\nRepeats: 0 0 0 0 1 3 5\\nSubword\\n5 LLaMA-2 (70B)31.8 65.6 82.8 85.6 68.8 69.9 69.3 67.7\\nBigram 40.6 83.1 91.7 92.6 86.5 87.0 86.9 81.2\\nTrigram 41.7 87.1 94.8 96.1 88.9 89.2 89.1 83.9\\nSubword\\n21 LLaMA-2 (70B)34.3 74.1 90.1 91.8 79.6 80.2 80.7 75.8\\nBigram 44.8 84.1 95.0 95.5 91.6 92.3 92.5 85.1\\nTrigram 46.5 89.1 96.9 97.8 91.4 92.7 92.9 86.7\\nTable 5. Ablation study for the proposed vocabulary expansion strategy on the 5-way-K-shot Mini-ImageNet classification benchmark.\\nV ocabulary Embedding FID ↓LPIPS↓PSNR↑\\nLLaMA-2 LLaMA-2 9.51 0.17 21.48\\nLLaMA-2 CLIP 4.58 0.11 23.58\\nLLaMA-2 P-LLaMA-2 3.41 0.08 23.56\\nTable 6. Ablation study on various LLM embeddings. We report\\nresults on ImageNet-1K val set.\\n(a) FID score v.s. denoising step.\\n(b) FID score v.s. condition length.\\nFigure 11. Ablation study on the denoising step (m) and the condi-\\ntion length (n) for the image inpainting task, using a 7B LLaMA-2.', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_13.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_14: [Document(page_content='tringaguttersandnectarsandlittoralisshearlingsheepskinsCoatcoatSatinwoodCabinetofKenilwoodcabinetofabeltsused動物園(Zoo)vaquitaSirenian定食(set meal)素食(vegetarian)料理部(restaurant)ActiveRecordfoulingbasketballgamePacersinbasketballplayerLoboIrishWolfgreywolfweerwolfeSkunkCSkunkSkunkTrainSkunkRiverFloridaKeetavingaBirdumbercoloredaglephorusCastIronpanPanZwillingFigure 12. More visualizations for semantic interpretation.\\nA man sits on the couch with his dog on his lap\\nA woman in a red shirt and black pants is runningA skateboarder is doing a trick with his skateboard\\nA vintage train car with a chandelierA large pizza with a pile of cheese on top\\nA person holding a slice of bread in a bowlA bed with curtains hanging over it in a bedroom\\nA large pool in front of a house\\nA group of zebras grazing in the grass\\nA large group of people gathered around a table\\nA messy bedroom with a queen size bed and a wooden floor\\nA large group of people gathered around a table\\nA herd of elephants is walking in the grass\\nA herd of cows are grazing on a field of grassA football match with a player running with the ball\\nA person holding a horse in front of a barn\\nFigure 13. Visualizations for image caption. Blue: ours. Orange: SPAE [54] (re-implementation).\\nQ1: What color is the sign?\\nOurs: red SPAE: red\\nQ2: What does the red sign say?\\nOurs: stop SPAE: No\\nQ3: What would a person park here?\\nOurs: car SPAE: a\\nQ1: Is this an adult party?\\nOurs: no SPAE: yes\\nQ3: What is being celebrated ?\\nOurs: birthday SPAE: ChineseQ2: Who is in front of the cake with candles?\\nOurs: mom SPAE: boy\\nQ1: What sport is being played?\\nOurs: baseball SPAE: tennis\\nQ2: What is the name of the teams?\\nOurs: Cubs SPAE: Barcelona\\nQ3: Is the catcher wearing safety gear?\\nOurs: yes SPAE: yesQ1: What type of animal is shown?\\nOurs: elephant  SPAE: raccoon\\nQ3: What kind of coat does the animal have?\\nOurs: fur SPAE: fur\\nQ2: How many animals are there?\\nOurs: 2 SPAE: 2\\nFigure 14. Visualizations for visual question answering. Blue: ours. Orange: SPAE [54] (re-implementation).', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_14.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_15: [Document(page_content='InputVQ-GANLQAESPAEOursInputVQ-GANLQAESPAEOursFigure 15. Visualizations for image reconstruction.\\nContexts\\nInputVQ-GANLQAESPAEOurs\\nFigure 16. Visualizations for image inpainting.\\nInputVQ-GANLQAESPAEOursContexts\\nFigure 17. Visualizations for image outpainting.', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_15.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_16: [Document(page_content='InputVQ-GANLQAESPAEOursContextsFigure 18. Visualizations for image deblurring.\\nInputVQ-GANLQAESPAEOursContexts\\nFigure 19. Visualizations for rotation restoration.\\nInputVQ-GANLQAESPAEOurs\\nContexts\\nFigure 20. Visualizations for shift restoration.', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_16.pdf', 'page': 0})]\n",
      "doc_2403_07874_pg_17: pdfs/2403_07874/single_page_pdfs/doc_2403_07874_pg_17\n",
      "doc_variables: None\n",
      "doc_2403_07874_pg_0: [Document(page_content='Beyond Text: Frozen Large Language Models in Visual Signal Comprehension\\nLei Zhu1Fangyun Wei2*Yanye Lu1\\n1Peking University2Microsoft Research Asia\\nzhulei@stu.pku.edu.cn fawe@microsoft.com yanye.lu@pku.edu.cn\\nAbstract\\nIn this work, we investigate the potential of a large\\nlanguage model (LLM) to directly comprehend visual sig-\\nnals without the necessity of fine-tuning on multi-modal\\ndatasets. The foundational concept of our method views\\nan image as a linguistic entity, and translates it to a set\\nof discrete words derived from the LLM’s vocabulary. To\\nachieve this, we present the Vision-to-Language Tokenizer,\\nabbreviated as V2T Tokenizer, which transforms an image\\ninto a “foreign language” with the combined aid of an\\nencoder-decoder, the LLM vocabulary, and a CLIP model.\\nWith this innovative image encoding, the LLM gains the\\nability not only for visual comprehension but also for im-\\nage denoising and restoration in an auto-regressive fash-\\nion—crucially, without any fine-tuning. We undertake rig-\\norous experiments to validate our method, encompassing\\nunderstanding tasks like image recognition, image cap-\\ntioning, and visual question answering, as well as image\\ndenoising tasks like inpainting, outpainting, deblurring,\\nand shift restoration. Code and models are available at\\nhttps://github.com/zh460045050/V2L-Tokenizer.\\n1. Introduction\\nSignificant advancements have been achieved in the field\\nof natural language processing (NLP) through the deploy-\\nment of large language models (LLMs), such as GPT [3,\\n30, 34, 35], PaLM [2, 6] and LLaMA [45, 46]. In pursuit\\nof addressing intricate challenges necessitating the combi-\\nnation of text and visual understanding, scholars are broad-\\nening the capacities of the off-the-shelf LLMs. This en-\\nhancement involves the incorporation of additional visual\\nprocessing components that facilitate the understanding of\\nvisual content [13, 23–25, 62] or the generation of images\\nfrom text [41, 50, 59, 61]. Subsequently, these improved\\nmodels undergo an extra re-training or fine-tuning using\\nvarious multi-modal datasets to align the visual latent space\\nwith the language latent space. Nevertheless, the refinement\\nprocess generally requires a substantial amount of training\\nresources.\\n*Corresponding author.\\nV2L\\nTokenizer\\nLLMdeb\\nug爪 G Link\\nAll cat Apす\\n好īna toBro\\nwn\\nEaね Eye LV ocabulary\\nLLM\\nImage Recognition                 VQA             Image Caption\\nInpainting      Outpainting       Deblur     Image RestorationFigure 1. Illustration of our V2L Tokenizer (Vision-to-Language\\nTokenizer). The V2L Tokenizer translates an image into a col-\\nlection of interpretable tokens derived from an LLM vocabulary.\\nSubsequently, the frozen LLM can comprehend the visual sig-\\nnals and perform multi-modal understanding tasks (highlighted in\\nBlue) and image denoising tasks (highlighted in Orange) without\\nthe necessity of fine-tuning.\\nAs illustrated in Figure 1, this work aims to equip a\\nlarge language model with the innate ability to comprehend\\nvisual signals, importantly, without the necessity of fine-\\ntuning. In our approach, we view each image as a linguis-\\ntic entity derived from a “foreign language”, adapting it to\\nsuit the input requirements of a plain LLM. Consequently,\\nthis alignment occurs in the input (token) space rather than\\nin the feature space, distinguishing our work from previ-\\nous multi-modal methodologies [1, 23, 24, 62] that require\\nfine-tuning for modality alignment. Thus, the fine-tuning or\\nre-training process on multi-modal datasets is avoidable in\\nour methodology. Our technique translates an image into\\na collection of discrete tokens that are within the vocabu-\\nlary of the LLM. Once translated, these tokens can be fed\\ninto the LLM, enabling it to process and comprehend visual\\ninformation, thereby facilitating a range of tasks involving\\nboth image understanding and denoising.\\nTranslating an image into a set of tokens that a frozen\\nLLM can understand is challenging. In this work, we intro-\\nduce a tokenizer designed to map images (a non-linguistic\\nmodality) to the input (token) space of a frozen LLM. This\\ntokenizer is termed the Vision-to-Language Tokenizer, or\\nV2L Tokenizer in brief. Drawing inspiration from the tri-\\numphant advances of VQ-GAN [12], the V2L TokenizerarXiv:2403.07874v1  [cs.CV]  12 Mar 2024', metadata={'source': 'pdfs/2403_07874/single_page_pdfs/2403_07874_0.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "folder_pdfs = \"pdfs\"\n",
    "arxiv_id = \"2403.07874\"\n",
    "\n",
    "# Call the function to create the folder\n",
    "create_folder(folder_pdfs)\n",
    "\n",
    "\n",
    "arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "# pdf_file_path = os.path.join(pdf_path, f\"{arxiv_name}.pdf\")\n",
    "\n",
    "\n",
    "def split_pdf_into_pages(pdf_path, arxiv_name):\n",
    "    # Create the directory if it doesn't exist\n",
    "    folder_single_pdfs = \"single_page_pdfs\"\n",
    "\n",
    "    pdf_file_path = f\"{pdf_path}/{arxiv_name}.pdf\"\n",
    "\n",
    "    single_page_pdf_path = os.path.join(pdf_path, folder_single_pdfs)\n",
    "    os.makedirs(single_page_pdf_path, exist_ok=True)\n",
    "\n",
    "    # Open the PDF file\n",
    "    with open(pdf_file_path, \"rb\") as pdf_file:\n",
    "        reader = PdfReader(pdf_file_path)\n",
    "        \n",
    "        # Iterate through each page and save it as a separate PDF file\n",
    "        for i in range(len(reader.pages)):\n",
    "            writer = PdfWriter()\n",
    "            writer.add_page(reader.pages[i])\n",
    "            \n",
    "            output_path = os.path.join(single_page_pdf_path, f\"{arxiv_name}_{i}.pdf\")\n",
    "            with open(output_path, \"wb\") as output_file:\n",
    "                writer.write(output_file)\n",
    "            loader = PyPDFLoader(output_path)\n",
    "            globals()[f\"doc_{arxiv_name}_pg_{i}\"] = loader.load()\n",
    "\n",
    "    print(\"PDF split into separate pages successfully!\")\n",
    "    return single_page_pdf_path\n",
    "\n",
    "single_page_pdf_path = split_pdf_into_pages(pdf_path, arxiv_name)\n",
    "single_page_pdf_path\n",
    "\n",
    "# pdf_file_path = f\"{pdf_path}/{arxiv_name}.pdf\"\n",
    "\n",
    "# pdf_file_path\n",
    "\n",
    "for var_name, var_value in globals().items():\n",
    "    if var_name.startswith(\"doc_\"):\n",
    "        print(f\"{var_name}: {var_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0be1e1d-0612-4017-a5e4-23b23096527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_2403_07874_pg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4456f79a-d28d-47af-8504-cd835f6e6d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4968"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_2403_07874_pg_6[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03c5da37-9b4f-4fac-9431-e2e3361b5fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_6': 'flavoredcoffeeespressos\\nnespressos\\nlungoconvinencestorePrimaMarkets\\nButchers\\nrefreshmentstand\\nExtremeSportsBunnyHop\\nbalancewheelin\\nspecializedtraining\\nRemoteDeskministation\\ncomputerdesks\\ncomputerdesksglazedpotceramist\\nRESSMUG\\nmug\\nCetoniaphaiston\\nEmeraldIs\\nchaferlar\\nFigure 6. Visualization for semantic interpretation.\\nMethod Codebook #Tokens CLIP ↑ CLIP-R ↑\\nSPAE [54] PaLM-2 5 0.1868 0.7147\\nOurs E-LLaMA-2 5 0.2576 0.9165\\nSPAE [54] PaLM-2 21 0.1815 0.6901\\nOurs E-LLaMA-2 21 0.2427 0.8520\\nTable 2. Semantic quality evaluatoin on ImageNet-1K val set. E-\\nLLaMA-2: expanded LLaMa-2 vocabulary.\\nrect only if all the generated tokens match the tokens of the\\nactual category name.\\nTable 1 shows the comparison between our approach em-\\nploying different LLaMa 2 model configurations, and prior\\nworks including LQAE [25], SPAE [54], and a baseline us-\\ning a frozen language model for multimodal few-shot learn-\\ning [47]. We examine various factors that could influence\\nN-way K-shot classification, including: (1) the value of\\nN; (2) the value of K; (3) task induction, defined as spec-\\nifying the particular N-way categories in the prompt; (4)\\nthe frequency of repetitions for each few-shot sample. We\\nhave two main observations: (1) Our model surpasses the\\npreviously best approach, SPAE [54], across all scenarios,\\ndespite using smaller LLMs (our 13B/70B LLaMa 2 ver-\\nsus SPAE’s 340B PaLM-2) and a more compact vocabulary\\n(our 11,908 versus SPAE’s 65,000); (2) The performance of\\nour model improves as the number of tokens used to rep-\\nresent the image increases. This can be attributed to the\\nintroduction of the vocabulary expansion, which generates\\na larger pool of semantically relevant token candidates.\\nImage Caption and Visual Question Answering. Fol-\\nlowing SPAE [54], we randomly select 10 image-caption\\npairs (or image-question-answer triplets) from COCO Cap-\\ntion [4] (or VQA [40]) training set to form the in-context\\nlearning samples in the image caption (or VQA) prompt, as\\ndescribed in Section 3.3. By default, we utilize 21 global\\ntokens to represent an image. The visualization results are\\npresented in Figure 5. Refer to supplementary materials for\\nmore results.\\nSemantic Interpretation. Figure 6 visualizes the top four\\nglobal tokens with the highest similarity scores for a set ofMethod Codebook #Tokens FID ↓LPIPS↓PSNR↑\\nVQ-GAN [12] Learnable 256 5.48 0.13 -\\nVQ-GAN [12] PaLM-2 256 7.44 0.17 -\\nVQ-GAN∗[12] LLaMA-2 256 9.51 0.17 21.48\\nSPAE [54] PaLM-2 341 9.49 0.17 -\\nSPAE [54] PaLM-2 597 4.41 0.12 -\\nSPAE [54] PaLM-2 1109 3.89 0.11 -\\nOurs LLaMA2 256 3.41 0.08 23.56\\nOurs Hybrid 277 2.88 0.08 23.25\\nTable 3. Reconstruction evaluation on ImageNet-1K val set. Hy-\\nbrid: local tokens (256) and global tokens (21) are derived from the\\nlocal codebook (LLaMA-2) and the global codebook (E-LLaMa-\\n2), respectively. *: re-implementation.\\nsix images chosen at random. Our vocabulary expansion\\ntechnique effectively increases the range of semantically\\npertinent token options (i.e. bigrams and trigrams). Extra\\nresults are available in the supplementary materials.\\nIn Table 2, we also quantitatively evaluate the seman-\\ntic quality of our global tokens, and compare the semantic\\nquality with SPAE [54] on ImageNet-1K validation set, us-\\ning the CLIP score and the relative CLIP score (CLIP-R),\\nwhich assess the degree of alignment between each image\\nand its associated language tokens. We observe consistent\\nimprovements over SAPE, despite SAPE utilizing a larger\\nvocabulary (SPAE’s 65,000 versus our 11,908).\\n4.3. Image Reconstruction and Denoising\\nReconstruction Evaluation. Our V2L Tokenizer encodes\\nan image into a set of local tokens derived from an LLM\\nvocabulary. These encoded tokens should capture the most\\nmeaningful information, enabling the decoder to recon-\\nstruct the original image and restore any degraded (“pol-\\nlutional”) images. In this study, we evaluate the reconstruc-\\ntion quality of our V2L Tokenizer using metrics including\\nFID, LPIPS, and PSNR. As shown in Table 3, we com-\\npare our approach with SPAE [54] and VQ-GAN [12] on\\nthe ImageNet-1K validation set. In our approach, we ex-\\nplore two distinct setups: (1) employing the decoder from\\nVQ-GAN without the involvement of global tokens; (2) uti-\\nlizing the proposed decoder, which incorporates extra Kg\\nglobal tokens for the decoding process (default configura-\\ntion as discussed in Section 3.2). Our approach outperforms\\nSPAE [54] across all metrics.\\nImage Denoising. We introduce the prompts used for in-\\npainting, outpainting, deblurring, shift and rotation restora-\\ntions, along with the process of restoring polluted images,\\nas shown in Figure 4. In Table 4, we study two factors im-\\npacting the quality of these five in-context image denois-\\ning tasks: (1) the image tokenizer, which encodes an image\\ninto a set of tokens; (2) the LLM, which aims to predict the\\nlocal tokens of the original images given the tokens of the\\npolluted images, with the aid of in-context learning samples',\n",
       " 'page_summary': 'Overall, our approach demonstrates superior performance in various tasks such as N-way K-shot classification, image captioning, visual question answering, semantic interpretation, image reconstruction, and image denoising. We outperform existing methods like SPAE and VQ-GAN across different evaluation metrics on the ImageNet-1K validation set. Our model benefits from a compact vocabulary and the introduction of vocabulary expansion, which enhances the semantic relevance of generated tokens.\\n\\nIn image captioning and visual question answering tasks, we leverage in-context learning samples to improve model performance. The visualization results show the effectiveness of our approach in generating relevant tokens for image representation. Additionally, our semantic interpretation results demonstrate the ability of our model to capture meaningful information from images.\\n\\nWhen it comes to image reconstruction and denoising, our V2L Tokenizer effectively encodes images into local tokens, enabling high-quality reconstruction and restoration of degraded images. We outperform existing methods like SPAE and VQ-GAN in terms of FID, LPIPS, and PSNR metrics, showcasing the robustness of our approach in image restoration tasks.\\n\\nOverall, our approach showcases the effectiveness of leveraging multimodal few-shot learning and vocabulary expansion techniques for various computer vision tasks. The results highlight the potential of our model in advancing the field of multimodal AI and improving the quality of image understanding and processing.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = get_env_variables()\n",
    "\n",
    "page_6 = doc_2403_07874_pg_6[0].page_content\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=key['OPENAI_API_KEY']) \n",
    "\n",
    "# This is an LLMChain to write a follow-up edit given the restaurant outline.\n",
    "prompt_page_6 = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "    - for each paragraph in {page_6}, condense to a concise sentence.\n",
    "    \"\"\"\n",
    ")\n",
    "chain_page_6 = LLMChain(llm=llm, prompt=prompt_page_6, output_key=\"page_summary\")\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_page_6],\n",
    "    input_variables=[\"page_6\"],\n",
    "    output_variables=[\"page_summary\"],\n",
    ")\n",
    "\n",
    "result = overall_chain({\"page_6\": doc_2403_07874_pg_6[0].page_content})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3c4c6af-fb2a-4898-961f-0cd32a42f3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overall, our approach demonstrates superior performance in various tasks such as N-way K-shot classification, image captioning, visual question answering, semantic interpretation, image reconstruction, and image denoising. We outperform existing methods like SPAE and VQ-GAN across different evaluation metrics on the ImageNet-1K validation set. Our model benefits from a compact vocabulary and the introduction of vocabulary expansion, which enhances the semantic relevance of generated tokens.\\n\\nIn image captioning and visual question answering tasks, we leverage in-context learning samples to improve model performance. The visualization results show the effectiveness of our approach in generating relevant tokens for image representation. Additionally, our semantic interpretation results demonstrate the ability of our model to capture meaningful information from images.\\n\\nWhen it comes to image reconstruction and denoising, our V2L Tokenizer effectively encodes images into local tokens, enabling high-quality reconstruction and restoration of degraded images. We outperform existing methods like SPAE and VQ-GAN in terms of FID, LPIPS, and PSNR metrics, showcasing the robustness of our approach in image restoration tasks.\\n\\nOverall, our approach showcases the effectiveness of leveraging multimodal few-shot learning and vocabulary expansion techniques for various computer vision tasks. The results highlight the potential of our model in advancing the field of multimodal AI and improving the quality of image understanding and processing.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['page_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e8589-2452-4334-b6c8-a40ce2f7ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the steps as templates and keys\n",
    "steps = [\n",
    "    {\n",
    "        \"template\": f\"\"\"\n",
    "        - for each paragraph in {{\"page_{i}\"}}, condense to a concise sentence.\n",
    "        \"\"\",\n",
    "        \"input_variable\": f\"doc_{arxiv_name}_pg_{i}\",\n",
    "        \"output_variable\": f\"page_summary_{i}\",\n",
    "    }\n",
    "    for i in range(1, num_pages + 1)  # num_pages is the total number of pages\n",
    "]\n",
    "\n",
    "# Create LLMChain objects for each step\n",
    "chains = []\n",
    "for step in steps:\n",
    "    prompt = PromptTemplate.from_template(template=step[\"template\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, output_key=step[\"output_variable\"])\n",
    "    chains.append(chain)\n",
    "\n",
    "# Create SequentialChain with all LLMChain objects\n",
    "overall_chain = SequentialChain(\n",
    "    chains=chains,\n",
    "    input_variables=[step[\"input_variable\"] for step in steps],\n",
    "    output_variables=[step[\"output_variable\"] for step in steps],\n",
    ")\n",
    "\n",
    "# Execute the overall_chain for each page\n",
    "for i in range(1, num_pages + 1):\n",
    "    page_content_variable = f\"doc_{arxiv_name}_pg_{i}\"\n",
    "    result = overall_chain({page_content_variable: page_contents[i - 1]})\n",
    "    # Do something with the result\n",
    "    # For example, you can access result[step[\"output_variable\"]] to get the page summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
