{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e445fe3-e5e0-4080-9a78-6570bfae7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip, AudioFileClip, concatenate_videoclips\n",
    "from pymilvus import Milvus, MilvusClient, IndexType, connections, utility\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from moviepy.editor import concatenate_videoclips\n",
    "from langchain.prompts import PromptTemplate\n",
    "from moviepy.config import change_settings\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pdf2image import convert_from_path\n",
    "from milvus import default_server\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import feedparser\n",
    "import requests\n",
    "import base64\n",
    "import pprint\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b35aae-ddeb-4c2b-acbe-4529e441a6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "change_settings({\"FFMPEG_BINARY\": \"/opt/homebrew/bin/ffmpeg\"})\n",
    "# Set up a Milvus client\n",
    "default_server.start()\n",
    "host=\"127.0.0.1\"\n",
    "connections.connect(host=host, port=default_server.listen_port)\n",
    "port=default_server.listen_port\n",
    "my_uri = \"http://localhost:\" + str(port)\n",
    "print(my_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8361215-27c0-4617-bba1-d4e41bcd7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_variables():\n",
    "    \"\"\"Fetch all necessary configurations from environment variables.\"\"\"\n",
    "    return {\n",
    "        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),\n",
    "        'ELEVEN_LABS_API_KEY': os.getenv('ELEVEN_LABS_API_KEY')\n",
    "    }\n",
    "\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"The folder '{folder_name}' has been created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{folder_name}' already exists.\")\n",
    "\n",
    "\n",
    "def arxiv_id_from_url(url):\n",
    "    # Extract the arXiv ID from the URL using a regular expression\n",
    "    match = re.search(r'arxiv\\.org/pdf/(\\d+\\.\\d+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def download_and_save_pdf(url, folder_pdfs):\n",
    "    \"\"\"\n",
    "    Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The arXiv.org URL of the paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: ArXiv ID of the downloaded paper if successful, or an error message.\n",
    "    \"\"\"\n",
    "    # Extract arXiv ID from the URL\n",
    "    arxiv_id = arxiv_id_from_url(url)\n",
    "\n",
    "    # Check if a valid arXiv ID was extracted\n",
    "    if arxiv_id:\n",
    "        try:\n",
    "            # Make a request to the arXiv API\n",
    "            feed = feedparser.parse(f'http://export.arxiv.org/api/query?id_list={arxiv_id}')\n",
    "\n",
    "            # Check if the response contains entries\n",
    "            if 'entries' in feed:\n",
    "                # Iterate over each entry (paper) in the feed\n",
    "                for entry in feed.entries:\n",
    "                    # Extract the PDF link from the entry\n",
    "                    pdf_link = entry.link.replace('/abs/', '/pdf/') + '.pdf'\n",
    "\n",
    "                    # Download the PDF\n",
    "                    response = requests.get(pdf_link)\n",
    "\n",
    "                    # Save the PDF in the local directory with the name based on the arXiv ID\n",
    "                    with open(f'{folder_pdfs}/{arxiv_id}.pdf', 'wb') as pdf_file:\n",
    "                        pdf_file.write(response.content)\n",
    "\n",
    "                    print(f\"\\nPDF downloaded and saved as {arxiv_id}.pdf\")\n",
    "                    return arxiv_id\n",
    "\n",
    "            else:\n",
    "                return f\"\\nNo entries found for arXiv ID {arxiv_id}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"\\nError extracting information: {e}\"\n",
    "    else:\n",
    "        return \"Invalid arXiv PDF URL format. Please enter a valid URL.\"\n",
    "\n",
    "\n",
    "def download_and_initialize_embedding_model(model_name=\"WhereIsAI/UAE-Large-V1\", device=None):\n",
    "    \"\"\"\n",
    "    Download and initialize the Sentence Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the Sentence Transformer model to download.\n",
    "    - device (str or torch.device): The device to use for the model (e.g., 'cuda:3' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - encoder (SentenceTransformer): The initialized Sentence Transformer model.\n",
    "    - EMBEDDING_DIM (int): The embedding dimension of the model.\n",
    "    - MAX_SEQ_LENGTH (int): The maximum sequence length.\n",
    "\n",
    "    Example usage:\n",
    "    encoder, EMBEDDING_DIM, max_seq_length = download_and_initialize_embedding_model()\n",
    "    \"\"\"\n",
    "    # Initialize torch settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    DEVICE = torch.device(device) if device else torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\ndevice: {DEVICE}\")\n",
    "\n",
    "    # Load the model from the Hugging Face model hub\n",
    "    encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "    print(f\"\\nDatatype of SentenceTransformer encoded object{type(encoder)}\\n\")\n",
    "    print(f\"\\nWhat the encoder object looks like: {encoder}\\n\")\n",
    "\n",
    "    # Get the model parameters and save for later\n",
    "    EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "    MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length()\n",
    "    # Assume tokens are 3 characters long\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "    HF_EOS_TOKEN_LENGTH = 1 * 3\n",
    "    # Test with 512 sequence length\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "    HF_EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "    # Inspect model parameters\n",
    "    print(f\"\\nmodel_name: {model_name}\")\n",
    "    print(f\"\\nEMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "    print(f\"\\nMAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "    return encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH\n",
    "\n",
    "\n",
    "def create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M=16, uri=my_uri):\n",
    "    \"\"\"\n",
    "    Create a no-schema Milvus collection and define the database index.\n",
    "\n",
    "    Parameters:\n",
    "    - uri (str): The URI of the Milvus server.\n",
    "    - COLLECTION_NAME (str): The name of the Milvus collection.\n",
    "    - EMBEDDING_DIM (int): The dimension of the embedding vectors.\n",
    "    - M (int): The maximum number of graph connections per layer for the HNSW index. Default is 16.\n",
    "\n",
    "    Returns:\n",
    "    - milvus_client (Milvus): The Milvus client instance.\n",
    "\n",
    "\n",
    "    Example usage:\n",
    "    my_uri = \"tcp://127.0.0.1:19530\"\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    my_EMBEDDING_DIM = 1024\n",
    "    \n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M=16, uri=my_uri)\n",
    "    \"\"\"\n",
    "    # Add custom HNSW search index to the collection.\n",
    "    # M = max number graph connections per layer. Large M = denser graph.\n",
    "    # Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "    # M = 16\n",
    "    # efConstruction = num_candidate_nearest_neighbors per layer. \n",
    "    # Use Rule of thumb: int. 8~512, efConstruction = M * 2.\n",
    "    efConstruction = M * 2\n",
    "    index_params = {\n",
    "        \"index_type\": IndexType.HNSW,\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {\"M\": M, \"efConstruction\": efConstruction}\n",
    "    }\n",
    "\n",
    "    # Use no-schema Milvus client using flexible json key:value format.\n",
    "    milvus_client = MilvusClient(uri=my_uri)\n",
    "\n",
    "    # Check if collection already exists, if so drop it.\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        print(f\"\\nCollection had previously been created, dropping previous collection to initialize anew: `{COLLECTION_NAME}`\")\n",
    "\n",
    "    # Create the collection.\n",
    "    milvus_client.create_collection(COLLECTION_NAME, EMBEDDING_DIM,\n",
    "                                    consistency_level=\"Eventually\",\n",
    "                                    auto_id=True,\n",
    "                                    overwrite=True,\n",
    "                                    params=index_params)\n",
    "\n",
    "    print(f\"\\nSuccessfully created collection: `{COLLECTION_NAME}`\")\n",
    "    print(milvus_client.describe_collection(COLLECTION_NAME))\n",
    "\n",
    "    return milvus_client\n",
    "\n",
    "\n",
    "def split_documents_to_chunks(docs, max_seq_length, hf_eos_token_length):\n",
    "    \"\"\"\n",
    "    Split documents into smaller recursive chunks using Sentence Transformers' RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be split.\n",
    "    - max_seq_length (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list): List of chunks.\n",
    "\n",
    "    Example usage:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    docs = [\"Document 1 text.\", \"Document 2 text.\", \"Document 3 text.\"]\n",
    "    \n",
    "    resulting_chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    print(resulting_chunks)\n",
    "    \"\"\"\n",
    "    # Calculate chunk size and overlap\n",
    "    chunk_size = max_seq_length - hf_eos_token_length\n",
    "    chunk_overlap = int(round(chunk_size * 0.10, 0))\n",
    "\n",
    "    # Create an instance of the RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Split the documents further into smaller, recursive chunks.\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, max_seq_length, hf_eos_token_length):\n",
    "    \"\"\"\n",
    "    Insert document chunks into a Milvus collection.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be inserted.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - encoder (SentenceTransformer): SentenceTransformer model for generating embeddings.\n",
    "    - milvus_client (Milvus): Milvus client instance.\n",
    "    - max_seq_length (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - insert_time (float): Time taken for the insertion process.\n",
    "\n",
    "    Example Usage assuming 'chunks' is a list of dictionaries with 'page_content' and 'metadata' keys:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    ENCODER_MODEL_NAME = \"WhereIsAI/UAE-Large-V1\"\n",
    "    # Initialize Milvus client\n",
    "    # Initialize encoder model\n",
    "    \n",
    "    resulting_insert_time = insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \"\"\"\n",
    "    # Convert chunks to a list of dictionaries.\n",
    "    chunk_list = []\n",
    "    for chunk in chunks:\n",
    "    \n",
    "        # Generate embeddings using encoder from HuggingFace.\n",
    "        embeddings = torch.tensor(encoder.encode([chunk.page_content]))\n",
    "        # embeddings = F.normalize(embeddings, p=2, dim=1) #use torch\n",
    "        embeddings = np.array(embeddings / np.linalg.norm(embeddings)) #use numpy\n",
    "        converted_values = list(map(np.float32, embeddings))[0]\n",
    "        \n",
    "        # Assemble embedding vector, original text chunk, metadata.\n",
    "        chunk_dict = {\n",
    "            'vector': converted_values,\n",
    "            'chunk': chunk.page_content,\n",
    "            'source': chunk.metadata['page']\n",
    "        }\n",
    "        chunk_list.append(chunk_dict)\n",
    "\n",
    "    # Insert data into the Milvus collection.\n",
    "    print(\"Start inserting entities\")\n",
    "\n",
    "    insert_result = milvus_client.insert(\n",
    "        COLLECTION_NAME,\n",
    "        data=chunk_list,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    print(\"Finished inserting entities\")\n",
    "\n",
    "    # After the final entity is inserted, call flush to stop growing segments left in memory.\n",
    "    milvus_client.flush(COLLECTION_NAME)\n",
    "\n",
    "    return print(f\"\\nNumber of chunks inserted into Milvus database: {len(inserted_chunks)} with chunk id starting at number: {inserted_chunks[0]}\\n\")\n",
    "\n",
    "\n",
    "def client_assemble_retrieved_context(retrieved_top_k, metadata_fields=[], num_shot_answers=3):\n",
    "    \"\"\" \n",
    "    For each question, assemble the context and metadata from the retrieved_top_k chunks.\n",
    "    retrieved_top_k: list of dicts\n",
    "\n",
    "    Example Usage:\n",
    "    formatted_results, context, context_metadata = client_assemble_retrieved_context(results, metadata_fields=metadata_fields, num_shot_answers=top_k)\n",
    "    \"\"\"\n",
    "    # Assemble the context as a stuffed string.\n",
    "    distances = []\n",
    "    context = []\n",
    "    context_metadata = []\n",
    "    i = 1\n",
    "    for r in retrieved_top_k[0]:\n",
    "        distances.append(r['distance'])\n",
    "        if i <= num_shot_answers:\n",
    "            if len(metadata_fields) > 0:\n",
    "                metadata = {}\n",
    "                for field in metadata_fields:\n",
    "                    metadata[field] = r['entity'][field]\n",
    "                context_metadata.append(metadata)\n",
    "            context.append(r['entity']['chunk'])\n",
    "        i += 1\n",
    "\n",
    "    # Assemble formatted results in a zipped list.\n",
    "    formatted_results = list(zip(distances, context, context_metadata))\n",
    "    # Return all the things for convenience.\n",
    "    return formatted_results, context, context_metadata\n",
    "    \n",
    "\n",
    "def search_and_generate_response(milvus_client, encoder, COLLECTION_NAME, SAMPLE_QUESTION, llm_name, temperature, random_seed, top_k=3, M=16):\n",
    "    \"\"\"\n",
    "    Search Milvus collection for relevant context and generate a response using the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - openai_client (OpenAI): OpenAI client instance.\n",
    "    - milvus_client (Milvus): Milvus client instance.\n",
    "    - encoder (SentenceTransformer): SentenceTransformer model for generating embeddings.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - SAMPLE_QUESTION (str): Sample question for search.\n",
    "    - llm_name (str): Name of the OpenAI language model.\n",
    "    - temperature (float): Temperature for response generation.\n",
    "    - random_seed (int): Random seed for response generation.\n",
    "    - top_k (int): Top K results to retrieve from Milvus search.\n",
    "    - M (Milvus): Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "\n",
    "    Returns:\n",
    "    - response_choices (list): List of response choices.\n",
    "\n",
    "    Example usage:\n",
    "    \n",
    "    response_choices = search_and_generate_response(\n",
    "        milvus_client,\n",
    "        encoder,\n",
    "        COLLECTION_NAME,\n",
    "        SAMPLE_QUESTION,\n",
    "        LLM_NAME,\n",
    "        TEMPERATURE,\n",
    "        RANDOM_SEED\n",
    "    )\n",
    "    \"\"\"\n",
    "    efConstruction = M * 2\n",
    "    \n",
    "    # Return top k results with HNSW index.\n",
    "    search_params = {\"ef\": efConstruction}\n",
    "\n",
    "    # Define output fields to return.\n",
    "    output_fields = [\"source\", \"chunk\"]\n",
    "\n",
    "    # Search Milvus collection\n",
    "    results = milvus_client.search(\n",
    "        COLLECTION_NAME,\n",
    "        data=encoder.encode([SAMPLE_QUESTION]),\n",
    "        search_params=search_params,\n",
    "        output_fields=output_fields,\n",
    "        limit=top_k,\n",
    "        consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "    # Assemble retrieved context\n",
    "    metadata_fields = [f for f in output_fields if f != 'chunk']\n",
    "    formatted_results, context, context_metadata = client_assemble_retrieved_context(results, metadata_fields=metadata_fields, num_shot_answers=top_k)\n",
    "    \n",
    "    SAMPLE_QUESTION = \"What are the key contributions of this paper and the evaluation metrics that prove that this paper advances previously known information?\"\n",
    "    \n",
    "    SYSTEM_PROMPT = f\"\"\"Answer in no less than 4000 characters. Use only the Context below to answer the user's question. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Outline the key points of the paper\n",
    "    - Edit the outline into a voiceover script for a 5 minute video\n",
    "    - Clearly state why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially\n",
    "    - regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources.\n",
    "    Context: {context}\n",
    "    Answer: The answer to the question in no less than 4000 characters in complete sentences as a narration. Do not pretend to be the author, just an instructor.\n",
    "    Grounding sources: {context_metadata[1]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Also create a template for function calls later.\n",
    "    SYSTEM_PROMPT_TEMPLATE = f\"\"\"Answer in no less than 4000 characters. Use only the Context below to answer the user's question. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Outline the key points of the paper\n",
    "    - Edit the outline into a voiceover script for a 5 minute video\n",
    "    - Clearly state why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially\n",
    "    - regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources.\n",
    "    Context: {context}\n",
    "    Answer: The answer to the question in no less than 4000 characters in complete sentences as a narration. Do not pretend to be the author, just an instructor.\n",
    "    Grounding sources: {context_metadata[1]}\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    load_dotenv()\n",
    "    key = get_env_variables()\n",
    "    openai_client = OpenAI(api_key=key[\"OPENAI_API_KEY\"])\n",
    "    \n",
    "    # Generate response using the OpenAI API\n",
    "    response = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "            {\"role\": \"user\", \"content\": f\"\\nquestion: {SAMPLE_QUESTION}\",}\n",
    "        ],\n",
    "        model=llm_name,\n",
    "        temperature=temperature,\n",
    "        seed=random_seed,\n",
    "    )\n",
    "\n",
    "    # Extract and print the contents of the number one ranked response:\n",
    "    response_choices = [choice.message.content for choice in response.choices]\n",
    "    for i, choice in enumerate(response_choices, 1):\n",
    "        pprint.pprint(f\"\\nAnswer {i}: {choice}\\n\")\n",
    "\n",
    "    return response_choices\n",
    "\n",
    "\n",
    "def text_to_speech(text_for_TTS, arxiv_id, folder_audio):\n",
    "\n",
    "    ELEVEN_LABS_API_KEY = os.environ.get(\"ELEVEN_LABS_API_KEY\")\n",
    "\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/bVMeCyTHy58xNoL34h3p\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVEN_LABS_API_KEY\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"text\": text_for_TTS,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Generate a unique filename based on timestamp\n",
    "        filename = f\"output_{arxiv_id}.mp3\"\n",
    "\n",
    "        # Save the recording to the unique file\n",
    "        with open(f\"{folder_audio}/{filename}\", 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"\\nRecording saved in {folder_name}/{filename}\")\n",
    "        return f\"{folder_name}/{filename}\"\n",
    "    else:\n",
    "        print(f\"\\n Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "def convert_pdf_to_png(folder_images, pdf_path):\n",
    "    # Create a folder for storing the PNGs\n",
    "    sub_folder_name = os.path.splitext(os.path.basename(pdf_path))[0] + \"_pngs\"\n",
    "    full_path = os.path.join(folder_images, sub_folder_name)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.makedirs(full_path)\n",
    "\n",
    "    # Convert each page of the PDF to PNG\n",
    "    images = convert_from_path(pdf_path, output_folder=full_path)\n",
    "\n",
    "    # Save each image as a separate PNG file\n",
    "    for i, image in enumerate(images):\n",
    "        png_path = os.path.join(full_path, f\"{folder_name}_page_{i + 1}.png\")\n",
    "        image.save(png_path, \"PNG\")\n",
    "\n",
    "    print(f\"\\nAll pages converted and saved in the folder: {folder_name}\")\n",
    "\n",
    "    # Clean up: Delete the .ppm files and uncropped files\n",
    "    for filename in os.listdir(full_path):\n",
    "        if filename.endswith(\".ppm\") or \"cropped\" not in filename:\n",
    "            file_to_remove_path = os.path.join(full_path, filename)\n",
    "            os.remove(file_to_remove_path)\n",
    "\n",
    "    print(f\"\\n.ppm and uncropped images deleted in the folder: {folder_name}\")\n",
    "\n",
    "\n",
    "def cut_pngs_in_half(image_folder):\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\"\\nError: Directory '{image_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for file_name in files:\n",
    "        # Check if the file is a PNG\n",
    "        if file_name.lower().endswith('.png'):\n",
    "            image_path = os.path.join(image_folder, file_name)\n",
    "\n",
    "            # Open the image\n",
    "            with Image.open(image_path) as img:\n",
    "                # Get the dimensions of the image\n",
    "                width, height = img.size\n",
    "\n",
    "                # Cut the image in half (top and bottom)\n",
    "                top_half = img.crop((0, 0, width, height // 2))\n",
    "                bottom_half = img.crop((0, height // 2, width, height))\n",
    "\n",
    "                # Save the top and bottom halves with \"_cropped_1\" and \"_cropped_2\" suffixes\n",
    "                top_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_1.png\"), 'PNG')\n",
    "                bottom_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_2.png\"), 'PNG')\n",
    "\n",
    "                print(f\"\\nImages saved: {file_name}_cropped_1.png (top) and {file_name}_cropped_2.png (bottom)\")\n",
    "\n",
    "\n",
    "def analyze_mp3_length(mp3_path):\n",
    "    audio = AudioSegment.from_file(mp3_path)\n",
    "    return len(audio) / 1000.0  # Length in seconds\n",
    "\n",
    "def fetch_cropped_images(image_folder):\n",
    "    # List all images in the folder\n",
    "    all_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Identify files to keep (those with the word \"cropped\" in their filenames)\n",
    "    cropped_images = [image for image in all_images if image.lower().endswith('.png') and 'cropped' in image.lower()]\n",
    "    \n",
    "    # Delete files that do not contain the word \"cropped\"\n",
    "    for image in all_images:\n",
    "        if image not in cropped_images:\n",
    "            os.remove(os.path.join(image_folder, image))\n",
    "    \n",
    "    # List the remaining images after deletion\n",
    "    remaining_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Sort the cropped images based on numeric values in their filenames\n",
    "    sorted_images = sorted(remaining_images, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    return sorted_images\n",
    "\n",
    "\n",
    "def create_video(mp3_path, image_folder, output_path):\n",
    "    # Sort the images by converting the numeric parts of filenames into integers\n",
    "    image_files = sorted([file for file in os.listdir(image_folder) if 'cropped' in file and file.lower().endswith('.png')],\n",
    "                         key=lambda x: [int(part) if part.isdigit() else part for part in re.split(r'(\\d+)', x)])\n",
    "    audio_clip = AudioFileClip(mp3_path)\n",
    "    \n",
    "    # Calculate the duration of each image based on the total duration of the audio and the number of images\n",
    "    image_duration = audio_clip.duration / len(image_files)\n",
    "    \n",
    "    clips = []\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        # Load each image using imageio\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = imageio.imread(image_path)\n",
    "    \n",
    "        if image.sum() == 0:\n",
    "            print(f\"Image {image_file} is blank. Skipping...\")\n",
    "            os.remove(image_file)\n",
    "            continue\n",
    "                \n",
    "        # Create a clip from the image and set its duration\n",
    "        image_clip = ImageClip(image).set_duration(image_duration)\n",
    "    \n",
    "        # Add the image clip to the list of clips\n",
    "        clips.append(image_clip)\n",
    "    \n",
    "    # Concatenate the image clips to create the final video\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_clip = final_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video with audio\n",
    "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24, verbose=True)\n",
    "    print(f\"\\nFinal video saved at: {output_path}.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6321b-abd9-447c-b48e-bdc9047f02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(url):\n",
    "\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    LLM_NAME = \"gpt-3.5-turbo\"\n",
    "    TEMPERATURE = 0.1\n",
    "    RANDOM_SEED = 415\n",
    "    M=16\n",
    "    \n",
    "    folder_pdfs = \"pdfs\"\n",
    "    folder_images = \"images\"\n",
    "    folder_final_videos = \"final_videos\"\n",
    "    folder_audio = \"audio_voiceover\"\n",
    "    \n",
    "    # Call the function to create the folder\n",
    "    create_folder(folder_pdfs)\n",
    "    create_folder(folder_images)\n",
    "    create_folder(folder_audio)\n",
    "    create_folder(folder_final_videos)\n",
    "    \n",
    "    # Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "    arxiv_id = download_and_save_pdf(url, folder_pdfs)\n",
    "    \n",
    "    arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "    pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "    create_folder(pdf_path)\n",
    "    image_folder = f\"{folder_images}/{arxiv_name}_pngs\" \n",
    "    mp3_path = f\"{folder_audio}/output_{arxiv_name}.mp3\"\n",
    "    output_path = f\"{folder_final_videos}/{arxiv_name}.mp4\" \n",
    "    \n",
    "    # Download open source embedding model \"WhereIsAI/UAE-Large-V1\" via Huggingface's Sentence Transformers\n",
    "    encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "    \n",
    "    # Create a no-schema milvus collection and define the database index\n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M, my_uri)\n",
    "    \n",
    "    # Load PDF's into a PDF object using LangChain's PyPDFLoader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Cut text from PDF's into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "    chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \n",
    "    # Insert text chunks into Milvus vector database using index type HNSW Indexing and Cosine Distance\n",
    "    insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \n",
    "    # Generate transcript using OpenAI based on the cosine distance search of the document then using gpt-3.5-turbo's chat completions\n",
    "    text_for_TTS_list = search_and_generate_response(\n",
    "        milvus_client, # Running session via docker container on port http://localhost:19531\n",
    "        encoder, # Sentence Transformer WhereIsAI/UAE-Large-V1\n",
    "        COLLECTION_NAME, # MilvusDocs by default\n",
    "        SAMPLE_QUESTION,\n",
    "        LLM_NAME,\n",
    "        TEMPERATURE,\n",
    "        RANDOM_SEED,\n",
    "        M)\n",
    "    \n",
    "    # convert text to speech with Elevenlabs\n",
    "    audio_path = text_to_speech(text_for_TTS_list[0], arxiv_id, folder_audio)\n",
    "    \n",
    "    # convert each pdf to a png\n",
    "    convert_pdf_to_png(folder_images, pdf_path)\n",
    "    \n",
    "    # cut png's in half\n",
    "    cut_pngs_in_half(image_folder)\n",
    "    \n",
    "    # combine png's with audio to generate an mp4\n",
    "    create_video(folder_audio, folder_images, folder_final_videos)\n",
    "    return folder_final_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d92e0-24b3-4dd3-834b-cf42ef7396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_url,\n",
    "    inputs=gr.Textbox(placeholder=\"Enter arXiv PDF URL\"),\n",
    "    outputs=gr.Video(),\n",
    "    live=True,\n",
    "    theme=\"sky\",\n",
    "    flagging_options=None,  # Disable the flag button\n",
    "    title=\"Arxiv2Video\",\n",
    ")\n",
    "\n",
    "# Add a submit button\n",
    "submit_button = gr.Button()\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23e133-4b96-4ddb-ab6f-b50851a71a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18507a8-242a-4f68-b551-a7212c325155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
