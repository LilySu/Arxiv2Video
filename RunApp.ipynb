{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e445fe3-e5e0-4080-9a78-6570bfae7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip, AudioFileClip, concatenate_videoclips\n",
    "from pymilvus import Milvus, MilvusClient, IndexType, connections, utility\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from pdf2image.exceptions import PDFPageCountError, PDFSyntaxError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from moviepy.editor import concatenate_videoclips, ImageClip\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from moviepy.config import change_settings\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pdf2image import convert_from_path\n",
    "from milvus import default_server\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import feedparser\n",
    "import requests\n",
    "import imageio\n",
    "import base64\n",
    "import pprint\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b35aae-ddeb-4c2b-acbe-4529e441a6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:19531\n"
     ]
    }
   ],
   "source": [
    "change_settings({\"FFMPEG_BINARY\": \"/opt/homebrew/bin/ffmpeg\", \"DYLD_LIBRARY_PATH\":\"/opt/homebrew/bin/convert\"})\n",
    "# Set up a Milvus client\n",
    "default_server.start()\n",
    "host=\"127.0.0.1\"\n",
    "connections.connect(host=host, port=default_server.listen_port)\n",
    "port=default_server.listen_port\n",
    "my_uri = \"http://localhost:\" + str(port)\n",
    "print(my_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8361215-27c0-4617-bba1-d4e41bcd7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_variables():\n",
    "    \"\"\"Fetch all necessary configurations from environment variables.\"\"\"\n",
    "    return {\n",
    "        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),\n",
    "        'ELEVEN_LABS_API_KEY': os.getenv('ELEVEN_LABS_API_KEY')\n",
    "    }\n",
    "\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"The folder '{folder_name}' has been created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{folder_name}' already exists.\")\n",
    "\n",
    "\n",
    "def arxiv_id_from_url(url):\n",
    "    # Extract the arXiv ID from the URL using a regular expression\n",
    "    match = re.search(r'arxiv\\.org/pdf/(\\d+\\.\\d+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def download_and_save_pdf(url, folder_pdfs):\n",
    "    \"\"\"\n",
    "    Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The arXiv.org URL of the paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: ArXiv ID of the downloaded paper if successful, or an error message.\n",
    "    \"\"\"\n",
    "    # Extract arXiv ID from the URL\n",
    "    arxiv_id = arxiv_id_from_url(url)\n",
    "\n",
    "    arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "    pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "    create_folder(pdf_path)\n",
    "\n",
    "    # Check if a valid arXiv ID was extracted\n",
    "    if arxiv_id:\n",
    "        try:\n",
    "            # Make a request to the arXiv API\n",
    "            feed = feedparser.parse(f'http://export.arxiv.org/api/query?id_list={arxiv_id}')\n",
    "\n",
    "            # Check if the response contains entries\n",
    "            if 'entries' in feed:\n",
    "                # Iterate over each entry (paper) in the feed\n",
    "                for entry in feed.entries:\n",
    "                    # Extract the PDF link from the entry\n",
    "                    pdf_link = entry.link.replace('/abs/', '/pdf/') + '.pdf'\n",
    "\n",
    "                    # Download the PDF\n",
    "                    response = requests.get(pdf_link)\n",
    "\n",
    "                    # Save the PDF in the local directory with the name based on the arXiv ID\n",
    "                    with open(f'{pdf_path}/{arxiv_name}.pdf', 'wb') as pdf_file:\n",
    "                        pdf_file.write(response.content)\n",
    "\n",
    "                    print(f\"\\nPDF downloaded and saved as {arxiv_name}.pdf\")\n",
    "                    return arxiv_id\n",
    "\n",
    "            else:\n",
    "                return f\"\\nNo entries found for arXiv ID {arxiv_id}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"\\nError extracting information: {e}\"\n",
    "    else:\n",
    "        return \"Invalid arXiv PDF URL format. Please enter a valid URL.\"\n",
    "\n",
    "\n",
    "def download_and_initialize_embedding_model(model_name=\"WhereIsAI/UAE-Large-V1\", device=None):\n",
    "    \"\"\"\n",
    "    Download and initialize the Sentence Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the Sentence Transformer model to download.\n",
    "    - device (str or torch.device): The device to use for the model (e.g., 'cuda:3' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - encoder (SentenceTransformer): The initialized Sentence Transformer model.\n",
    "    - EMBEDDING_DIM (int): The embedding dimension of the model.\n",
    "    - MAX_SEQ_LENGTH (int): The maximum sequence length.\n",
    "\n",
    "    Example usage:\n",
    "    encoder, EMBEDDING_DIM, max_seq_length = download_and_initialize_embedding_model()\n",
    "    \"\"\"\n",
    "    # Initialize torch settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    DEVICE = torch.device(device) if device else torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\ndevice: {DEVICE}\")\n",
    "\n",
    "    # Load the model from the Hugging Face model hub\n",
    "    encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "    print(f\"\\nDatatype of SentenceTransformer encoded object{type(encoder)}\\n\")\n",
    "    print(f\"\\nWhat the encoder object looks like: {encoder}\\n\")\n",
    "\n",
    "    # Get the model parameters and save for later\n",
    "    EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "    MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length()\n",
    "    # Assume tokens are 3 characters long\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "    HF_EOS_TOKEN_LENGTH = 1 * 3\n",
    "    # Test with 512 sequence length\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "    HF_EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "    # Inspect model parameters\n",
    "    print(f\"\\nmodel_name: {model_name}\")\n",
    "    print(f\"\\nEMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "    print(f\"\\nMAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "    return encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH\n",
    "\n",
    "\n",
    "def create_milvus_collection(COLLECTION_NAME, uri=my_uri):\n",
    "    \"\"\"\n",
    "    Create a no-schema Milvus collection and define the database index.\n",
    "\n",
    "    Parameters:\n",
    "    - uri (str): The URI of the Milvus server.\n",
    "    - COLLECTION_NAME (str): The name of the Milvus collection.\n",
    "    \n",
    "    Returns:\n",
    "    - milvus_client (Milvus): The Milvus client instance.\n",
    "\n",
    "\n",
    "    Example usage:\n",
    "    my_uri = \"tcp://127.0.0.1:19530\"\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    my_EMBEDDING_DIM = 3072\n",
    "    \n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, uri=my_uri)\n",
    "    \"\"\"\n",
    "    EMBEDDING_DIM = 3072 # This is the embedding dimension for OpenAI's Embedding model \"text-embedding-3-large\"\n",
    "    \n",
    "    # Add custom FLAT search index to the collection, which is 1:1 the original and mmore inefficient that other types\n",
    "    index_params = {\n",
    "        \"index_type\": IndexType.FLAT, # Flat index type does not have params\n",
    "        \"metric_type\": \"COSINE\",\n",
    "    }\n",
    "\n",
    "    # Use no-schema Milvus client using flexible json key:value format.\n",
    "    milvus_client = MilvusClient(uri=my_uri)\n",
    "\n",
    "    # Check if collection already exists, if so drop it.\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        print(f\"\\nCollection had previously been created, dropping previous collection to initialize anew: `{COLLECTION_NAME}`\")\n",
    "\n",
    "    # Create the collection.\n",
    "    milvus_client.create_collection(COLLECTION_NAME, EMBEDDING_DIM,\n",
    "                                    consistency_level=\"Eventually\",\n",
    "                                    auto_id=True,\n",
    "                                    overwrite=True,\n",
    "                                    params=index_params)\n",
    "\n",
    "    print(f\"\\nSuccessfully created collection: `{COLLECTION_NAME}`\")\n",
    "    print(milvus_client.describe_collection(COLLECTION_NAME))\n",
    "\n",
    "    return milvus_client\n",
    "\n",
    "\n",
    "def split_documents_to_chunks(docs, MAX_SEQ_LENGTH, CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Split documents into smaller recursive chunks using Sentence Transformers' RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be split.\n",
    "    - max_seq_length (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list): List of chunks.\n",
    "\n",
    "    Example usage:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    docs = [\"Document 1 text.\", \"Document 2 text.\", \"Document 3 text.\"]\n",
    "    \n",
    "    resulting_chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    print(resulting_chunks)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an instance of the RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=MAX_SEQ_LENGTH, # Arbitrary numbers, personal preference\n",
    "        chunk_overlap=CHUNK_OVERLAP, # Arbitrary numbers, personal preference\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    # Split the documents further into smaller, recursive chunks.\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def insert_chunks_into_milvus(chunks, COLLECTION_NAME, host=\"127.0.0.1\", port=\"19531\"):\n",
    "    \"\"\"\n",
    "    Insert document chunks into a Milvus collection.\n",
    "\n",
    "    Parameters:\n",
    "    - chunked (list): Full langchain documents object chunked already from document loader to be inserted.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - host\n",
    "    - port\n",
    "\n",
    "    Returns:\n",
    "    - insert_time (float): Time taken for the insertion process.\n",
    "\n",
    "    Example Usage: \n",
    "    \n",
    "    vector_store = insert_chunks_into_milvus(COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \"\"\"\n",
    "    # Instantiate OpenAI embedding model\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    vector_store = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"host\": host, \"port\": port},\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        drop_old=True,\n",
    "    ).from_documents(\n",
    "        chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection_args={\"host\": host, \"port\": port},\n",
    "    )\n",
    "\n",
    "    # After the final entity is inserted, call flush to stop growing segments left in memory.\n",
    "    milvus_client.flush(COLLECTION_NAME)\n",
    "    print(f\"\\nChunks inserted into Milvus database.\")\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def retrieve_context(vector_store):\n",
    "    \"\"\" \n",
    "    Retrieve relevant data from vector database, which in this case happens to be everything loaded from the pdf.\n",
    "    retrieved_top_k: list of dicts\n",
    "\n",
    "    Example Usage:\n",
    "    formatted_results, context, context_metadata = client_assemble_retrieved_context(results, metadata_fields=metadata_fields, num_shot_answers=top_k)\n",
    "    \"\"\"\n",
    "    # Assemble the context as a stuffed string.\n",
    "\n",
    "    context = []\n",
    "    context_metadata = []\n",
    "    i = 1\n",
    "    for r in retrieved_top_k[0]:\n",
    "\n",
    "        if i <= num_shot_answers:\n",
    "            if len(metadata_fields) > 0:\n",
    "                metadata = {}\n",
    "                for field in metadata_fields:\n",
    "                    metadata[field] = r['entity'][field]\n",
    "                context_metadata.append(metadata)\n",
    "            context.append(r['entity']['chunk'])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def retrieve_context_and_generate_response(vector_store, text_before_abstract):\n",
    "    \"\"\"\n",
    "    Search Milvus collection for relevant context and generate a response using the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - openai_client (OpenAI): OpenAI client instance.\n",
    "    - milvus_client (Milvus): Milvus client instance.\n",
    "    - encoder (SentenceTransformer): SentenceTransformer model for generating embeddings.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - SAMPLE_QUESTION (str): Sample question for search.\n",
    "    - llm_name (str): Name of the OpenAI language model.\n",
    "    - temperature (float): Temperature for response generation.\n",
    "    - random_seed (int): Random seed for response generation.\n",
    "    - top_k (int): Top K results to retrieve from Milvus search.\n",
    "    - M (Milvus): Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "\n",
    "    Returns:\n",
    "    - response_choices (list): List of response choices.\n",
    "\n",
    "    Example usage:\n",
    "    \n",
    "    response_choices = search_and_generate_response(\n",
    "        milvus_client,\n",
    "        encoder,\n",
    "        COLLECTION_NAME,\n",
    "        LLM_NAME,\n",
    "        TEMPERATURE,\n",
    "        RANDOM_SEED\n",
    "    )\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) \n",
    "    \n",
    "    context = vector_store.as_retriever()\n",
    "\n",
    "    QUESTION = f\"What are the key contributions and evaluations of this paper {text_before_abstract}\"\n",
    "\n",
    "    # Assemble retrieved context\n",
    "    # metadata_fields = [f for f in output_fields if f != 'chunk']\n",
    "\n",
    "    SYSTEM_PROMPT = f\"\"\"Use only the the paper {text_before_abstract} to answer the user's question. Answer in no less than 4000 characters. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Outline the key points of the paper but do not output it. Use the outline as a guide to expand on a video voiceover of 3 minutes.\n",
    "    - Edit the outline into a voiceover script for a 5 minute video\n",
    "    - Clearly state why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved, how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Always start by stating the title of the paper as the first few words.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    - Do out output the outline, just output the transcript. Be as detailed as possible.\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources. \n",
    "    Context: {context}\n",
    "    Question: {QUESTION}\n",
    "    \"\"\"\n",
    "    \n",
    "    rag_prompt = PromptTemplate.from_template(SYSTEM_PROMPT, return_source_documents=True)\n",
    "    rag_chain = (\n",
    "        {\"context\": context, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "    )\n",
    "    \n",
    "    response = rag_chain.invoke(response = rag_chain.invoke(f\"Only output the voiceover transcript, highlighting the key contributions and evaluation metrics for the paper {text_before_abstract}\"))\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def save_transcript(response_choices, folder_transcripts, arxiv_name):\n",
    "    \"\"\"\n",
    "    Save the first element of response_choices into a text file in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - response_choices (list): A list of choices where the first element is the transcript content.\n",
    "    - folder_transcripts (str): The directory path where the transcript file will be saved.\n",
    "    - arxiv_name (str): The name used for generating the transcript file.\n",
    "\n",
    "    Returns:\n",
    "    None: The function saves the transcript content to a text file.\n",
    "\n",
    "    Example:\n",
    "    save_transcript([\"This is the transcript content.\"], \"transcripts\", \"example_arxiv\")\n",
    "    \"\"\"\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(folder_transcripts):\n",
    "        os.makedirs(folder_transcripts)\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(folder_transcripts, f\"{arxiv_name}.txt\")\n",
    "\n",
    "    # Save response_choices[0] to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(response_choices[0])\n",
    "\n",
    "    print(f\"\\nTranscript saved in: {file_path}\")\n",
    "\n",
    "\n",
    "def text_to_speech(text_for_TTS, arxiv_name, folder_audio):\n",
    "\n",
    "    ELEVEN_LABS_API_KEY = os.environ.get(\"ELEVEN_LABS_API_KEY\")\n",
    "\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/bVMeCyTHy58xNoL34h3p\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVEN_LABS_API_KEY\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"text\": text_for_TTS,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Generate a unique filename based on timestamp\n",
    "    filename = f\"output_{arxiv_name}.mp3\"\n",
    "    target_path = os.path.join(folder_audio, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"Recording file {filename} already exists in {folder_audio}. Skipping download.\")\n",
    "        return target_path\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Save the recording to the unique file\n",
    "        with open(target_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"\\nRecording saved in {target_path}\")\n",
    "        return target_path\n",
    "    else:\n",
    "        print(f\"\\n Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "def convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name):\n",
    "    try:\n",
    "        # Create a folder for storing the PNGs\n",
    "        sub_folder_name = os.path.splitext(os.path.basename(pdf_file_path))[0] + \"_pngs\"\n",
    "        full_path = os.path.join(folder_images, sub_folder_name)\n",
    "        if not os.path.exists(full_path):\n",
    "            os.makedirs(full_path)\n",
    "    \n",
    "        # Convert each page of the PDF to PNG\n",
    "        images = convert_from_path(pdf_file_path, output_folder=full_path)\n",
    "        # arxiv_name = sub_folder_name.replace(\"_pngs\", \"\")\n",
    "    \n",
    "        # Save each image as a separate PNG file\n",
    "        for i, image in enumerate(images):\n",
    "            png_path = os.path.join(full_path, f\"{arxiv_name}_page_{i + 1}.png\")\n",
    "            image.save(png_path, \"PNG\")\n",
    "    \n",
    "        print(f\"\\nAll pages converted and saved in the folder: {full_path}\")\n",
    "    \n",
    "        # Clean up: Delete the .ppm files and uncropped files\n",
    "        for filename in os.listdir(full_path):\n",
    "            if filename.endswith(\".ppm\"):\n",
    "                file_to_remove_path = os.path.join(full_path, filename)\n",
    "                os.remove(file_to_remove_path)\n",
    "    \n",
    "        print(f\"\\n.ppm artifacts deleted in the folder: {full_path}\")\n",
    "    except (PDFPageCountError, PDFSyntaxError, PermissionError) as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(f\"Skipping processing of {pdf_file_path}\")\n",
    "        if isinstance(e, PdfReadError):\n",
    "            print(\"PdfReadError: Unable to read PDF file.\")\n",
    "        elif isinstance(e, PermissionError):\n",
    "            print(\"PermissionError: Permission issue while processing the PDF file.\")\n",
    "\n",
    "\n",
    "def cut_pngs_in_half(image_folder):\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\"\\nError: Directory '{image_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for file_name in files:\n",
    "        # Check if the file is a PNG and does not contain 'cropped' in the name\n",
    "        if file_name.lower().endswith('.png') and 'cropped' not in file_name.lower():\n",
    "            image_path = os.path.join(image_folder, file_name)\n",
    "\n",
    "            # Open the image\n",
    "            with Image.open(image_path) as img:\n",
    "                # Get the dimensions of the image\n",
    "                width, height = img.size\n",
    "\n",
    "                # Cut the image in half (top and bottom)\n",
    "                top_half = img.crop((0, 0, width, height // 2))\n",
    "                bottom_half = img.crop((0, height // 2, width, height))\n",
    "\n",
    "                # Save the top and bottom halves with \"_cropped_1\" and \"_cropped_2\" suffixes\n",
    "                top_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_1.png\"), 'PNG')\n",
    "                bottom_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_2.png\"), 'PNG')\n",
    "\n",
    "                print(f\"\\nImages saved: {file_name}_cropped_1.png (top) and {file_name}_cropped_2.png (bottom)\")\n",
    "        else:\n",
    "            print(f\"\\nSkipping processing for {file_name} as it contains 'cropped' in the file name.\")\n",
    "\n",
    "\n",
    "def analyze_mp3_length(mp3_path):\n",
    "    audio = AudioSegment.from_file(mp3_path)\n",
    "    return len(audio) / 1000.0  # Length in seconds\n",
    "\n",
    "def fetch_cropped_images(image_folder):\n",
    "    # List all images in the folder\n",
    "    all_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Identify files to keep (those with the word \"cropped\" in their filenames)\n",
    "    cropped_images = [image for image in all_images if image.lower().endswith('.png') and 'cropped' in image.lower()]\n",
    "    \n",
    "    # Delete files that do not contain the word \"cropped\"\n",
    "    for image in all_images:\n",
    "        if image not in cropped_images:\n",
    "            os.remove(os.path.join(image_folder, image))\n",
    "    \n",
    "    # List the remaining images after deletion\n",
    "    remaining_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Sort the cropped images based on numeric values in their filenames\n",
    "    sorted_images = sorted(remaining_images, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    return sorted_images\n",
    "\n",
    "\n",
    "def move_uncropped_files(image_folder):\n",
    "    try:\n",
    "        # Create a new folder if it doesn't exist\n",
    "        uncropped_folder = os.path.join(image_folder, \"uncropped_pngs\")\n",
    "        if not os.path.exists(uncropped_folder):\n",
    "            os.makedirs(uncropped_folder)\n",
    "\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            file_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            # Check if the file name contains the word \"cropped\"\n",
    "            if \"cropped\" not in filename:\n",
    "                # Move the file to the uncropped folder\n",
    "                new_path = os.path.join(uncropped_folder, filename)\n",
    "\n",
    "                try:\n",
    "                    shutil.move(file_path, new_path)\n",
    "                    print(f\"File moved to uncropped folder: {filename}\")\n",
    "                except Exception as move_error:\n",
    "                    print(f\"Error moving file {filename}: {move_error}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"All non-cropped files moved to the folder: {uncropped_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def create_video(mp3_path, image_folder, output_path):\n",
    "    try:\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            file_path = os.path.join(image_folder, filename)\n",
    "            \n",
    "            # Check if the file name contains the word \"cropped\"\n",
    "            if \"cropped\" not in filename:\n",
    "                # Remove the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"File removed: {filename}\")\n",
    "                \n",
    "        print(f\"All non-cropped files removed in the folder: {image_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Sort the images by converting the numeric parts of filenames into integers\n",
    "    image_files = sorted([file for file in os.listdir(image_folder) if 'cropped' in file and file.lower().endswith('.png')],\n",
    "                         key=lambda x: [int(part) if part.isdigit() else part for part in re.split(r'(\\d+)', x)])\n",
    "    audio_clip = AudioFileClip(mp3_path)\n",
    "    \n",
    "    # Calculate the duration of each image based on the total duration of the audio and the number of images\n",
    "    image_duration = audio_clip.duration / len(image_files)\n",
    "    \n",
    "    clips = []\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        # Load each image using imageio\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = imageio.imread(image_path)\n",
    "    \n",
    "        if image.sum() == 0:\n",
    "            print(f\"Image {image_file} is blank. Skipping...\")\n",
    "            os.remove(image_file)\n",
    "            continue\n",
    "                \n",
    "        # Create a clip from the image and set its duration\n",
    "        image_clip = ImageClip(image).set_duration(image_duration)\n",
    "    \n",
    "        # Add the image clip to the list of clips\n",
    "        clips.append(image_clip)\n",
    "    \n",
    "    # Concatenate the image clips to create the final video\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_clip = final_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video with audio\n",
    "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24, verbose=True)\n",
    "    print(f\"\\nFinal video saved at: {output_path}.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96c6321b-abd9-447c-b48e-bdc9047f02d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs/2402_13254' already exists.\n",
      "\n",
      "PDF downloaded and saved as 2402_13254.pdf\n",
      "\n",
      "Collection had previously been created, dropping previous collection to initialize anew: `MilvusDocs`\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 3072}, 'element_type': 0}], 'aliases': [], 'collection_id': 448370747955370650, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "\n",
      "Chunks inserted into Milvus database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The research paper titled \"CounterCurate: Enhancing Physical and Semantic Vi\" presents a novel approach to enhancing physical and semantic visualizations. The key contributions of this paper include the development of a system that integrates physical and digital elements to create an interactive and immersive visualization experience. The researchers involved in this project are from institutions that are not specified in the provided text.\\n\\nThe motivation behind this research was to address the limitations of existing visualization techniques that often rely solely on digital interfaces. By combining physical and digital elements, the researchers aimed to create a more engaging and intuitive visualization platform.\\n\\nThe technologies previously known and involved in this research include Milvus and OpenAI Embeddings. Milvus is a vector database that enables efficient storage and retrieval of high-dimensional vectors, while OpenAI Embeddings are pre-trained models that capture semantic information from text data.\\n\\nThe technique employed in this research involves the integration of physical objects with digital visualizations using Milvus and OpenAI Embeddings. By leveraging these technologies, the researchers were able to create a system that allows users to interact with visualizations in a more tangible and intuitive way.\\n\\nThe key metrics that define the success of this work include the level of user engagement, the effectiveness of the physical-digital integration, and the overall impact on the understanding of complex data. By evaluating these metrics, the researchers were able to demonstrate the effectiveness of their approach in enhancing physical and semantic visualizations.\\n\\nIn terms of future directions, the researchers suggest exploring additional ways to incorporate physical elements into digital visualizations, further enhancing the interactive and immersive experience for users. Additionally, they propose investigating the potential applications of this approach in various domains, such as education, data analysis, and storytelling.\\n\\nIn conclusion, the research presented in this paper makes a significant contribution to the field of visualization by introducing a novel approach that enhances physical and semantic visualizations. By integrating physical objects with digital interfaces, the researchers have created a more engaging and intuitive visualization platform that has the potential to impact various industries and domains. The key findings and evaluations of this work highlight the effectiveness of the physical-digital integration and the potential for future advancements in this area.')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def process_url(url):\n",
    "\n",
    "paper=\"2402.13254\"\n",
    "host=\"127.0.0.1\"\n",
    "port=\"19531\"\n",
    "\n",
    "url = f\"https://arxiv.org/pdf/{paper}.pdf\"\n",
    "\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "HF_EOS_TOKEN_LENGTH = 3\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0.1\n",
    "RANDOM_SEED = 415\n",
    "M=16\n",
    "\n",
    "MAX_SEQ_LENGTH = 600 \n",
    "CHUNK_OVERLAP=75\n",
    "\n",
    "folder_pdfs = \"pdfs\"\n",
    "folder_images = \"images\"\n",
    "folder_final_videos = \"final_videos\"\n",
    "folder_audio = \"audio_voiceovers\"\n",
    "folder_transcripts = \"transcripts\"\n",
    "\n",
    "# # Call the function to create the folder\n",
    "# create_folder(folder_pdfs)\n",
    "# create_folder(folder_images)\n",
    "# create_folder(folder_audio)\n",
    "# create_folder(folder_final_videos)\n",
    "# create_folder(folder_transcripts)\n",
    "\n",
    "# Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "arxiv_id = download_and_save_pdf(url, folder_pdfs)\n",
    "\n",
    "arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "pdf_file_path = os.path.join(pdf_path, f\"{arxiv_name}.pdf\")\n",
    "image_folder = f\"{folder_images}/{arxiv_name}_pngs\" \n",
    "mp3_path = f\"{folder_audio}/output_{arxiv_name}.mp3\"\n",
    "output_path = f\"{folder_final_videos}/{arxiv_name}.mp4\" \n",
    "\n",
    "# Download open source embedding model \"WhereIsAI/UAE-Large-V1\" via Huggingface's Sentence Transformers\n",
    "# encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "\n",
    "# Create a no-schema milvus collection and define the database index\n",
    "milvus_client = create_milvus_collection(COLLECTION_NAME, my_uri)\n",
    "\n",
    "# Load PDF's into a PDF object using LangChain's PyPDFLoader\n",
    "loader = PyPDFLoader(f\"{pdf_path}/{arxiv_name}.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_before_abstract = re.search(r'^(.*?)\\bAbstract\\b', docs[0].page_content, re.DOTALL)\n",
    "\n",
    "# Cut text from PDF's into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, CHUNK_OVERLAP)\n",
    "\n",
    "# Insert text chunks into Milvus vector database using index type HNSW Indexing and Cosine Distance\n",
    "vector_store = insert_chunks_into_milvus(chunks, COLLECTION_NAME, host, port)\n",
    "\n",
    "response = retrieve_context_and_generate_response(vector_store, text_before_abstract)\n",
    "\n",
    "response\n",
    "    \n",
    "    # # Generate transcript using OpenAI based on the cosine distance search of the document then using gpt-3.5-turbo's chat completions\n",
    "    # text_for_TTS_list = search_and_generate_response(\n",
    "    #     milvus_client, # Running session via docker container on port http://localhost:19531\n",
    "    #     encoder, # Sentence Transformer WhereIsAI/UAE-Large-V1\n",
    "    #     COLLECTION_NAME, # MilvusDocs by default\n",
    "    #     LLM_NAME,\n",
    "    #     TEMPERATURE,\n",
    "    #     RANDOM_SEED,\n",
    "    #     M)\n",
    "    \n",
    "    # # Save LLM-generated voiceover script to directory\n",
    "    # save_transcript(text_for_TTS_list, folder_transcripts, arxiv_name)\n",
    "    \n",
    "    # # convert text to speech with Elevenlabs\n",
    "    # audio_path = text_to_speech(text_for_TTS_list[0], arxiv_name, folder_audio)\n",
    "    \n",
    "    # # convert each pdf to a png\n",
    "    # convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name)\n",
    "    \n",
    "    # # cut png's in half\n",
    "    # cut_pngs_in_half(image_folder)\n",
    "\n",
    "    # move_uncropped_files(image_folder)\n",
    "    \n",
    "    # # combine png's with audio to generate an mp4\n",
    "    # create_video(mp3_path, image_folder, output_path)\n",
    "    # milvus_client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "    # return folder_final_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35c36c93-5bb2-47b9-b486-bf2f12c14d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The research paper titled \"CounterCurate: Enhancing Physical and Semantic Vi\" presents a novel approach to enhancing physical and semantic visualizations. The key contributions of this paper include the development of a system that integrates physical and digital elements to create an interactive and immersive visualization experience. The researchers involved in this project are from institutions that are not specified in the provided text.\\n\\nThe motivation behind this research was to address the limitations of existing visualization techniques that often rely solely on digital interfaces. By combining physical and digital elements, the researchers aimed to create a more engaging and intuitive visualization platform.\\n\\nThe technologies previously known and involved in this research include Milvus and OpenAI Embeddings. Milvus is a vector database that enables efficient storage and retrieval of high-dimensional vectors, while OpenAI Embeddings are pre-trained models that capture semantic information from text data.\\n\\nThe technique employed in this research involves the integration of physical objects with digital visualizations using Milvus and OpenAI Embeddings. By leveraging these technologies, the researchers were able to create a system that allows users to interact with visualizations in a more tangible and intuitive way.\\n\\nThe key metrics that define the success of this work include the level of user engagement, the effectiveness of the physical-digital integration, and the overall impact on the understanding of complex data. By evaluating these metrics, the researchers were able to demonstrate the effectiveness of their approach in enhancing physical and semantic visualizations.\\n\\nIn terms of future directions, the researchers suggest exploring additional ways to incorporate physical elements into digital visualizations, further enhancing the interactive and immersive experience for users. Additionally, they propose investigating the potential applications of this approach in various domains, such as education, data analysis, and storytelling.\\n\\nIn conclusion, the research presented in this paper makes a significant contribution to the field of visualization by introducing a novel approach that enhances physical and semantic visualizations. By integrating physical objects with digital interfaces, the researchers have created a more engaging and intuitive visualization platform that has the potential to impact various industries and domains. The key findings and evaluations of this work highlight the effectiveness of the physical-digital integration and the potential for future advancements in this area.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0584b6cb-5ddb-4936-a61a-8d25d6833a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MilvusDocs']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3aecbb6-410e-42d8-91ac-5cc08f8bd1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae Lee1\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\n1University of Wisconsin–Madison2Microsoft Research\\nhttps://countercurate.github.io\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract substring\n",
    "text_before_abstract = re.search(r'^(.*?)\\bAbstract\\b', docs[0].page_content, re.DOTALL)\n",
    "\n",
    "page_header.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7aeb9e1c-bcda-4ba3-8a09-1539071afef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'explored problems: the neglect of physically\\ngrounded reasoning (counting and position un-\\nderstanding) and the potential of using highly\\ncapable text and image generation models for\\nsemantic counterfactual fine-tuning. Our work\\npioneers an approach in addressing these gaps.\\nWe first spotlight the near-chance performance\\nof multimodal models like CLIP and LLaV A in\\nphysically grounded compositional reasoning.\\nWe then apply simple data augmentation using\\ngrounded image generation model GLIGEN to\\ngenerate fine-tuning data, resulting in signif-\\nicant performance improvements: +33% and',\n",
       " 'source': 'pdfs/2402_13254/2402_13254.pdf',\n",
       " 'page': 0,\n",
       " 'pk': 448370747954954739}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymilvus import Collection\n",
    "collection = Collection(\"MilvusDocs\")      # Get an existing collection.\n",
    "collection.load()\n",
    "\n",
    "result = collection.query(\n",
    "  expr=\"\", \n",
    "  output_fields = [\"text\", \"source\", \"page\"], # only other option is vector\n",
    "  limit = 10\n",
    ")\n",
    "\n",
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b898b179-cd0d-4080-a26f-2790bb75e05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae Lee1\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\n1University of Wisconsin–Madison2Microsoft Research\\nhttps://countercurate.github.io\\nAbstract\\nWe propose CounterCurate, a framework to\\ncomprehensively improve the visio-linguistic\\ncompositional reasoning capability for both\\ncontrastive and generative multimodal models.\\nIn particular, we identify two critical under-\\nexplored problems: the neglect of physically'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c63d7-e6a2-4689-ac5c-533fef5221cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c525fd8-e29f-43bf-b1f8-0c7974a6304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"summarize the paper <re.Match object; span=(0, 315), match='CounterCurate: Enhancing Physical and Semantic Vi> \",\n",
       " 'chat_history': '',\n",
       " 'answer': 'The paper \"CounterCurate\" proposes a framework to enhance visio-linguistic compositional reasoning in multimodal models. It addresses the neglect of physically grounded reasoning and utilizes text and image generation models for semantic counterfactual fine-tuning. The framework shows significant performance improvements on benchmarks, outperforming other models. By curating challenging semantic counterfactuals and fine-tuning with accurate negative samples, CounterCurate demonstrates effectiveness in improving compositional reasoning capabilities. The authors will release their code, dataset, benchmark, and checkpoints for further research.',\n",
       " 'source_documents': [Document(page_content='CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae Lee1\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\n1University of Wisconsin–Madison2Microsoft Research\\nhttps://countercurate.github.io\\nAbstract\\nWe propose CounterCurate, a framework to\\ncomprehensively improve the visio-linguistic\\ncompositional reasoning capability for both\\ncontrastive and generative multimodal models.\\nIn particular, we identify two critical under-\\nexplored problems: the neglect of physically', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0, 'pk': 448370747954954738}),\n",
       "  Document(page_content='5 Conclusion\\nIn conclusion, CounterCurate significantly en-\\nhances the visio-linguistic compositional reasoning\\ncapabilities of multimodal contrastive and genera-\\ntive models. This is achieved by addressing the ne-\\nglect of physically grounded reasoning and exploit-\\ning the potential of using text and image generation\\nmodels for semantic counterfactual fine-tuning. We\\nbelieve our contributions can pave the way for fur-\\nther research in compositional reasoning.\\nAcknowledgement\\nThis work was supported in part by NSF CA-\\nREER IIS2150012, and Institute of Information', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 8, 'pk': 448370747954954806}),\n",
       "  Document(page_content='icant performance improvements: +33% and\\n+37% for CLIP and LLaV A, respectively, on\\nour newly curated Flickr30k-Positions bench-\\nmark. Moreover, we exploit the capabilities\\nof high-performing text generation and image\\ngeneration models, specifically GPT-4V and\\nDALLE-3, to curate challenging semantic coun-\\nterfactuals, thereby further enhancing compo-\\nsitional reasoning capabilities on benchmarks\\nsuch as SugarCrepe, where CounterCurate out-\\nperforms GPT-4V .\\nTo facilitate future research, we will release our\\ncode, dataset, benchmark, and checkpoints at\\nhttps://countercurate.github.io/ .', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0, 'pk': 448370747954954740}),\n",
       "  Document(page_content='CounterCurate shows better performance com-\\npared to the prior rule-based methods or less desir-\\nable models that generates negatives. These signifi-\\ncant improvements show that fine-tuning with accu-\\nrate and hard negative samples is important, again\\ndemonstrating the effectiveness of our data cura-\\ntion and fine-tuning pipeline for both contrastive\\nmodels and text generation models for improving\\nsemantic counterfactual understanding.\\n4.5 In-Depth Analysis\\nEffectiveness of Negative Images, Negative Cap-\\ntions, and Grouping The core of CounterCurate', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 7, 'pk': 448370747954954800})]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# chat_history = vector_store\n",
    "# SYSTEM_PROMPT = \"What are the key contributions of this paper and the evaluation metrics for the paper titled CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"summarize the paper {text_before_abstract} \"\n",
    "\n",
    "chat = llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) \n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, retriever=vector_store.as_retriever(), return_source_documents=True\n",
    ")\n",
    "\n",
    "result =  qa({\"question\": SYSTEM_PROMPT, \"chat_history\": \"\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79de1892-0297-4404-beec-17093720f607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper \"CounterCurate\" proposes a framework to enhance visio-linguistic compositional reasoning in multimodal models. It addresses the neglect of physically grounded reasoning and utilizes text and image generation models for semantic counterfactual fine-tuning. The framework shows significant performance improvements on benchmarks, outperforming other models. By curating challenging semantic counterfactuals and fine-tuning with accurate negative samples, CounterCurate demonstrates effectiveness in improving compositional reasoning capabilities. The authors will release their code, dataset, benchmark, and checkpoints for further research.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ba45aa4-fb24-4c5f-845d-361840aec8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"tell me about the research approach of the paper {text_before_abstract} and key findings. What metrics are used to evaluate performance?\"\n",
    "\n",
    "chat = llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) \n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, retriever=vector_store.as_retriever(), return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f558e41a-127b-4745-9c96-ed52aeca9a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"tell me about the research approach of the paper <re.Match object; span=(0, 315), match='CounterCurate: Enhancing Physical and Semantic Vi> and key findings. What metrics are used to evaluate performance?\",\n",
       " 'chat_history': '',\n",
       " 'answer': 'The research approach of the paper \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\" involves proposing the CounterCurate framework to improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. The framework addresses the neglect of physically grounded reasoning and utilizes text and image generation models for semantic counterfactual fine-tuning.\\n\\nKey findings of the paper include significant performance improvements compared to vanilla CLIP/LLaV A-1.5 model and advanced models like GPT-4V. The fine-tuned CounterCurate model outperforms the state-of-the-art LMM GPT-4V in various categories, showing significant improvements over the \"add\" category. The framework also demonstrates better performance on datasets like Flickr30k-Positions and benchmarks like SugarCrepe.\\n\\nTo evaluate performance, the paper uses metrics such as percentage improvements over baseline models like CLIP and LLaV A, as well as comparisons against advanced models like GPT-4V. Specific performance metrics are not detailed in the provided context, but the paper mentions significant performance boosts (+33% and +37% for CLIP and LLaV A) on the newly curated Flickr30k-Positions benchmark. Additionally, improvements on other datasets are observed and mentioned in the paper\\'s Appendix E.',\n",
       " 'source_documents': [Document(page_content='CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae Lee1\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\n1University of Wisconsin–Madison2Microsoft Research\\nhttps://countercurate.github.io\\nAbstract\\nWe propose CounterCurate, a framework to\\ncomprehensively improve the visio-linguistic\\ncompositional reasoning capability for both\\ncontrastive and generative multimodal models.\\nIn particular, we identify two critical under-\\nexplored problems: the neglect of physically', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0, 'pk': 448370747954954738}),\n",
       "  Document(page_content='and generative multimodal models. The best performances are bolded, and improvements against CLIP and LLaV A\\nare measured in the parentheses. CounterCurate shows significant performance boost compared to both vanilla\\nCLIP/LLaV A-1.5 model and advanced models such as GPT-4V .\\nour fine-tuned model outperforms the SOTA LMM\\nGPT-4V both on average and in two of the cate-\\ngories, most significantly over the “add” category.\\nWe observe improvements on other datasets (Di-\\nwan et al., 2022) as well in Appendix E.\\nCounterCurate shows better performance com-', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 7, 'pk': 448370747954954799}),\n",
       "  Document(page_content='icant performance improvements: +33% and\\n+37% for CLIP and LLaV A, respectively, on\\nour newly curated Flickr30k-Positions bench-\\nmark. Moreover, we exploit the capabilities\\nof high-performing text generation and image\\ngeneration models, specifically GPT-4V and\\nDALLE-3, to curate challenging semantic coun-\\nterfactuals, thereby further enhancing compo-\\nsitional reasoning capabilities on benchmarks\\nsuch as SugarCrepe, where CounterCurate out-\\nperforms GPT-4V .\\nTo facilitate future research, we will release our\\ncode, dataset, benchmark, and checkpoints at\\nhttps://countercurate.github.io/ .', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0, 'pk': 448370747954954740}),\n",
       "  Document(page_content='5 Conclusion\\nIn conclusion, CounterCurate significantly en-\\nhances the visio-linguistic compositional reasoning\\ncapabilities of multimodal contrastive and genera-\\ntive models. This is achieved by addressing the ne-\\nglect of physically grounded reasoning and exploit-\\ning the potential of using text and image generation\\nmodels for semantic counterfactual fine-tuning. We\\nbelieve our contributions can pave the way for fur-\\nther research in compositional reasoning.\\nAcknowledgement\\nThis work was supported in part by NSF CA-\\nREER IIS2150012, and Institute of Information', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 8, 'pk': 448370747954954806})]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 =  qa({\"question\": SYSTEM_PROMPT, \"chat_history\": \"\"})\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6e2e174-f9a8-454e-a971-d80bdab947ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The research approach of the paper \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\" involves proposing the CounterCurate framework to improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. The framework addresses the neglect of physically grounded reasoning and utilizes text and image generation models for semantic counterfactual fine-tuning.\\n\\nKey findings of the paper include significant performance improvements compared to vanilla CLIP/LLaV A-1.5 model and advanced models like GPT-4V. The fine-tuned CounterCurate model outperforms the state-of-the-art LMM GPT-4V in various categories, showing significant improvements over the \"add\" category. The framework also demonstrates better performance on datasets like Flickr30k-Positions and benchmarks like SugarCrepe.\\n\\nTo evaluate performance, the paper uses metrics such as percentage improvements over baseline models like CLIP and LLaV A, as well as comparisons against advanced models like GPT-4V. Specific performance metrics are not detailed in the provided context, but the paper mentions significant performance boosts (+33% and +37% for CLIP and LLaV A) on the newly curated Flickr30k-Positions benchmark. Additionally, improvements on other datasets are observed and mentioned in the paper\\'s Appendix E.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff138a37-6442-44aa-8c36-32e7268948a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create PromptTemplates\u001b[39;00m\n\u001b[1;32m     12\u001b[0m SUMMARY_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39msummary_template, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_before_abstract\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m GUEST_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguest_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m HOST_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mhost_template)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# LLM to use in each stage\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tts-yt/lib/python3.9/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/tts-yt/lib/python3.9/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/anaconda3/envs/tts-yt/lib/python3.9/site-packages/pydantic/v1/main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/anaconda3/envs/tts-yt/lib/python3.9/site-packages/langchain_core/prompts/base.py:127\u001b[0m, in \u001b[0;36mBasePromptTemplate.validate_variable_names\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@root_validator\u001b[39m()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_variable_names\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate variable names do not include restricted names.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_variables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have an input variable named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, as it is used internally,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m please rename.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_variables\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_variables'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Define your prompt templates\n",
    "summary_template = f\"Summarize the key points of the paper {text_before_abstract}. What is the key research contribution of the paper {text_before_abstract}\"\n",
    "guest_template = f\"What are the previous or related work and existing advancements that the paper {text_before_abstract} is based upon and is advancing?\"\n",
    "host_template = f\"tell me about the research approach of the paper {text_before_abstract} and key findings. What metrics are used to evaluate performance?\"\n",
    "\n",
    "# Create PromptTemplates\n",
    "SUMMARY_PROMPT = PromptTemplate(template=summary_template, input_variables=[\"text_before_abstract\"])\n",
    "GUEST_PROMPT = PromptTemplate(template=guest_template)\n",
    "HOST_PROMPT = PromptTemplate(template=host_template)\n",
    "\n",
    "# LLM to use in each stage\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create LLMChains for each stage\n",
    "summary_llm_chain = LLMChain(llm=llm, prompt=SUMMARY_PROMPT)\n",
    "guest_llm_chain = LLMChain(llm=llm, prompt=GUEST_PROMPT)\n",
    "host_llm_chain = LLMChain(llm=llm, prompt=HOST_PROMPT)\n",
    "\n",
    "# Create SequentialDocumentsChain\n",
    "chain = SimpleSequentialChain(\n",
    "    chains=[summary_llm_chain, guest_llm_chain, host_llm_chain], verbose=True, input_variables=[\"text_before_abstract\"]\n",
    ")\n",
    "\n",
    "output = chain.run(\"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee788305-425f-4799-85c3-162f17bdc61a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SequentialDocumentsChain' from 'langchain.chains' (/Users/lily/anaconda3/envs/tts-yt/lib/python3.9/site-packages/langchain/chains/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialDocumentsChain, LLMChain\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SequentialDocumentsChain' from 'langchain.chains' (/Users/lily/anaconda3/envs/tts-yt/lib/python3.9/site-packages/langchain/chains/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define your prompt templates\n",
    "summary_template = f\"Summarize the key points of the paper {text_before_abstract}. What is the key research contribution of the paper {text_before_abstract}\"\n",
    "guest_template = f\"What are the previous or related work and existing advancements that the paper {text_before_abstract} is based upon and is advancing?\"\n",
    "host_template = f\"tell me about the research approach of the paper {text_before_abstract} and key findings. What metrics are used to evaluate performance?\"\n",
    "\n",
    "# Create PromptTemplates\n",
    "SUMMARY_PROMPT = PromptTemplate(template=summary_template, input_variables=[\"text\"])\n",
    "GUEST_PROMPT = PromptTemplate(template=guest_template, input_variables=[\"text\"])\n",
    "HOST_PROMPT = PromptTemplate(template=host_template, input_variables=[\"text\"])\n",
    "\n",
    "# LLM to use in each stage\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create LLMChains for each stage\n",
    "summary_llm_chain = LLMChain(llm=llm, prompt=SUMMARY_PROMPT)\n",
    "guest_llm_chain = LLMChain(llm=llm, prompt=GUEST_PROMPT)\n",
    "host_llm_chain = LLMChain(llm=llm, prompt=HOST_PROMPT)\n",
    "\n",
    "# Create SequentialDocumentsChain\n",
    "chain = SimpleSequentialChain(\n",
    "    chains=[summary_llm_chain, guest_llm_chain, host_llm_chain]\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "results = chain.run(retriever=vector_store.as_retriever())\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d33a4c6d-5e17-4fc8-97ce-8f47a5e980b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that focuses on the positive side of things. Whenever you analyze a text, you look for the positive aspects and highlight them. Here is the text:\\n{input}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x280286100>, async_client=<openai.resources.completions.AsyncCompletions object at 0x280324430>, openai_api_key='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', openai_proxy='')),\n",
       " 'neutral': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, not favoring any positive or negative aspects. Here is the text:\\n{input}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x280286100>, async_client=<openai.resources.completions.AsyncCompletions object at 0x280324430>, openai_api_key='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', openai_proxy='')),\n",
       " 'negative': LLMChain(prompt=PromptTemplate(input_variables=['input'], template='You are an AI that is designed to find the negative aspects in a text. You analyze a text and show the potential downsides. Here is the text:\\n{input}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x280286100>, async_client=<openai.resources.completions.AsyncCompletions object at 0x280324430>, openai_api_key='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', openai_proxy=''))}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \"\"\"\n",
    "You are a helpful bot that creates a 'thank you' reponse text. \n",
    "If customers are unsatisfied, offer them a real world assitant to talk to. \n",
    "You will get a sentiment and subject as into and evaluate. \n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "outline_template = PromptTemplate(input_variables=[\"input\"], template=response_template)\n",
    "outline_chain = LLMChain(llm=llm, prompt=outline_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "875f27c7-23a3-4e64-bf42-b3af0a620fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_before_abstract = re.search(r'^(.*?)\\bAbstract\\b', docs[0].page_content, re.DOTALL)\n",
    "\n",
    "text_before_abstract = page_header.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfcba75a-14a3-4250-8111-1504da384b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\nCompositional Reasoning via Counterfactual Examples\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae Lee1\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\n1University of Wisconsin–Madison2Microsoft Research\\nhttps://countercurate.github.io\\n',\n",
       " 'outline': '\\nIntroduction:\\n- Introduce the research scientists involved: Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee.\\n- Mention the institution involved: University of Wisconsin-Madison and Microsoft Research.\\n- Explain that this paper details the research project titled \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\".\\n- Our research presents a novel approach to enhancing visual and linguistic reasoning in machines through the use of counterfactual examples.\\n\\nWhy was the research done?\\n- The main motivation behind this research was the need to improve the reasoning capabilities of machines in the realm of visual and linguistic tasks.\\n- Despite the advancements in computer vision and natural language processing, there is still a gap in the ability of machines to understand and reason about the world in a human-like manner.\\n- This paper aims to bridge that gap by proposing a new approach that leverages the power of counterfactual examples.\\n\\nTechnologies previously involved:\\n- The paper discusses the use of deep learning and neural networks in the fields of computer vision and natural language processing.\\n- These technologies have been widely used in various tasks such as object recognition, image captioning, and machine translation.\\n- However, these approaches often struggle',\n",
       " 'summary': 'Hello viewers, I am your host, representing the renowned researchers Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee from the University of Wisconsin-Madison and Microsoft Research. Today, we will be taking a closer look at their recent groundbreaking research project titled \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples.\"\\n\\n    This research was conducted with the aim of improving the reasoning capabilities of machines in the fields of computer vision and natural language processing. Despite the significant progress in these areas, machines still struggle with understanding and reasoning about the world in a human-like manner. This paper presents a new approach that utilizes counterfactual examples to bridge this gap and enhance the visual and linguistic reasoning abilities of machines.\\n\\n    The researchers first studied the current state-of-the-art technologies in computer vision and natural language processing, such as deep learning and neural networks. These technologies have been widely used in various tasks such as object recognition, image captioning, and machine translation. However, they often struggle with reasoning tasks that require structural and compositional understanding.\\n\\n    To address this issue, the researchers proposed a new approach that uses counterfactual examples to enhance the reasoning capabilities of machines. They introduced',\n",
       " 'edit': '\\nHello viewers, I am your host and I am excited to present to you the groundbreaking research project titled \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\" by Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee from the University of Wisconsin-Madison and Microsoft Research.\\n\\nThis research aims to improve the reasoning capabilities of machines in the fields of computer vision and natural language processing. Despite significant progress in these areas, machines still struggle with understanding and reasoning about the world in a human-like manner. This paper presents a new approach that utilizes counterfactual examples to bridge this gap and enhance the visual and linguistic reasoning abilities of machines.\\n\\nTo understand the context of this research, the team first studied the current state-of-the-art technologies in computer vision and natural language processing, such as deep learning and neural networks. These technologies have been widely used in various tasks such as object recognition, image captioning, and machine translation. However, they often struggle with reasoning tasks that require structural and compositional understanding.\\n\\nIn this paper, the researchers proposed a new approach that uses counterfactual examples to enhance the reasoning capabilities of machines. Counterfactual examples are hypothetical scenarios that change'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dish_name = text_before_abstract\n",
    "\n",
    "\n",
    "\n",
    "prompt_outline = PromptTemplate.from_template(\n",
    "    template= \"\"\"\n",
    "    - Use only the the paper {dish_name} to answer the user's question. \n",
    "    - Answer in no less than 4000 characters or tokens. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Outline the key points of the paper but do not output it. Use the outline as a guide to expand on a video voiceover of 3 minutes.\n",
    "    - Edit the outline into a voiceover script for a 5 minute video\n",
    "    - Clearly state why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have at least 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - Pick the top 10 most occurring terminology in {dish_name} and define the terms.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved, how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work and what are future directions that lie ahead.\n",
    "    - Make sure the output is at least 4000 characters or tokens in length, if not reference the paper {dish_name} to answer the previous points more in depth\n",
    "\"\"\")\n",
    "\n",
    "# outline_template = PromptTemplate(input_variables=[\"input\"], template=response_template)\n",
    "chain_outline = LLMChain(llm=llm, prompt=prompt_outline, output_key=\"outline\")\n",
    "\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# This is an LLMChain to write a outline given a dish name and the experience.\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    template=\"\"\"  Use only the the paper {dish_name} to complete this task:\n",
    "    - Use this {outline} as structure and {dish_name} for factual reference to convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    - how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - The output should be 3 times as long as the outline, using the {dish_name} paper to summarize the related work, methods, evaluation metrics in detail.\n",
    "    - refer to the paper {dish_name} to state facts presented, answering how the research was conducted.\n",
    "\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with 'Good day, esteemed viewers.', do not greet audience, do not pretend to be the researcher\n",
    "    - Make sure the output is at least 4000 characters or tokens in length, if not reference the paper {dish_name} to answer the previous points more in depth\n",
    "    - \"\"\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "\n",
    "# This is an LLMChain to write a follow-up edit given the restaurant outline.\n",
    "prompt_edit = PromptTemplate.from_template(\n",
    "    template=\"\"\"Edit the entire script {summary} for good flow, professionalism, redundancy removal in a voiceover text format. \n",
    "    - Make sure that it is obviously stated to the video viewer the related work that the paper is built on, \n",
    "    - In the edit, ensure that the voiceover text states the following specifics, if not, go back to the {dish_name} paper to extract more specific details:\n",
    "    - what research was previously known in the past that the paper is based upon that the paper advances previously known information, what are the technologies that were previously known involved\n",
    "    - specific methods used,\n",
    "    - how is the technique, actions or methods performed advanced the field, or challenges the status-quo\n",
    "    - what are the specific major contributions, and what are the evaluation metrics to prove it. What are the key metrics that define the success of the work\n",
    "    - make sure to always reference the paper {dish_name}  what are the technologies that were previously known involved, \n",
    "    - what are future directions that lie ahead. Cite the grounding sources. \n",
    "    - Always start by stating the title and authors of the paper as the first few words.\n",
    "    - Make sure the output is at least 4000 characters or tokens in length, if not reference the paper {dish_name} to answer the previous points more in depth\n",
    "    \"\"\"\n",
    ")\n",
    "chain_edit = LLMChain(llm=llm, prompt=prompt_edit, output_key=\"edit\")\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_outline, chain_summary, chain_edit],\n",
    "    input_variables=[\"dish_name\"],\n",
    "    output_variables=[\"outline\",\"summary\", \"edit\"],\n",
    ")\n",
    "\n",
    "result = overall_chain({\"dish_name\": text_before_abstract})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "02a59419-d6aa-4607-97e7-e7dafefaedbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello and welcome to our latest production, featuring the groundbreaking research paper titled \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples.\" This paper was written by Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee, esteemed research scientists from the University of Wisconsin-Madison and Microsoft Research.\\n\\nThis paper builds upon previous research in the field of physical and semantic visio-linguistic reasoning. While previous methods have struggled to effectively combine visual and linguistic information, the team behind CounterCurate has introduced the concept of counterfactual examples, greatly enhancing reasoning in this area.\\n\\nTo achieve this, the team developed a framework consisting of an image encoder, text encoder, and counterfactual generator. By utilizing counterfactual examples, the model is able to learn to reason in a more compositional manner, resulting in improved performance on datasets such as COCO and Visual as Structure. This advancement has challenged the status-quo and pushed the boundaries of visio-linguistic reasoning.\\n\\nThe paper\\'s major contributions include the introduction of counterfactual examples and their incorporation into the model, as well as the development of a framework that combines visual and linguistic information in a'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['edit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3dc1486-9bf0-4251-81be-244166725026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The authors of the paper \"CounterCurate: Enhancing Physical and Semantic Vi\" are not explicitly mentioned in the provided text. However, based on the context of the research paper, it is likely that the authors are researchers or scientists involved in the development and implementation of the CounterCurate system.\\n\\nIn this research paper, the authors present the CounterCurate system, which focuses on enhancing both the physical and semantic aspects of virtual reality (VR) experiences. The key objective of this research is to improve the overall quality and realism of VR environments by integrating physical interactions and semantic understanding.\\n\\nThe technologies involved in this research include Milvus, an open-source vector database, and OpenAI Embeddings, which are used to represent and process semantic information within the VR environment. These technologies are known for their capabilities in handling large-scale data and extracting meaningful insights from complex datasets.\\n\\nThe technique employed in this research involves leveraging Milvus and OpenAI Embeddings to enhance the physical and semantic aspects of VR experiences. By combining these technologies, the CounterCurate system is able to create more immersive and interactive VR environments that respond to user actions and provide contextually relevant information.\\n\\nThe key metrics that define the success of this work include the level of user engagement, the realism of the VR environment, and the accuracy of semantic understanding and interactions. By measuring these metrics, the researchers can evaluate the effectiveness of the CounterCurate system in enhancing physical and semantic aspects of VR experiences.\\n\\nIn terms of future directions, the authors suggest further exploration of advanced AI techniques and integration of additional sensory inputs to enhance the overall VR experience. By incorporating more sophisticated algorithms and expanding the capabilities of the system, the researchers aim to push the boundaries of VR technology and create even more realistic and immersive virtual environments.\\n\\nOverall, the research paper \"CounterCurate: Enhancing Physical and Semantic Vi\" highlights the importance of integrating physical interactions and semantic understanding in VR environments to improve user experience and realism. By leveraging technologies such as Milvus and OpenAI Embeddings, the CounterCurate system represents a significant advancement in the field of VR technology and sets the stage for future innovations in this area.')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) \n",
    "    \n",
    "    context = vector_store.as_retriever()\n",
    "\n",
    "    # QUESTION = \"What are the key contributions of this paper and the evaluation metrics for the paper titled CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\"\n",
    "    QUESTION = f\"who are the authors of the paper {text_before_abstract}\"\n",
    "    # Assemble retrieved context\n",
    "    # metadata_fields = [f for f in output_fields if f != 'chunk']\n",
    "\n",
    "    SYSTEM_PROMPT = f\"\"\"Use only the the paper {text_before_abstract} to answer the user's question. Answer in no less than 4000 characters. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Outline the key points of the paper\n",
    "    - Edit the outline into a voiceover script for a 5 minute video\n",
    "    - Clearly state why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Always start by stating the title of the paper as the first few words.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources. \n",
    "    Context: {context}\n",
    "    Question: {QUESTION}\n",
    "    \"\"\"\n",
    "    \n",
    "    rag_prompt = PromptTemplate.from_template(SYSTEM_PROMPT)\n",
    "    rag_chain = (\n",
    "        {\"context\": context, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "    )\n",
    "    \n",
    "    response = rag_chain.invoke(SYSTEM_PROMPT)\n",
    "    response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7a3b734-0b65-4bf8-9674-3cdeb79a633f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\\n\\nOutline Summary:\\n\\n1. Introduction\\n    - Introduce the research paper titled \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\"\\n    - Highlight the key researchers involved in the study and their affiliations\\n    - Provide an overview of the purpose of the research and the technologies involved\\n\\n2. Background\\n    - Discuss the existing technologies and methods related to physical and semantic visio-linguistic compositional reasoning\\n    - Explain the limitations of current approaches and the need for advancements in the field\\n\\n3. Methodology\\n    - Detail the technique of using counterfactual examples to enhance reasoning in physical and semantic visio-linguistic tasks\\n    - Describe how the researchers implemented this technique and the specific actions taken in the study\\n\\n4. Results\\n    - Present the key findings of the research, including improvements in reasoning capabilities and compositional understanding\\n    - Discuss the evaluation metrics used to measure the success of the study and the significance of the results\\n\\n5. Future Directions\\n    - Outline potential future directions for research in enhancing physical and semantic visio-linguistic reasoning\\n    - Address the implications of the study\\'s findings for the advancement of the field and potential areas for further exploration\\n\\n6. Conclusion\\n    - Summarize the major contributions of the research paper and its impact on the field of visio-linguistic reasoning\\n    - Emphasize the importance of using counterfactual examples in enhancing compositional reasoning abilities\\n\\nScript:\\n\\nTitle: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\\n\\nHello, I am here to present an overview of the research paper titled \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples.\" This groundbreaking study was conducted by a team of esteemed researchers from renowned institutions, aiming to advance the field of physical and semantic visio-linguistic reasoning.\\n\\nIn the realm of physical and semantic visio-linguistic compositional reasoning, existing technologies and methods have shown limitations in effectively enhancing reasoning capabilities. This research paper addresses the need for advancements in this field by introducing a novel technique using counterfactual examples.\\n\\nThe methodology employed in this study involves leveraging counterfactual examples to enhance reasoning in physical and semantic visio-linguistic tasks. By implementing this technique, the researchers were able to improve compositional understanding and reasoning abilities in a significant manner.\\n\\nThe results of the research demonstrate a notable enhancement in reasoning capabilities and compositional understanding through the use of counterfactual examples. Key findings highlight the effectiveness of this approach in advancing physical and semantic visio-linguistic reasoning, showcasing the potential for significant progress in the field.\\n\\nEvaluation metrics utilized in the study played a crucial role in defining the success of the research. These metrics provided a quantitative measure of the improvements achieved in reasoning capabilities and compositional understanding, emphasizing the significance of the results obtained.\\n\\nLooking towards the future, potential directions for research in enhancing physical and semantic visio-linguistic reasoning are outlined. The implications of the study\\'s findings suggest promising avenues for further exploration and development in this field, paving the way for continued advancements.\\n\\nIn conclusion, the research paper \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\" presents a significant contribution to the field of visio-linguistic reasoning. By utilizing counterfactual examples, this study has opened new possibilities for enhancing compositional reasoning abilities and driving innovation in the field.\\n\\nBy delving deep into the key findings and evaluation metrics of the research, it becomes evident that the use of counterfactual examples has the potential to revolutionize physical and semantic visio-linguistic reasoning. The future directions outlined in the study offer exciting prospects for further research and development, highlighting the importance of this work in advancing the field.\\n\\nCiting the grounding sources, the research paper \"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples\" stands as a testament to the innovative approach taken by the researchers and the significant impact it has on the field of visio-linguistic reasoning.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieve_context_and_generate_response(vector_store)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b87f57-84ff-43ba-ae44-7b887466b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_list = [\"2402.13254\", \"2403.07874\", \"2403.07872\", \"2403.07870\",\"2403.07869\"]\n",
    "paper_list = [\"2403.07867\",\"2308.08079\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75ab254-dc96-47fe-82cc-5533180a82ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs' already exists.\n",
      "The folder 'images' already exists.\n",
      "The folder 'audio_voiceovers' already exists.\n",
      "The folder 'final_videos' already exists.\n",
      "The folder 'transcripts' already exists.\n",
      "The folder 'pdfs/2403_07867' already exists.\n",
      "\n",
      "PDF downloaded and saved as 2403_07867.pdf\n",
      "\n",
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name WhereIsAI/UAE-Large-V1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datatype of SentenceTransformer encoded object<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "\n",
      "\n",
      "What the encoder object looks like: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "\n",
      "\n",
      "model_name: WhereIsAI/UAE-Large-V1\n",
      "\n",
      "EMBEDDING_DIM: 1024\n",
      "\n",
      "MAX_SEQ_LENGTH: 512\n",
      "\n",
      "Collection had previously been created, dropping previous collection to initialize anew: `MilvusDocs`\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 1024}, 'element_type': 0}], 'aliases': [], 'collection_id': 448349453448642865, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "Start inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 27.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of chunks inserted into Milvus database: 86 with chunk id starting at number: 448349453448642987\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'Answer 1: Title: LazyBoE: Lazy Propagation with Edge Selection for '\n",
      " 'Kinodynamic Planning\\n'\n",
      " '\\n'\n",
      " 'Outline Summary:\\n'\n",
      " '\\n'\n",
      " 'I. Introduction\\n'\n",
      " '    A. Introduce the research paper titled \"LazyBoE: Lazy Propagation with '\n",
      " 'Edge Selection for Kinodynamic Planning.\"\\n'\n",
      " '    B. Mention the research scientists involved and the institutions where '\n",
      " 'the research was conducted.\\n'\n",
      " '    C. Highlight the importance of kinodynamic motion planning and the need '\n",
      " 'for efficient algorithms in this field.\\n'\n",
      " '\\n'\n",
      " 'II. Background\\n'\n",
      " '    A. Discuss the existing challenges in kinodynamic motion planning.\\n'\n",
      " '    B. Explain the significance of lazy techniques in improving '\n",
      " 'performance.\\n'\n",
      " '    C. Introduce the concept of LazyBoE and its approach to kinodynamic '\n",
      " 'planning.\\n'\n",
      " '\\n'\n",
      " 'III. Methodology\\n'\n",
      " '    A. Describe the LazyBoE algorithm in detail, focusing on lazy '\n",
      " 'propagation and edge selection.\\n'\n",
      " '    B. Explain how LazyBoE utilizes probabilistic evaluation of edges to '\n",
      " 'enhance exploration and search efficiency.\\n'\n",
      " '    C. Highlight the key features of LazyBoE that differentiate it from '\n",
      " 'traditional methods.\\n'\n",
      " '\\n'\n",
      " 'IV. Results\\n'\n",
      " '    A. Present the key findings of the research, including success rates and '\n",
      " 'efficiency metrics.\\n'\n",
      " \"    B. Discuss the statistical significance of LazyBoE's performance \"\n",
      " 'improvements compared to baseline algorithms.\\n'\n",
      " '    C. Provide insights into the speed and cost-effectiveness of LazyBoE in '\n",
      " 'kinodynamic planning.\\n'\n",
      " '\\n'\n",
      " 'V. Evaluation Metrics\\n'\n",
      " '    A. Define the key metrics used to evaluate the success of LazyBoE, such '\n",
      " 'as success rates, solution diversity, and efficiency.\\n'\n",
      " '    B. Explain the significance testing conducted using the Mann-Whitney U '\n",
      " 'test to validate the results.\\n'\n",
      " '    C. Discuss the statistical analysis results and the implications for '\n",
      " 'advancing the field of kinodynamic planning.\\n'\n",
      " '\\n'\n",
      " 'VI. Conclusion and Future Directions\\n'\n",
      " '    A. Summarize the major contributions of LazyBoE in kinodynamic motion '\n",
      " 'planning.\\n'\n",
      " '    B. Highlight the future directions and potential applications of LazyBoE '\n",
      " 'in robotics and automation.\\n'\n",
      " '    C. Emphasize the significance of LazyBoE in improving performance '\n",
      " 'without sacrificing optimality.\\n'\n",
      " '\\n'\n",
      " 'Complete Script:\\n'\n",
      " '\\n'\n",
      " 'Title: LazyBoE: Lazy Propagation with Edge Selection for Kinodynamic '\n",
      " 'Planning\\n'\n",
      " '\\n'\n",
      " 'Hello, and welcome to this overview of the research paper titled \"LazyBoE: '\n",
      " 'Lazy Propagation with Edge Selection for Kinodynamic Planning.\" This '\n",
      " 'groundbreaking study was conducted by a team of researchers at the '\n",
      " 'Department of Computer Science, University of Colorado Boulder, aiming to '\n",
      " 'revolutionize kinodynamic motion planning through the LazyBoE algorithm.\\n'\n",
      " '\\n'\n",
      " 'Kinodynamic motion planning poses significant challenges in robotics, '\n",
      " 'requiring efficient algorithms to navigate complex environments. The LazyBoE '\n",
      " 'approach addresses these challenges by introducing lazy propagation and edge '\n",
      " 'selection techniques to enhance exploration and search efficiency.\\n'\n",
      " '\\n'\n",
      " 'Lazy techniques have shown promise in improving performance in kinodynamic '\n",
      " 'planning by minimizing wasted computations and finding high-quality '\n",
      " 'solutions quickly. LazyBoE builds upon these principles, offering a '\n",
      " 'multi-query approach that leverages probabilistic evaluation of edges to '\n",
      " 'expand the search frontier rapidly.\\n'\n",
      " '\\n'\n",
      " 'The LazyBoE algorithm stands out by evaluating edges probabilistically, '\n",
      " 'allowing for their reuse and adding diversity to exploration. This approach '\n",
      " 'leads to higher success rates, with LazyBoE achieving a remarkable 92% '\n",
      " 'success rate compared to 80-88% for other methods, showcasing its efficiency '\n",
      " 'and effectiveness in kinodynamic planning.\\n'\n",
      " '\\n'\n",
      " \"Key findings of the research demonstrate LazyBoE's speedup improvement, with \"\n",
      " 'statistically significant results compared to baseline algorithms such as '\n",
      " \"RRT and SST. LazyBoE's performance enhancements are evident in its speed, \"\n",
      " 'cost-effectiveness, and solution diversity, making it a promising '\n",
      " 'advancement in the field of kinodynamic motion planning.\\n'\n",
      " '\\n'\n",
      " 'The evaluation metrics used in this study include success rates, solution '\n",
      " 'diversity, and efficiency, all of which validate the effectiveness of '\n",
      " 'LazyBoE in generating collision-free solution paths quickly while '\n",
      " 'maintaining a broad exploration. Statistical analysis using the Mann-Whitney '\n",
      " 'U test further reinforces the results, highlighting the significance of '\n",
      " \"LazyBoE's performance improvements.\\n\"\n",
      " '\\n'\n",
      " 'In conclusion, LazyBoE offers a competitive solution in terms of quality and '\n",
      " 'efficiency, with results comparable to the best-performing alternatives in '\n",
      " \"kinodynamic planning. The algorithm's ability to find diverse solutions \"\n",
      " 'quickly without sacrificing optimality sets it apart in the field, paving '\n",
      " 'the way for future advancements in robotics and automation.\\n'\n",
      " '\\n'\n",
      " 'Moving forward, the future directions for LazyBoE include exploring its '\n",
      " 'applications in various robotic systems and environments, further optimizing '\n",
      " \"its performance, and integrating it into real-world scenarios. LazyBoE's \"\n",
      " 'success opens up new possibilities for enhancing kinodynamic motion planning '\n",
      " 'and pushing the boundaries of robotics research.\\n'\n",
      " '\\n'\n",
      " 'In summary, the LazyBoE algorithm represents a significant leap forward in '\n",
      " 'kinodynamic motion planning, offering a blend of efficiency, speed, and '\n",
      " 'cost-effectiveness that sets a new standard in the field. With its '\n",
      " 'innovative approach and promising results, LazyBoE is poised to shape the '\n",
      " 'future of robotics and automation, driving advancements that were previously '\n",
      " 'thought to be unattainable.\\n'\n",
      " '\\n'\n",
      " 'Thank you for joining us in this exploration of LazyBoE and its '\n",
      " 'transformative impact on kinodynamic planning.\\n')\n",
      "\n",
      "Transcript saved in: transcripts/2403_07867.txt\n",
      "Recording file output_2403_07867.mp3 already exists in audio_voiceovers. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All pages converted and saved in the folder: images/2403_07867_pngs\n",
      "\n",
      ".ppm artifacts deleted in the folder: images/2403_07867_pngs\n",
      "\n",
      "Images saved: 2403_07867_page_1.png_cropped_1.png (top) and 2403_07867_page_1.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2403_07867_page_3.png_cropped_1.png (top) and 2403_07867_page_3.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2403_07867_page_2.png_cropped_1.png (top) and 2403_07867_page_2.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2403_07867_page_6.png_cropped_1.png (top) and 2403_07867_page_6.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2403_07867_page_7.png_cropped_1.png (top) and 2403_07867_page_7.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2403_07867_page_5.png_cropped_1.png (top) and 2403_07867_page_5.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2403_07867_page_4.png_cropped_1.png (top) and 2403_07867_page_4.png_cropped_2.png (bottom)\n",
      "Error moving file 2403_07867_page_1.png: name 'shutil' is not defined\n",
      "Error moving file 2403_07867_page_3.png: name 'shutil' is not defined\n",
      "Error moving file 2403_07867_page_2.png: name 'shutil' is not defined\n",
      "Error moving file 2403_07867_page_6.png: name 'shutil' is not defined\n",
      "Error moving file 2403_07867_page_7.png: name 'shutil' is not defined\n",
      "Error moving file 2403_07867_page_5.png: name 'shutil' is not defined\n",
      "Error moving file 2403_07867_page_4.png: name 'shutil' is not defined\n",
      "All non-cropped files moved to the folder: images/2403_07867_pngs/uncropped_pngs\n",
      "File removed: 2403_07867_page_1.png\n",
      "File removed: 2403_07867_page_3.png\n",
      "File removed: 2403_07867_page_2.png\n",
      "File removed: 2403_07867_page_6.png\n",
      "File removed: 2403_07867_page_7.png\n",
      "File removed: 2403_07867_page_5.png\n",
      "File removed: 2403_07867_page_4.png\n",
      "All non-cropped files removed in the folder: images/2403_07867_pngs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_videos/2403_07867.mp4.\n",
      "MoviePy - Writing audio in 2403_07867TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_videos/2403_07867.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_videos/2403_07867.mp4\n",
      "\n",
      "Final video saved at: final_videos/2403_07867.mp4.\n",
      "The folder 'pdfs' already exists.\n",
      "The folder 'images' already exists.\n",
      "The folder 'audio_voiceovers' already exists.\n",
      "The folder 'final_videos' already exists.\n",
      "The folder 'transcripts' already exists.\n",
      "The folder 'pdfs/2308_08079' has been created.\n",
      "\n",
      "PDF downloaded and saved as 2308_08079.pdf\n",
      "\n",
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name WhereIsAI/UAE-Large-V1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datatype of SentenceTransformer encoded object<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "\n",
      "\n",
      "What the encoder object looks like: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "\n",
      "\n",
      "model_name: WhereIsAI/UAE-Large-V1\n",
      "\n",
      "EMBEDDING_DIM: 1024\n",
      "\n",
      "MAX_SEQ_LENGTH: 512\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 1024}, 'element_type': 0}], 'aliases': [], 'collection_id': 448349453449043471, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "Start inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of chunks inserted into Milvus database: 156 with chunk id starting at number: 448349453448643074\n",
      "\n",
      "('\\n'\n",
      " 'Answer 1: Title: Advancements in Lower-Dimensional Space Stabilization Using '\n",
      " 'Rigid Transformations: A Novel Approach\\n'\n",
      " '\\n'\n",
      " 'Hello, esteemed viewers. Today, we will delve into a groundbreaking research '\n",
      " 'paper titled \"Advancements in Lower-Dimensional Space Stabilization Using '\n",
      " 'Rigid Transformations: A Novel Approach.\" This paper, authored by a team of '\n",
      " 'esteemed researchers, presents a novel technique that significantly advances '\n",
      " \"the field of lower-dimensional space stabilization. Let's explore the key \"\n",
      " 'contributions of this paper and the evaluation metrics that demonstrate how '\n",
      " 'this work pushes the boundaries of previously known information.\\n'\n",
      " '\\n'\n",
      " '1. Introduction:\\n'\n",
      " '- The research team, led by prominent scientists, embarked on this study to '\n",
      " 'address the challenges associated with lower-dimensional space '\n",
      " 'stabilization.\\n'\n",
      " '- The paper builds upon existing knowledge in the field of data analysis and '\n",
      " 'spatial data processing to introduce a novel approach using rigid '\n",
      " 'transformations.\\n'\n",
      " '\\n'\n",
      " '2. Background and Previous Technologies:\\n'\n",
      " '- The paper references prior works by renowned researchers such as Aggarwal, '\n",
      " 'Hinneburg, and Keim, highlighting the behavior of distance metrics in '\n",
      " 'high-dimensional space.\\n'\n",
      " '- Previous studies by Law, Zhang, and Jain focused on nonlinear manifold '\n",
      " 'learning for data streams, providing a foundation for the current research.\\n'\n",
      " '- The technique of rigid transformations, initially applied in the '\n",
      " 'subsurface domain, is extended to two-dimensional datasets, showcasing a '\n",
      " 'significant advancement in the field.\\n'\n",
      " '\\n'\n",
      " '3. Advancements in the Field:\\n'\n",
      " '- The key innovation presented in this paper is the application of rigid '\n",
      " 'transformations to stabilize lower-dimensional spaces, ensuring robustness '\n",
      " 'and consistency in data analysis.\\n'\n",
      " '- By introducing a novel workflow that incorporates rigid transformations, '\n",
      " 'the research team demonstrates the effectiveness of the approach in '\n",
      " 'enhancing the stability and reliability of lower-dimensional '\n",
      " 'representations.\\n'\n",
      " '\\n'\n",
      " '4. Evaluation Metrics and Key Findings:\\n'\n",
      " '- The success of the proposed technique is evaluated based on the stability '\n",
      " 'and invariance of the lower-dimensional space under rigid transformations.\\n'\n",
      " '- The research team introduces a theorem, Theorem A.1, which establishes the '\n",
      " 'ensemble expectation of rigid transformed solutions for multiple '\n",
      " 'realizations, demonstrating the efficacy of the approach.\\n'\n",
      " '- Evaluation metrics include the computation of dissimilarity matrices, '\n",
      " 'visualization of uncertainty spaces, and analysis of out-of-sample points '\n",
      " 'within confidence intervals, showcasing the robustness and accuracy of the '\n",
      " 'method.\\n'\n",
      " '\\n'\n",
      " '5. Future Directions:\\n'\n",
      " '- The paper sets the stage for future research directions in the field of '\n",
      " 'lower-dimensional space stabilization, emphasizing the importance of further '\n",
      " 'exploration and validation of the proposed technique.\\n'\n",
      " '- Future studies could focus on expanding the application of rigid '\n",
      " 'transformations to diverse datasets and domains, enhancing the scalability '\n",
      " 'and versatility of the approach.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, \"Advancements in Lower-Dimensional Space Stabilization Using '\n",
      " 'Rigid Transformations: A Novel Approach\" presents a significant contribution '\n",
      " 'to the field of data analysis and spatial data processing. The evaluation '\n",
      " 'metrics outlined in the paper demonstrate the effectiveness and reliability '\n",
      " 'of the proposed technique, paving the way for future advancements in '\n",
      " 'lower-dimensional space stabilization. This research not only expands our '\n",
      " 'understanding of data analysis but also opens up new possibilities for '\n",
      " 'enhancing the stability and robustness of lower-dimensional '\n",
      " 'representations.\\n')\n",
      "\n",
      "Transcript saved in: transcripts/2308_08079.txt\n",
      "\n",
      "Recording saved in audio_voiceovers/output_2308_08079.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All pages converted and saved in the folder: images/2308_08079_pngs\n",
      "\n",
      ".ppm artifacts deleted in the folder: images/2308_08079_pngs\n",
      "\n",
      "Images saved: 2308_08079_page_15.png_cropped_1.png (top) and 2308_08079_page_15.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_29.png_cropped_1.png (top) and 2308_08079_page_29.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_28.png_cropped_1.png (top) and 2308_08079_page_28.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_14.png_cropped_1.png (top) and 2308_08079_page_14.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_16.png_cropped_1.png (top) and 2308_08079_page_16.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_17.png_cropped_1.png (top) and 2308_08079_page_17.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_13.png_cropped_1.png (top) and 2308_08079_page_13.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_12.png_cropped_1.png (top) and 2308_08079_page_12.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_10.png_cropped_1.png (top) and 2308_08079_page_10.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_11.png_cropped_1.png (top) and 2308_08079_page_11.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_8.png_cropped_1.png (top) and 2308_08079_page_8.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_9.png_cropped_1.png (top) and 2308_08079_page_9.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_1.png_cropped_1.png (top) and 2308_08079_page_1.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_2.png_cropped_1.png (top) and 2308_08079_page_2.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_3.png_cropped_1.png (top) and 2308_08079_page_3.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_7.png_cropped_1.png (top) and 2308_08079_page_7.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_6.png_cropped_1.png (top) and 2308_08079_page_6.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_4.png_cropped_1.png (top) and 2308_08079_page_4.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_5.png_cropped_1.png (top) and 2308_08079_page_5.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_20.png_cropped_1.png (top) and 2308_08079_page_20.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_21.png_cropped_1.png (top) and 2308_08079_page_21.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_23.png_cropped_1.png (top) and 2308_08079_page_23.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_22.png_cropped_1.png (top) and 2308_08079_page_22.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_26.png_cropped_1.png (top) and 2308_08079_page_26.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_27.png_cropped_1.png (top) and 2308_08079_page_27.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_19.png_cropped_1.png (top) and 2308_08079_page_19.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_25.png_cropped_1.png (top) and 2308_08079_page_25.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_24.png_cropped_1.png (top) and 2308_08079_page_24.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_30.png_cropped_1.png (top) and 2308_08079_page_30.png_cropped_2.png (bottom)\n",
      "\n",
      "Images saved: 2308_08079_page_18.png_cropped_1.png (top) and 2308_08079_page_18.png_cropped_2.png (bottom)\n",
      "Error moving file 2308_08079_page_15.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_29.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_28.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_14.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_16.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_17.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_13.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_12.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_10.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_11.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_8.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_9.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_1.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_2.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_3.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_7.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_6.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_4.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_5.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_20.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_21.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_23.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_22.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_26.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_27.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_19.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_25.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_24.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_30.png: name 'shutil' is not defined\n",
      "Error moving file 2308_08079_page_18.png: name 'shutil' is not defined\n",
      "All non-cropped files moved to the folder: images/2308_08079_pngs/uncropped_pngs\n",
      "File removed: 2308_08079_page_15.png\n",
      "File removed: 2308_08079_page_29.png\n",
      "File removed: 2308_08079_page_28.png\n",
      "File removed: 2308_08079_page_14.png\n",
      "File removed: 2308_08079_page_16.png\n",
      "File removed: 2308_08079_page_17.png\n",
      "File removed: 2308_08079_page_13.png\n",
      "File removed: 2308_08079_page_12.png\n",
      "File removed: 2308_08079_page_10.png\n",
      "File removed: 2308_08079_page_11.png\n",
      "File removed: 2308_08079_page_8.png\n",
      "File removed: 2308_08079_page_9.png\n",
      "File removed: 2308_08079_page_1.png\n",
      "File removed: 2308_08079_page_2.png\n",
      "File removed: 2308_08079_page_3.png\n",
      "File removed: 2308_08079_page_7.png\n",
      "File removed: 2308_08079_page_6.png\n",
      "File removed: 2308_08079_page_4.png\n",
      "File removed: 2308_08079_page_5.png\n",
      "File removed: 2308_08079_page_20.png\n",
      "File removed: 2308_08079_page_21.png\n",
      "File removed: 2308_08079_page_23.png\n",
      "File removed: 2308_08079_page_22.png\n",
      "File removed: 2308_08079_page_26.png\n",
      "File removed: 2308_08079_page_27.png\n",
      "File removed: 2308_08079_page_19.png\n",
      "File removed: 2308_08079_page_25.png\n",
      "File removed: 2308_08079_page_24.png\n",
      "File removed: 2308_08079_page_30.png\n",
      "File removed: 2308_08079_page_18.png\n",
      "All non-cropped files removed in the folder: images/2308_08079_pngs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_videos/2308_08079.mp4.\n",
      "MoviePy - Writing audio in 2308_08079TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_videos/2308_08079.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_videos/2308_08079.mp4\n",
      "\n",
      "Final video saved at: final_videos/2308_08079.mp4.\n"
     ]
    }
   ],
   "source": [
    "for paper in paper_list:\n",
    "\n",
    "    url = f\"https://arxiv.org/pdf/{paper}.pdf\"\n",
    "    process_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d92e0-24b3-4dd3-834b-cf42ef7396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_url,\n",
    "    inputs=gr.Textbox(placeholder=\"Enter arXiv PDF URL\"),\n",
    "    outputs=gr.Video(),\n",
    "    live=True,\n",
    "    theme=\"sky\",\n",
    "    flagging_options=None,  # Disable the flag button\n",
    "    title=\"Arxiv2Video\",\n",
    ")\n",
    "\n",
    "# Add a submit button\n",
    "submit_button = gr.Button()\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23e133-4b96-4ddb-ab6f-b50851a71a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18507a8-242a-4f68-b551-a7212c325155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
