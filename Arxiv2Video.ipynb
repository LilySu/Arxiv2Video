{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72d006d7-cc2a-4de2-a3cb-3a7507a4d8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ez_setup\n",
      "  Downloading ez_setup-0.9.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ez_setup\n",
      "  Building wheel for ez_setup (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ez_setup: filename=ez_setup-0.9-py3-none-any.whl size=10998 sha256=bc16389f65b2c310e526e149e3208f8dc31a99ea43e08726658e4cc62006921b\n",
      "  Stored in directory: /Users/lilysu/Library/Caches/pip/wheels/99/10/f0/d0b8db5bf513290d7f30f8d0fb836532af46782bae00146ee9\n",
      "Successfully built ez_setup\n",
      "Installing collected packages: ez_setup\n",
      "Successfully installed ez_setup-0.9\n",
      "Requirement already satisfied: moviepy in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (1.24.3)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (68.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install gradio\n",
    "# !pip install langchain\n",
    "# !pip install pypdf\n",
    "# !pip install selenium Pillow\n",
    "# !pip install pdf2image\n",
    "# !pip install feedparser\n",
    "# !pip install milvus\n",
    "# !pip install pymilvus\n",
    "# !pip install torch sentence_transformers\n",
    "# !pip install langchain_openai\n",
    "# !pip install --upgrade sentence_transformers\n",
    "# !pip install moviepy\n",
    "# !pip install opencv-python\n",
    "# !pip install ffmpeg-python\n",
    "# !pip install moviepy --upgrade\n",
    "!pip install ez_setup\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffae872-e745-4017-a12e-e7c65b714cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages/gradio/blocks.py:565: UserWarning: Cannot load sky. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/sky (Request ID: Root=1-65edf23c-56083a2147356792721c3baf;dcae1c72-1b27-4a16-bc56-20c16f034cf1)\n",
      "\n",
      "Sorry, we can't find the page you are looking for.\n",
      "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://72ad4fd02bc8ecefdd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://72ad4fd02bc8ecefdd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "import feedparser\n",
    "import requests\n",
    "\n",
    "def download_and_save_pdf(url):\n",
    "    arxiv_id = arxiv_id_from_url(url)\n",
    "    if arxiv_id:\n",
    "        try:\n",
    "            # Make a request to the arXiv API\n",
    "            feed = feedparser.parse(f'http://export.arxiv.org/api/query?id_list={arxiv_id}')\n",
    "\n",
    "            # Check if the response contains entries\n",
    "            if 'entries' in feed:\n",
    "                # Iterate over each entry (paper) in the feed\n",
    "                for entry in feed.entries:\n",
    "                    # Extract the PDF link from the entry\n",
    "                    pdf_link = entry.link.replace('/abs/', '/pdf/') + '.pdf'\n",
    "\n",
    "                    # Download the PDF\n",
    "                    response = requests.get(pdf_link)\n",
    "\n",
    "                    # Save the PDF in the local directory with the name based on the arXiv ID\n",
    "                    with open(f'{arxiv_id}.pdf', 'wb') as pdf_file:\n",
    "                        pdf_file.write(response.content)\n",
    "\n",
    "                    return f\"PDF downloaded and saved as {arxiv_id}.pdf\"\n",
    "\n",
    "            else:\n",
    "                return f\"No entries found for arXiv ID {arxiv_id}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting information: {e}\"\n",
    "    else:\n",
    "        return \"Invalid arXiv PDF URL format. Please enter a valid URL.\"\n",
    "\n",
    "def arxiv_id_from_url(url):\n",
    "    # Extract the arXiv ID from the URL using a regular expression\n",
    "    match = re.search(r'arxiv\\.org/pdf/(\\d+\\.\\d+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    ".container {\n",
    "    width: 40%;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    with gr.Column(elem_classes=[\"container\"]):\n",
    "        # elem_classes=[\"container\"],\n",
    "        fn=download_and_save_pdf,\n",
    "        inputs=gr.Textbox(placeholder=\"Enter arXiv PDF URL\"),\n",
    "        outputs=\"text\",\n",
    "        theme=\"sky\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=download_and_save_pdf,\n",
    "    inputs=gr.Textbox(placeholder=\"Enter arXiv PDF URL\"),  # Set width as needed\n",
    "    outputs=gr.Textbox(visible=False),  # Dummy output\n",
    "    theme=\"sky\",\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n",
    "# https://61cc1e9bf3c8b6ff21.gradio.live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c48f76-70d1-4669-9576-634f0a7283c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e34293-a3ef-44e9-b83d-3bec8df639d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:19530\n"
     ]
    }
   ],
   "source": [
    "from milvus import default_server\n",
    "from pymilvus import connections, utility\n",
    "default_server.start()\n",
    "connections.connect(host=\"127.0.0.1\", port=default_server.listen_port)\n",
    "port=default_server.listen_port\n",
    "host=\"127.0.0.1\"\n",
    "my_uri = \"http://localhost:\" + str(port)\n",
    "print(my_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c3d21a-d77e-44d4-bbfe-596d69400d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 7 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "loader = PyPDFLoader(\"1808.05092.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e96fc0f-f2ca-4028-ac2e-670449ae8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our research presents a paper detailing a non-parallel many-to-many voice conversion method using an auxiliary classifier variational autoencoder (ACVAE). The research was conducted by Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and Nobukatsu Hojo from NTT Communication Science Laboratories, NTT Corporation in Japan. The paper aims to address the limitations of existing voice conversion methods that require accurately aligned parallel data of source and target speech, which can be costly and time-consuming to collect. The proposed method utilizes a variant of the conditional variational autoencoder (VAE) called an auxiliary classifier VAE (ACVAE) to overcome these challenges.\n",
      "\n",
      "The key features of the proposed method include the adoption of fully convolutional architectures for the encoder and decoder networks. This allows the networks to learn conversion rules that capture time dependencies in the acoustic feature sequences of source and target speech. Additionally, an information-theoretic regularization is used for model training to ensure that the information in the attribute class label is not lost in the conversion process. By introducing an auxiliary classifier, the encoder and decoder are trained to correctly predict the attribute classes of the decoder outputs. This helps in controlling the voice characteristics of the input speech.\n",
      "\n",
      "To avoid producing buzzy-sounding speech at test time, the method transplants the spectral details of the input speech into its converted version. Subjective evaluation experiments revealed that this approach worked reasonably well in a non-parallel many-to-many speaker identity conversion task. The research contributes to the field of voice conversion by offering a novel method that does not require parallel data and addresses the limitations of existing approaches. The key metrics defining the success of the work include sound quality and speaker similarity, which were evaluated through subjective listening tests.\n",
      "\n",
      "Future directions for this research include further refining the model architecture, exploring different types of auxiliary classifiers, and investigating the application of the method to other voice conversion tasks. The research opens up possibilities for improving voice conversion techniques and advancing the field of speech processing. Through the proposed ACVAE method, the researchers have made significant strides in non-parallel voice conversion and have laid the groundwork for future advancements in the field.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "key = get_env_variables()\n",
    "\n",
    "\n",
    "\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "llm = ChatOpenAI(api_key=key[\"OPENAI_API_KEY\"], temperature=TEMPERATURE, model_name=LLM_NAME)\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"\n",
    "You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "- Outline the key points of the paper\n",
    "- Edit the outline into a voiceover script for a 5 minute video\n",
    "- Clearly state why was the research done, what are the technologies that were previously known involved,\n",
    "how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "and what are future directions that lie ahead.\n",
    "- Do not write any fact which is not present in the paper\n",
    "- The final script should contain 4000 words\n",
    "\n",
    "- First, pretend you are a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving your dissertation defense.\n",
    "- Write a clearly organized and to-the-point outline summary of the following research:\n",
    "\"{text}\",\n",
    "- The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "- Introduce the research scientists involved and the institutions involved if known.\n",
    "- Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "- Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "- Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "- Use present tense and do not use past tense.\n",
    "- Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "- Make sure to answer clearly what is the major contribution of this body of work.\n",
    "- The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "and what are future directions that lie ahead.\n",
    "\n",
    "- After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially\n",
    "- regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "and what are future directions that lie ahead.\n",
    "- Assume the role of the editor of the best ranking tv production company in the world. \n",
    "- Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "- Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "\n",
    "- Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "and what are future directions that lie ahead.\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d55086-5193-4f31-a2ca-3727f28cf199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'audio_voiceover' has been created.\n",
      "Recording saved in audio_voiceover/output_20240310_10-59-36.mp3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"The folder '{folder_name}' has been created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{folder_name}' already exists.\")\n",
    "\n",
    "\n",
    "def text_to_speech(text_for_TTS):\n",
    "    ELEVEN_LABS_API_KEY = os.environ.get(\"ELEVEN_LABS_API_KEY\")\n",
    "\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/bVMeCyTHy58xNoL34h3p\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVEN_LABS_API_KEY\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"text\": text_for_TTS,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Generate a unique filename based on timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "        filename = f'output_{timestamp}.mp3'\n",
    "\n",
    "        # Save the recording to the unique file\n",
    "        with open(f\"{folder_name}/{filename}\", 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"Recording saved in {folder_name}/{filename}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Specify the folder name\n",
    "folder_name = \"audio_voiceover\"\n",
    "\n",
    "# Call the function to create the folder\n",
    "create_folder(folder_name)\n",
    "\n",
    "text_to_speech(stuff_chain.run(docs)[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d95a774-23e0-4083-a274-2914a07a19fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pages converted and saved in the folder: 1808.05092_pngs\n",
      ".ppm files deleted in the folder: 1808.05092_pngs\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdf_to_png(pdf_path):\n",
    "    # Create a folder for storing the PNGs\n",
    "    folder_name = os.path.splitext(os.path.basename(pdf_path))[0] + \"_pngs\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Convert each page of the PDF to PNG\n",
    "    images = convert_from_path(pdf_path, output_folder=folder_name)\n",
    "\n",
    "    # Save each image as a separate PNG file\n",
    "    for i, image in enumerate(images):\n",
    "        png_path = os.path.join(folder_name, f\"{folder_name}_page_{i + 1}.png\")\n",
    "        image.save(png_path, \"PNG\")\n",
    "\n",
    "    print(f\"All pages converted and saved in the folder: {folder_name}\")\n",
    "\n",
    "    # Clean up: Delete the .ppm files\n",
    "    for filename in os.listdir(folder_name):\n",
    "        if filename.endswith(\".ppm\"):\n",
    "            ppm_path = os.path.join(folder_name, filename)\n",
    "            os.remove(ppm_path)\n",
    "\n",
    "    print(f\".ppm files deleted in the folder: {folder_name}\")\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"1808.05092.pdf\"\n",
    "convert_pdf_to_png(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f77433c-a7dc-4bff-993c-811ec5f73eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The images provided are pages from a document or research paper with figures and diagrams embedded within them. Here's a summary of the file names and their locations:\\n\\n- Figure 1, at the top of the fourth page.\\n- Figure 2, at the top of the fifth page.\\n- Figure 3, middle of the sixth page.\\n- Figure 4, at the top of the seventh page.\", role='assistant', function_call=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def generate_image_responses(image_folder):\n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # List to store messages for the OpenAI API call\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"If the image has a diagram or visual, output the file name in a list format and whether it is at the top or bottom of the page?\",\n",
    "        },\n",
    "              ]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    # Iterate through the images in the folder\n",
    "    for image_filename in os.listdir(image_folder):\n",
    "        if image_filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            image_path = f\"{image_folder}/{image_filename}\"  # Replace your_base_url with the actual base URL\n",
    "            # Getting the base64 string\n",
    "            base64_image = encode_image(image_path)\n",
    "            images={\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        \"detail\": \"low\"},\n",
    "                }\n",
    "            # Append the images structure to the content list in the messages dictionary\n",
    "            messages[0][\"content\"].append(images)\n",
    "\n",
    "    # Make the OpenAI API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "\n",
    "    # Print the generated responses\n",
    "    for choice in response.choices:\n",
    "        print(choice)\n",
    "\n",
    "# Example usage:\n",
    "image_folder = \"1808.05092_pngs\"\n",
    "generate_image_responses(image_folder)\n",
    "\n",
    "# Output:\n",
    "# Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"\n",
    "# The images provided are pages from a document or research paper with\n",
    "# figures and diagrams embedded within them. Here's a summary of \n",
    "# the file names and their locations:\\n\\n- \n",
    "# Figure 1, at the top of the fourth page.\\n- \n",
    "# Figure 2, at the top of the fifth page.\\n- \n",
    "# Figure 3, middle of the sixth page.\\n- \n",
    "# Figure 4, at the top of the seventh page.\n",
    "# \", role='assistant', function_call=None, tool_calls=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47715132-6ce4-4781-8b4d-3934cd3d979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved: 1808.05092_pngs_page_7.png_cropped_1.png (top) and 1808.05092_pngs_page_7.png_cropped_2.png (bottom)\n",
      "Images saved: 1808.05092_pngs_page_6.png_cropped_1.png (top) and 1808.05092_pngs_page_6.png_cropped_2.png (bottom)\n",
      "Images saved: 1808.05092_pngs_page_4.png_cropped_1.png (top) and 1808.05092_pngs_page_4.png_cropped_2.png (bottom)\n",
      "Images saved: 1808.05092_pngs_page_5.png_cropped_1.png (top) and 1808.05092_pngs_page_5.png_cropped_2.png (bottom)\n",
      "Images saved: 1808.05092_pngs_page_1.png_cropped_1.png (top) and 1808.05092_pngs_page_1.png_cropped_2.png (bottom)\n",
      "Images saved: 1808.05092_pngs_page_2.png_cropped_1.png (top) and 1808.05092_pngs_page_2.png_cropped_2.png (bottom)\n",
      "Images saved: 1808.05092_pngs_page_3.png_cropped_1.png (top) and 1808.05092_pngs_page_3.png_cropped_2.png (bottom)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def cut_pngs_in_half(directory_path):\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for file_name in files:\n",
    "        # Check if the file is a PNG\n",
    "        if file_name.lower().endswith('.png'):\n",
    "            image_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Open the image\n",
    "            with Image.open(image_path) as img:\n",
    "                # Get the dimensions of the image\n",
    "                width, height = img.size\n",
    "\n",
    "                # Cut the image in half (top and bottom)\n",
    "                top_half = img.crop((0, 0, width, height // 2))\n",
    "                bottom_half = img.crop((0, height // 2, width, height))\n",
    "\n",
    "                # Save the top and bottom halves with \"_cropped_1\" and \"_cropped_2\" suffixes\n",
    "                top_half.save(os.path.join(directory_path, f\"{os.path.splitext(file_name)[0]}_cropped_1.png\"), 'PNG')\n",
    "                bottom_half.save(os.path.join(directory_path, f\"{os.path.splitext(file_name)[0]}_cropped_2.png\"), 'PNG')\n",
    "\n",
    "                print(f\"Images saved: {file_name}_cropped_1.png (top) and {file_name}_cropped_2.png (bottom)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input for the directory path\n",
    "    directory_path = \"1808.05092_pngs\"\n",
    "\n",
    "    # Call the function to cut PNGs in half\n",
    "    cut_pngs_in_half(directory_path)\n",
    "# output_20240310_030318.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc28b1b-90ac-4c5f-b920-1f210e153422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_videos/final_video_with_audio.mp4.\n",
      "MoviePy - Writing audio in final_video_with_audioTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_videos/final_video_with_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_videos/final_video_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from moviepy.editor import ImageSequenceClip, AudioFileClip, concatenate_videoclips\n",
    "from moviepy.config import change_settings\n",
    "from moviepy.editor import concatenate_videoclips\n",
    "# Set the FFMPEG_BINARY path\n",
    "change_settings({\"FFMPEG_BINARY\": \"/opt/homebrew/bin/ffmpeg\"})\n",
    "\n",
    "# Import the missing function\n",
    "from moviepy.editor import concatenate_videoclips\n",
    "\n",
    "def analyze_mp3_length(mp3_path):\n",
    "    audio = AudioSegment.from_file(mp3_path)\n",
    "    return len(audio) / 1000.0  # Length in seconds\n",
    "\n",
    "def fetch_cropped_images(image_directory):\n",
    "    cropped_images = [image for image in os.listdir(image_directory) if image.lower().endswith('.png') and 'cropped' in image.lower()]\n",
    "    sorted_images = sorted(cropped_images, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    return sorted_images\n",
    "\n",
    "def create_video_with_audio(mp3_path, image_directory, output_path):\n",
    "    # Sort the images in alphanumeric order\n",
    "    image_files = sorted(os.listdir(image_directory))\n",
    "    audio_clip = AudioFileClip(mp3_path)   \n",
    "    \n",
    "    # Calculate the duration of each image based on the total duration of the audio and number of images\n",
    "    image_duration = audio_clip.duration / len(image_files)\n",
    "    \n",
    "    clips = []\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        # Load each image and set its duration\n",
    "        image_path = os.path.join(image_directory, image_file)\n",
    "        image_clip = ImageSequenceClip([image_path], fps=24).set_duration(image_duration)\n",
    "        \n",
    "        # Add the image clip to the list of clips\n",
    "        clips.append(image_clip)\n",
    "    \n",
    "    # Concatenate the image clips to create the final video\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_clip = final_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video with audio\n",
    "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24, verbose=False)\n",
    "\n",
    "\n",
    "output_path = \"final_videos/final_video_with_audio.mp4\"  # Update with your desired output path\n",
    "# Call the function with the provided paths\n",
    "mp3_path = \"audio_voiceover/output_20240310_10-59-36.mp3\"\n",
    "image_directory = \"1808.05092_pngs\"\n",
    "create_video_with_audio(mp3_path, image_directory, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff01ffe-d0fa-47db-97bd-ca083452a05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
