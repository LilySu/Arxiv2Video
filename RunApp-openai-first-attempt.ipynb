{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e445fe3-e5e0-4080-9a78-6570bfae7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip, AudioFileClip, concatenate_videoclips\n",
    "from pymilvus import Milvus, MilvusClient, IndexType, connections, utility\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from pdf2image.exceptions import PDFPageCountError, PDFSyntaxError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from moviepy.editor import concatenate_videoclips, ImageClip\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from moviepy.config import change_settings\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.vectorstores import Milvus\n",
    "from pdf2image import convert_from_path\n",
    "from milvus import default_server\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import feedparser\n",
    "import requests\n",
    "import imageio\n",
    "import base64\n",
    "import pprint\n",
    "import torch\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b35aae-ddeb-4c2b-acbe-4529e441a6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:19531\n"
     ]
    }
   ],
   "source": [
    "change_settings({\"FFMPEG_BINARY\": \"/opt/homebrew/bin/ffmpeg\", \"DYLD_LIBRARY_PATH\":\"/opt/homebrew/bin/convert\"})\n",
    "# Set up a Milvus client\n",
    "default_server.start()\n",
    "host=\"127.0.0.1\"\n",
    "connections.connect(host=host, port=default_server.listen_port)\n",
    "port=default_server.listen_port\n",
    "my_uri = \"http://localhost:\" + str(port)\n",
    "print(my_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac66928-549a-4da3-adb7-1faddc4a9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_uri=\"http://localhost:19531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8361215-27c0-4617-bba1-d4e41bcd7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_variables():\n",
    "    \"\"\"Fetch all necessary configurations from environment variables.\"\"\"\n",
    "    return {\n",
    "        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),\n",
    "        'ELEVEN_LABS_API_KEY': os.getenv('ELEVEN_LABS_API_KEY')\n",
    "    }\n",
    "\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"The folder '{folder_name}' has been created.\")\n",
    "    else:\n",
    "        print(f\"The folder '{folder_name}' already exists.\")\n",
    "\n",
    "\n",
    "def arxiv_id_from_url(url):\n",
    "    # Extract the arXiv ID from the URL using a regular expression\n",
    "    match = re.search(r'arxiv\\.org/pdf/(\\d+\\.\\d+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def download_and_save_pdf(url, folder_pdfs):\n",
    "    \"\"\"\n",
    "    Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The arXiv.org URL of the paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: ArXiv ID of the downloaded paper if successful, or an error message.\n",
    "    \"\"\"\n",
    "    # Extract arXiv ID from the URL\n",
    "    arxiv_id = arxiv_id_from_url(url)\n",
    "\n",
    "    arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "    pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "    create_folder(pdf_path)\n",
    "\n",
    "    # Check if a valid arXiv ID was extracted\n",
    "    if arxiv_id:\n",
    "        try:\n",
    "            # Make a request to the arXiv API\n",
    "            feed = feedparser.parse(f'http://export.arxiv.org/api/query?id_list={arxiv_id}')\n",
    "\n",
    "            # Check if the response contains entries\n",
    "            if 'entries' in feed:\n",
    "                # Iterate over each entry (paper) in the feed\n",
    "                for entry in feed.entries:\n",
    "                    # Extract the PDF link from the entry\n",
    "                    pdf_link = entry.link.replace('/abs/', '/pdf/') + '.pdf'\n",
    "\n",
    "                    # Download the PDF\n",
    "                    response = requests.get(pdf_link)\n",
    "\n",
    "                    # Save the PDF in the local directory with the name based on the arXiv ID\n",
    "                    with open(f'{pdf_path}/{arxiv_name}.pdf', 'wb') as pdf_file:\n",
    "                        pdf_file.write(response.content)\n",
    "\n",
    "                    print(f\"\\nPDF downloaded and saved as {arxiv_name}.pdf\")\n",
    "                    return arxiv_id\n",
    "\n",
    "            else:\n",
    "                return f\"\\nNo entries found for arXiv ID {arxiv_id}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"\\nError extracting information: {e}\"\n",
    "    else:\n",
    "        return \"Invalid arXiv PDF URL format. Please enter a valid URL.\"\n",
    "\n",
    "\n",
    "def download_and_initialize_embedding_model(model_name=\"WhereIsAI/UAE-Large-V1\", device=None):\n",
    "    \"\"\"\n",
    "    Download and initialize the Sentence Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the Sentence Transformer model to download.\n",
    "    - device (str or torch.device): The device to use for the model (e.g., 'cuda:3' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - encoder (SentenceTransformer): The initialized Sentence Transformer model.\n",
    "    - EMBEDDING_DIM (int): The embedding dimension of the model.\n",
    "    - MAX_SEQ_LENGTH (int): The maximum sequence length.\n",
    "\n",
    "    Example usage:\n",
    "    encoder, EMBEDDING_DIM, max_seq_length = download_and_initialize_embedding_model()\n",
    "    \"\"\"\n",
    "    # Initialize torch settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    DEVICE = torch.device(device) if device else torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\ndevice: {DEVICE}\")\n",
    "\n",
    "    # Load the model from the Hugging Face model hub\n",
    "    encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "    print(f\"\\nDatatype of SentenceTransformer encoded object{type(encoder)}\\n\")\n",
    "    print(f\"\\nWhat the encoder object looks like: {encoder}\\n\")\n",
    "\n",
    "    # Get the model parameters and save for later\n",
    "    EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "    MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length()\n",
    "    # Assume tokens are 3 characters long\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "    HF_EOS_TOKEN_LENGTH = 1 * 3\n",
    "    # Test with 512 sequence length\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "    HF_EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "    # Inspect model parameters\n",
    "    print(f\"\\nmodel_name: {model_name}\")\n",
    "    print(f\"\\nEMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "    print(f\"\\nMAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "    return encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH\n",
    "\n",
    "\n",
    "def create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M=16, uri=my_uri):\n",
    "    \"\"\"\n",
    "    Create a no-schema Milvus collection and define the database index.\n",
    "\n",
    "    Parameters:\n",
    "    - uri (str): The URI of the Milvus server.\n",
    "    - COLLECTION_NAME (str): The name of the Milvus collection.\n",
    "    - EMBEDDING_DIM (int): The dimension of the embedding vectors.\n",
    "    - M (int): The maximum number of graph connections per layer for the HNSW index. Default is 16.\n",
    "\n",
    "    Returns:\n",
    "    - milvus_client (Milvus): The Milvus client instance.\n",
    "\n",
    "\n",
    "    Example usage:\n",
    "    my_uri = \"tcp://127.0.0.1:19530\"\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    my_EMBEDDING_DIM = 1024\n",
    "    \n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M=16, uri=my_uri)\n",
    "    \"\"\"\n",
    "    # Add custom HNSW search index to the collection.\n",
    "    # M = max number graph connections per layer. Large M = denser graph.\n",
    "    # Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "    # M = 16\n",
    "    # efConstruction = num_candidate_nearest_neighbors per layer. \n",
    "    # Use Rule of thumb: int. 8~512, efConstruction = M * 2.\n",
    "    efConstruction = M * 2\n",
    "    index_params = {\n",
    "        \"index_type\": IndexType.HNSW,\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {\"M\": M, \"efConstruction\": efConstruction}\n",
    "    }\n",
    "\n",
    "    # Use no-schema Milvus client using flexible json key:value format.\n",
    "    milvus_client = MilvusClient(uri=my_uri)\n",
    "\n",
    "    # Check if collection already exists, if so drop it.\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "        print(f\"\\nCollection had previously been created, dropping previous collection to initialize anew: `{COLLECTION_NAME}`\")\n",
    "\n",
    "    # Create the collection.\n",
    "    milvus_client.create_collection(COLLECTION_NAME, EMBEDDING_DIM,\n",
    "                                    consistency_level=\"Eventually\",\n",
    "                                    auto_id=True,\n",
    "                                    overwrite=True,\n",
    "                                    params=index_params)\n",
    "\n",
    "    print(f\"\\nSuccessfully created collection: `{COLLECTION_NAME}`\")\n",
    "    print(milvus_client.describe_collection(COLLECTION_NAME))\n",
    "\n",
    "    return milvus_client\n",
    "\n",
    "\n",
    "def split_documents_to_chunks(docs, max_seq_length, hf_eos_token_length):\n",
    "    \"\"\"\n",
    "    Split documents into smaller recursive chunks using Sentence Transformers' RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be split.\n",
    "    - max_seq_length (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list): List of chunks.\n",
    "\n",
    "    Example usage:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    docs = [\"Document 1 text.\", \"Document 2 text.\", \"Document 3 text.\"]\n",
    "    \n",
    "    resulting_chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    print(resulting_chunks)\n",
    "    \"\"\"\n",
    "    # Calculate chunk size and overlap\n",
    "    chunk_size = max_seq_length - hf_eos_token_length\n",
    "    chunk_overlap = int(round(chunk_size * 0.15, 0))\n",
    "\n",
    "    # Create an instance of the RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Split the documents further into smaller, recursive chunks.\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, max_seq_length, hf_eos_token_length):\n",
    "    \"\"\"\n",
    "    Insert document chunks into a Milvus collection.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): List of documents to be inserted.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - encoder (SentenceTransformer): SentenceTransformer model for generating embeddings.\n",
    "    - milvus_client (Milvus): Milvus client instance.\n",
    "    - max_seq_length (int): Maximum sequence length.\n",
    "    - hf_eos_token_length (int): Length of the EOS token.\n",
    "\n",
    "    Returns:\n",
    "    - insert_time (float): Time taken for the insertion process.\n",
    "\n",
    "    Example Usage assuming 'chunks' is a list of dictionaries with 'page_content' and 'metadata' keys:\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    ENCODER_MODEL_NAME = \"WhereIsAI/UAE-Large-V1\"\n",
    "    # Initialize Milvus client\n",
    "    # Initialize encoder model\n",
    "    \n",
    "    resulting_insert_time = insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \"\"\"\n",
    "    # Convert chunks to a list of dictionaries.\n",
    "    chunk_list = []\n",
    "    for chunk in chunks:\n",
    "    \n",
    "        # Generate embeddings using encoder from HuggingFace.\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", chunk_size=max_seq_length)\n",
    "        \n",
    "        # Assemble embedding vector, original text chunk, metadata.\n",
    "        # chunk_dict = {\n",
    "        #     'vector': converted_values,\n",
    "        #     'chunk': chunk.page_content,\n",
    "        #     'source': chunk.metadata['page']\n",
    "        # }\n",
    "        chunk_list = embeddings.embed_query(chunk)\n",
    "\n",
    "    # Insert data into the Milvus collection.\n",
    "    print(\"Start inserting entities\")\n",
    "\n",
    "    inserted_chunks = milvus_client.insert(\n",
    "        COLLECTION_NAME,\n",
    "        data=chunk_list,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    print(\"Finished inserting entities\")\n",
    "\n",
    "    # After the final entity is inserted, call flush to stop growing segments left in memory.\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "    return print(f\"\\nNumber of chunks inserted into Milvus database: {len(inserted_chunks)} with chunk id starting at number: {inserted_chunks[0]}\\n\")\n",
    "\n",
    "\n",
    "def client_assemble_retrieved_context(retrieved_top_k, metadata_fields=[], num_shot_answers=3):\n",
    "    \"\"\" \n",
    "    For each question, assemble the context and metadata from the retrieved_top_k chunks.\n",
    "    retrieved_top_k: list of dicts\n",
    "\n",
    "    Example Usage:\n",
    "    formatted_results, context, context_metadata = client_assemble_retrieved_context(results, metadata_fields=metadata_fields, num_shot_answers=top_k)\n",
    "    \"\"\"\n",
    "    # Assemble the context as a stuffed string.\n",
    "    distances = []\n",
    "    context = []\n",
    "    context_metadata = []\n",
    "    i = 1\n",
    "    for r in retrieved_top_k[0]:\n",
    "        distances.append(r['distance'])\n",
    "        if i <= num_shot_answers:\n",
    "            if len(metadata_fields) > 0:\n",
    "                metadata = {}\n",
    "                for field in metadata_fields:\n",
    "                    metadata[field] = r['entity'][field]\n",
    "                context_metadata.append(metadata)\n",
    "            context.append(r['entity']['chunk'])\n",
    "        i += 1\n",
    "\n",
    "    # Assemble formatted results in a zipped list.\n",
    "    formatted_results = list(zip(distances, context, context_metadata))\n",
    "    # Return all the things for convenience.\n",
    "    return formatted_results, context, context_metadata\n",
    "    \n",
    "\n",
    "def search_and_generate_response(milvus_client, encoder, COLLECTION_NAME, llm_name, temperature, random_seed, top_k=3, M=16):\n",
    "    \"\"\"\n",
    "    Search Milvus collection for relevant context and generate a response using the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - openai_client (OpenAI): OpenAI client instance.\n",
    "    - milvus_client (Milvus): Milvus client instance.\n",
    "    - encoder (SentenceTransformer): SentenceTransformer model for generating embeddings.\n",
    "    - COLLECTION_NAME (str): Name of the Milvus collection.\n",
    "    - SAMPLE_QUESTION (str): Sample question for search.\n",
    "    - llm_name (str): Name of the OpenAI language model.\n",
    "    - temperature (float): Temperature for response generation.\n",
    "    - random_seed (int): Random seed for response generation.\n",
    "    - top_k (int): Top K results to retrieve from Milvus search.\n",
    "    - M (Milvus): Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "\n",
    "    Returns:\n",
    "    - response_choices (list): List of response choices.\n",
    "\n",
    "    Example usage:\n",
    "    \n",
    "    response_choices = search_and_generate_response(\n",
    "        milvus_client,\n",
    "        encoder,\n",
    "        COLLECTION_NAME,\n",
    "        LLM_NAME,\n",
    "        TEMPERATURE,\n",
    "        RANDOM_SEED\n",
    "    )\n",
    "    \"\"\"\n",
    "    efConstruction = M * 2\n",
    "    \n",
    "    # Return top k results with HNSW index.\n",
    "    search_params = {\"ef\": efConstruction}\n",
    "\n",
    "    # Define output fields to return.\n",
    "    output_fields = [\"source\", \"chunk\"]\n",
    "\n",
    "    SAMPLE_QUESTION = \"What are the key contributions of this paper and the evaluation metrics that prove that this paper advances previously known information?\"\n",
    "    \n",
    "    # Search Milvus collection\n",
    "    results = milvus_client.search(\n",
    "        COLLECTION_NAME,\n",
    "        data=encoder.encode([SAMPLE_QUESTION]),\n",
    "        search_params=search_params,\n",
    "        output_fields=output_fields,\n",
    "        limit=top_k,\n",
    "        consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "    # Assemble retrieved context\n",
    "    metadata_fields = [f for f in output_fields if f != 'chunk']\n",
    "    formatted_results, context, context_metadata = client_assemble_retrieved_context(results, metadata_fields=metadata_fields, num_shot_answers=top_k)\n",
    "    \n",
    "    SYSTEM_PROMPT = f\"\"\"Answer in no less than 4000 characters. \n",
    "    - It is of utmost importance to use only the information from the Context to answer the user's question. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Create an outline the key points of the paper\n",
    "    - Clearly state in your outline why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Always start by stating the title of the paper as the first few words.\n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources.\n",
    "    Context: {context}\n",
    "    Answer: The answer to the question in no less than 4000 characters in complete sentences as a narration. Do not pretend to be the author, just an instructor.\n",
    "    Grounding sources: {context_metadata[1]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Also create a template for function calls later.\n",
    "    SYSTEM_PROMPT_TEMPLATE = f\"\"\"Answer in no less than 4000 characters. \n",
    "    - It is of utmost importance to use only the information from the Context to answer the user's question. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Create an outline the key points of the paper\n",
    "    - Clearly state in your outline why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Always start by stating the title of the paper as the first few words.\n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources.\n",
    "    Context: {context}\n",
    "    Answer: The answer to the question in no less than 4000 characters in complete sentences as a narration. Do not pretend to be the author, just an instructor.\n",
    "    Grounding sources: {context_metadata[1]}\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    load_dotenv()\n",
    "    key = get_env_variables()\n",
    "    openai_client = OpenAI(api_key=key[\"OPENAI_API_KEY\"])\n",
    "    \n",
    "    # Generate response using the OpenAI API\n",
    "    response = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "            {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "        ],\n",
    "        model=llm_name,\n",
    "        temperature=temperature,\n",
    "        seed=random_seed,\n",
    "    )\n",
    "\n",
    "    # Extract and print the contents of the number one ranked response:\n",
    "    response_choices = [choice.message.content for choice in response.choices]\n",
    "    for i, choice in enumerate(response_choices, 1):\n",
    "        pprint.pprint(f\"\\nAnswer {i}: {choice}\\n\")\n",
    "\n",
    "    return response_choices\n",
    "\n",
    "\n",
    "def save_transcript(response_choices, folder_transcripts, arxiv_name):\n",
    "    \"\"\"\n",
    "    Save the first element of response_choices into a text file in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - response_choices (list): A list of choices where the first element is the transcript content.\n",
    "    - folder_transcripts (str): The directory path where the transcript file will be saved.\n",
    "    - arxiv_name (str): The name used for generating the transcript file.\n",
    "\n",
    "    Returns:\n",
    "    None: The function saves the transcript content to a text file.\n",
    "\n",
    "    Example:\n",
    "    save_transcript([\"This is the transcript content.\"], \"transcripts\", \"example_arxiv\")\n",
    "    \"\"\"\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(folder_transcripts):\n",
    "        os.makedirs(folder_transcripts)\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(folder_transcripts, f\"{arxiv_name}.txt\")\n",
    "\n",
    "    # Save response_choices[0] to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(response_choices[0])\n",
    "\n",
    "    print(f\"\\nTranscript saved in: {file_path}\")\n",
    "\n",
    "\n",
    "def text_to_speech(text_for_TTS, arxiv_name, folder_audio):\n",
    "\n",
    "    ELEVEN_LABS_API_KEY = os.environ.get(\"ELEVEN_LABS_API_KEY\")\n",
    "\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/bVMeCyTHy58xNoL34h3p\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVEN_LABS_API_KEY\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"text\": text_for_TTS,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Generate a unique filename based on timestamp\n",
    "    filename = f\"output_{arxiv_name}.mp3\"\n",
    "    target_path = os.path.join(folder_audio, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"Recording file {filename} already exists in {folder_audio}. Skipping download.\")\n",
    "        return target_path\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Save the recording to the unique file\n",
    "        with open(target_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"\\nRecording saved in {target_path}\")\n",
    "        return target_path\n",
    "    else:\n",
    "        print(f\"\\n Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "def convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name):\n",
    "    try:\n",
    "        # Create a folder for storing the PNGs\n",
    "        sub_folder_name = os.path.splitext(os.path.basename(pdf_file_path))[0] + \"_pngs\"\n",
    "        full_path = os.path.join(folder_images, sub_folder_name)\n",
    "        if not os.path.exists(full_path):\n",
    "            os.makedirs(full_path)\n",
    "    \n",
    "        # Convert each page of the PDF to PNG\n",
    "        images = convert_from_path(pdf_file_path, output_folder=full_path)\n",
    "        # arxiv_name = sub_folder_name.replace(\"_pngs\", \"\")\n",
    "    \n",
    "        # Save each image as a separate PNG file\n",
    "        for i, image in enumerate(images):\n",
    "            png_path = os.path.join(full_path, f\"{arxiv_name}_page_{i + 1}.png\")\n",
    "            image.save(png_path, \"PNG\")\n",
    "    \n",
    "        print(f\"\\nAll pages converted and saved in the folder: {full_path}\")\n",
    "    \n",
    "        # Clean up: Delete the .ppm files and uncropped files\n",
    "        for filename in os.listdir(full_path):\n",
    "            if filename.endswith(\".ppm\"):\n",
    "                file_to_remove_path = os.path.join(full_path, filename)\n",
    "                os.remove(file_to_remove_path)\n",
    "    \n",
    "        print(f\"\\n.ppm artifacts deleted in the folder: {full_path}\")\n",
    "    except (PDFPageCountError, PDFSyntaxError, PermissionError) as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(f\"Skipping processing of {pdf_file_path}\")\n",
    "        if isinstance(e, PdfReadError):\n",
    "            print(\"PdfReadError: Unable to read PDF file.\")\n",
    "        elif isinstance(e, PermissionError):\n",
    "            print(\"PermissionError: Permission issue while processing the PDF file.\")\n",
    "\n",
    "\n",
    "def cut_pngs_in_half(image_folder):\n",
    "    # Ensure the directory path is valid\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\"\\nError: Directory '{image_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for file_name in files:\n",
    "        # Check if the file is a PNG and does not contain 'cropped' in the name\n",
    "        if file_name.lower().endswith('.png') and 'cropped' not in file_name.lower():\n",
    "            image_path = os.path.join(image_folder, file_name)\n",
    "\n",
    "            # Open the image\n",
    "            with Image.open(image_path) as img:\n",
    "                # Get the dimensions of the image\n",
    "                width, height = img.size\n",
    "\n",
    "                # Cut the image in half (top and bottom)\n",
    "                top_half = img.crop((0, 0, width, height // 2))\n",
    "                bottom_half = img.crop((0, height // 2, width, height))\n",
    "\n",
    "                # Save the top and bottom halves with \"_cropped_1\" and \"_cropped_2\" suffixes\n",
    "                top_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_1.png\"), 'PNG')\n",
    "                bottom_half.save(os.path.join(image_folder, f\"{os.path.splitext(file_name)[0]}_cropped_2.png\"), 'PNG')\n",
    "\n",
    "                print(f\"\\nImages saved: {file_name}_cropped_1.png (top) and {file_name}_cropped_2.png (bottom)\")\n",
    "        else:\n",
    "            print(f\"\\nSkipping processing for {file_name} as it contains 'cropped' in the file name.\")\n",
    "\n",
    "\n",
    "def analyze_mp3_length(mp3_path):\n",
    "    audio = AudioSegment.from_file(mp3_path)\n",
    "    return len(audio) / 1000.0  # Length in seconds\n",
    "\n",
    "def fetch_cropped_images(image_folder):\n",
    "    # List all images in the folder\n",
    "    all_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Identify files to keep (those with the word \"cropped\" in their filenames)\n",
    "    cropped_images = [image for image in all_images if image.lower().endswith('.png') and 'cropped' in image.lower()]\n",
    "    \n",
    "    # Delete files that do not contain the word \"cropped\"\n",
    "    for image in all_images:\n",
    "        if image not in cropped_images:\n",
    "            os.remove(os.path.join(image_folder, image))\n",
    "    \n",
    "    # List the remaining images after deletion\n",
    "    remaining_images = os.listdir(image_folder)\n",
    "    \n",
    "    # Sort the cropped images based on numeric values in their filenames\n",
    "    sorted_images = sorted(remaining_images, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    return sorted_images\n",
    "\n",
    "\n",
    "def move_uncropped_files(image_folder):\n",
    "    try:\n",
    "        # Create a new folder if it doesn't exist\n",
    "        uncropped_folder = os.path.join(image_folder, \"uncropped_pngs\")\n",
    "        if not os.path.exists(uncropped_folder):\n",
    "            os.makedirs(uncropped_folder)\n",
    "\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            file_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            # Check if the file name contains the word \"cropped\"\n",
    "            if \"cropped\" not in filename:\n",
    "                # Move the file to the uncropped folder\n",
    "                new_path = os.path.join(uncropped_folder, filename)\n",
    "\n",
    "                try:\n",
    "                    shutil.move(file_path, new_path)\n",
    "                    print(f\"File moved to uncropped folder: {filename}\")\n",
    "                except Exception as move_error:\n",
    "                    print(f\"Error moving file {filename}: {move_error}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"All non-cropped files moved to the folder: {uncropped_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def create_video(mp3_path, image_folder, output_path):\n",
    "    try:\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            file_path = os.path.join(image_folder, filename)\n",
    "            \n",
    "            # Check if the file name contains the word \"cropped\"\n",
    "            if \"cropped\" not in filename:\n",
    "                # Remove the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"File removed: {filename}\")\n",
    "                \n",
    "        print(f\"All non-cropped files removed in the folder: {image_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Sort the images by converting the numeric parts of filenames into integers\n",
    "    image_files = sorted([file for file in os.listdir(image_folder) if 'cropped' in file and file.lower().endswith('.png')],\n",
    "                         key=lambda x: [int(part) if part.isdigit() else part for part in re.split(r'(\\d+)', x)])\n",
    "    audio_clip = AudioFileClip(mp3_path)\n",
    "    \n",
    "    # Calculate the duration of each image based on the total duration of the audio and the number of images\n",
    "    image_duration = audio_clip.duration / len(image_files)\n",
    "    \n",
    "    clips = []\n",
    "    \n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        # Load each image using imageio\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = imageio.imread(image_path)\n",
    "    \n",
    "        if image.sum() == 0:\n",
    "            print(f\"Image {image_file} is blank. Skipping...\")\n",
    "            os.remove(image_file)\n",
    "            continue\n",
    "                \n",
    "        # Create a clip from the image and set its duration\n",
    "        image_clip = ImageClip(image).set_duration(image_duration)\n",
    "    \n",
    "        # Add the image clip to the list of clips\n",
    "        clips.append(image_clip)\n",
    "    \n",
    "    # Concatenate the image clips to create the final video\n",
    "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_clip = final_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video with audio\n",
    "    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24, verbose=True)\n",
    "    print(f\"\\nFinal video saved at: {output_path}.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4805c95b-6633-4b98-8e69-0e8483ef64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs/2402_13254' already exists.\n",
      "\n",
      "PDF downloaded and saved as 2402_13254.pdf\n",
      "chunks: \n",
      "\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='CounterCurate: Enhancing Physical and Semantic \"\n",
      " 'Visio-Linguistic\\\\nCompositional Reasoning via Counterfactual '\n",
      " 'Examples\\\\nJianrui Zhang*1Mu Cai∗1Tengyang Xie1,2Yong Jae '\n",
      " 'Lee1\\\\njzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu\\\\n1University '\n",
      " 'of Wisconsin–Madison2Microsoft '\n",
      " 'Research\\\\nhttps://countercurate.github.io\\\\nAbstract\\\\nWe propose '\n",
      " 'CounterCurate, a framework to\\\\ncomprehensively improve the '\n",
      " 'visio-linguistic\\\\ncompositional reasoning capability for both\\\\ncontrastive '\n",
      " 'and generative multimodal models.\\\\nIn particular, we identify two critical '\n",
      " 'under-\\\\nexplored problems: the neglect of physically\\\\ngrounded reasoning '\n",
      " '(counting and position un-\\\\nderstanding) and the potential of using '\n",
      " 'highly\\\\ncapable text and image generation models for\\\\nsemantic '\n",
      " 'counterfactual fine-tuning. Our work\\\\npioneers an approach in addressing '\n",
      " 'these gaps.\\\\nWe first spotlight the near-chance performance\\\\nof multimodal '\n",
      " 'models like CLIP and LLaV A in\\\\nphysically grounded compositional '\n",
      " \"reasoning.\\\\nWe then apply simple data augmentation using' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='of multimodal models like CLIP and LLaV A in\\\\nphysically \"\n",
      " 'grounded compositional reasoning.\\\\nWe then apply simple data augmentation '\n",
      " 'using\\\\ngrounded image generation model GLIGEN to\\\\ngenerate fine-tuning '\n",
      " 'data, resulting in signif-\\\\nicant performance improvements: +33% and\\\\n+37% '\n",
      " 'for CLIP and LLaV A, respectively, on\\\\nour newly curated '\n",
      " 'Flickr30k-Positions bench-\\\\nmark. Moreover, we exploit the '\n",
      " 'capabilities\\\\nof high-performing text generation and image\\\\ngeneration '\n",
      " 'models, specifically GPT-4V and\\\\nDALLE-3, to curate challenging semantic '\n",
      " 'coun-\\\\nterfactuals, thereby further enhancing compo-\\\\nsitional reasoning '\n",
      " 'capabilities on benchmarks\\\\nsuch as SugarCrepe, where CounterCurate '\n",
      " 'out-\\\\nperforms GPT-4V .\\\\nTo facilitate future research, we will release '\n",
      " 'our\\\\ncode, dataset, benchmark, and checkpoints '\n",
      " 'at\\\\nhttps://countercurate.github.io/ .\\\\n1 Introduction\\\\nLarge language '\n",
      " 'models such as ChatGPT (OpenAI,\\\\n2023a), GPT4 (OpenAI, 2023b), and LLaMA '\n",
      " \"(Tou-\\\\nvron et al., 2023) have demonstrated remarkable' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 0}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Large language models such as ChatGPT (OpenAI,\\\\n2023a), GPT4 \"\n",
      " '(OpenAI, 2023b), and LLaMA (Tou-\\\\nvron et al., 2023) have demonstrated '\n",
      " 'remarkable\\\\nknowledge and reasoning abilities. Recently, large\\\\nmultimodal '\n",
      " 'models (LMMs) (Radford et al., 2021;\\\\nOpenAI, 2023c) further leverage '\n",
      " 'large-scale image-\\\\ntext pairs for improving visio-linguistic '\n",
      " 'understand-\\\\ning capabilities. However, these models exhibit\\\\n*Equal '\n",
      " 'Contribution.\\\\n:Caption 1 is correct because the image shows a person on '\n",
      " 'the left side wearing a red dress that ends above the knee, and a person on '\n",
      " 'the right side wearing a white wedding dress with a long train.\\\\n:Which '\n",
      " 'caption correspond to this image?0: the dress on the left is long and the '\n",
      " 'dress on the right is short1: the dress on the left is short and the dress '\n",
      " 'on the right is long\\\\n:The image shows an ice cream cone with a blue scoop '\n",
      " 'of ice cream on top and what appears to be a pink scoop on the bottom. '\n",
      " \"Therefore, Caption 1 is correct.' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 0}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content=':Which caption correspond to this image?0: a pink scoop on \"\n",
      " 'top and a blue scoop on bottom 1: a pink scoop on bottom and a blue scoop on '\n",
      " 'top\\\\nFigure 1: Representative examples of GPT-4V failure\\\\ncases. In both '\n",
      " 'questions, GPT-4V correctly identifies all\\\\nobjects in question, but '\n",
      " 'chooses the wrong answer be-\\\\ncause it fails to distinguish between either '\n",
      " 'left and right\\\\n(the left question) or up and down (the right '\n",
      " 'question).\\\\nsubpar performance in compositional reasoning\\\\n(Diwan et al., '\n",
      " '2022; Hsieh et al., 2023), such as\\\\ndifferentiating between semantic '\n",
      " 'distractors like\\\\n“white shirts and black pants ” and “ black shirts '\n",
      " 'and\\\\nwhite pants ”. Current research either concentrates\\\\non curating '\n",
      " 'evaluation benchmarks (Diwan et al.,\\\\n2022; Hsieh et al., 2023; Le et al., '\n",
      " '2023) or enhanc-\\\\ning compositional reasoning abilities by '\n",
      " 'creating\\\\nrule-based counterfactual fine-tuning data (Yuksek-\\\\ngonul et '\n",
      " 'al., 2023; Le et al., 2023).\\\\nAt present, the majority of research focuses '\n",
      " \"on' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='rule-based counterfactual fine-tuning data (Yuksek-\\\\ngonul \"\n",
      " 'et al., 2023; Le et al., 2023).\\\\nAt present, the majority of research '\n",
      " 'focuses on\\\\nsemantic compositional reasoning, leaving another\\\\nfundamental '\n",
      " 'problem, physically grounded compo-\\\\nsitional reasoning, under-explored. '\n",
      " 'This includes\\\\ntasks such as counting and distinguishing left/right\\\\nand '\n",
      " 'up/down positions between objects. For exam-\\\\nple, in Figure 1, GPT-4V '\n",
      " \"(OpenAI, 2023c) notably\\\\n1arXiv:2402.13254v2  [cs.CV]  12 Mar 2024' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 0}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='made the wrong choice even though it identified all\\\\nobjects \"\n",
      " 'in question, demonstrating capable seman-\\\\ntical yet weak physical '\n",
      " 'compositional reasoning.\\\\nWith such an observation, it is not surprising '\n",
      " 'to\\\\nfind that both contrastive models like CLIP (Rad-\\\\nford et al., 2021) '\n",
      " 'and generative models like\\\\nLLaV A (Liu et al., 2023b) deliver near '\n",
      " 'random\\\\nchance performance on our newly curated bench-\\\\nmarks. We '\n",
      " 'hypothesize that modern LMMs are\\\\nlargely oblivious to positional '\n",
      " 'differences. To ad-\\\\ndress this, we generate counterfactual '\n",
      " 'image-text\\\\npairs using both simple methods, such as horizontal\\\\nflipping '\n",
      " 'for left/right distinctions, and leveraging\\\\na grounded image generation '\n",
      " 'model, GLIGEN (Li\\\\net al., 2023), to accurately replace/remove objects\\\\nof '\n",
      " 'interest for up/down and counting tasks. Our\\\\napproach shows significant '\n",
      " 'improvements such\\\\nas 33% for CLIP and 37% for LLaV A on '\n",
      " 'our\\\\nFlickr30k-Positions benchmark.\\\\nExisting methods also do not fully '\n",
      " \"utilize the' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '1}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='as 33% for CLIP and 37% for LLaV A on \"\n",
      " 'our\\\\nFlickr30k-Positions benchmark.\\\\nExisting methods also do not fully '\n",
      " 'utilize the\\\\ncapabilities of powerful generative models when\\\\ncreating '\n",
      " 'semantic counterfactual fine-tuning data.\\\\nWe argue that the key to '\n",
      " 'enhancing compositional\\\\nreasoning capabilities lies in the careful '\n",
      " 'curation\\\\nof accurate and sufficiently challenging image and\\\\ntext '\n",
      " 'counterfactuals. For example, augmented neg-\\\\native captions with '\n",
      " 'linguistic rules used by recent\\\\nfine-tuning approaches (e.g., Yuksekgonul '\n",
      " 'et al.,\\\\n2023; Le et al., 2023) could unintentionally follow\\\\nunnatural '\n",
      " 'language distributions, which are easily\\\\ndistinguishable by a text-only '\n",
      " 'model without any\\\\nimage evidence (Lin et al., 2023).\\\\nTo overcome this, '\n",
      " 'we utilize high-performance\\\\ntext generation model GPT-4V (OpenAI, '\n",
      " '2023c)\\\\nand image generation model DALLE-3 (Betker\\\\net al., 2023) to '\n",
      " 'curate reasonably and sufficiently\\\\ndifficult negatives. Our method '\n",
      " \"empirically demon-\\\\nstrates a significant performance boost by fine-' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 1}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='difficult negatives. Our method empirically demon-\\\\nstrates \"\n",
      " 'a significant performance boost by fine-\\\\ntuning CLIP and LLaV A using our '\n",
      " 'data generation\\\\npipeline on benchmarks such as SugarCrepe, where\\\\nwe '\n",
      " 'surpass NegCLIP and GPT-4V .\\\\nOur contributions are summarized as '\n",
      " 'follows:\\\\n•We systematically study physically grounded\\\\ncompositional '\n",
      " 'reasoning, including positional\\\\nunderstanding and counting, and highlight '\n",
      " 'the\\\\nnear random performance of multimodal mod-\\\\nels, including CLIP and '\n",
      " 'LLaV A, on our newly\\\\ncurated benchmarks.\\\\n•We significantly improve '\n",
      " 'physical reasoning\\\\ncapabilities by utilizing simple data augmen-\\\\ntation '\n",
      " 'techniques and a grounded image gen-eration model, GLIGEN, to generate '\n",
      " 'counter-\\\\nfactual images and captions.\\\\n•We employ the most capable image '\n",
      " 'and text\\\\ngeneration models to generate semantically\\\\ncounterfactual '\n",
      " 'image/text pairs, further en-\\\\nhancing performance over SOTA methods.\\\\n2 '\n",
      " 'Related Works\\\\n2.1 Large Multimodal Models\\\\nMultimodal models aim at '\n",
      " \"connecting multiple data' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 1}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='hancing performance over SOTA methods.\\\\n2 Related \"\n",
      " 'Works\\\\n2.1 Large Multimodal Models\\\\nMultimodal models aim at connecting '\n",
      " 'multiple data\\\\nmodalities. For visio-linguistic tasks, multimodal\\\\nmodels '\n",
      " 'are constructed either as contrastive (Rad-\\\\nford et al., 2021; Jia et al., '\n",
      " '2021) or generative (Ope-\\\\nnAI, 2023b; Alayrac et al., 2022; Liu et al., '\n",
      " '2023b).\\\\nRecent large multimodal models are trained with\\\\nlarge-scale '\n",
      " 'pretraining data and billions of trainable\\\\nparameters, allowing them to '\n",
      " 'excel at generaliza-\\\\ntion. Contrastive models such as CLIP (Radford\\\\net '\n",
      " 'al., 2021) connect images and texts by aligning\\\\ntwo modality-specific '\n",
      " 'encoders using contrastive\\\\nloss. Generative models such as LLaV A (Liu et '\n",
      " 'al.,\\\\n2023b,a) embed images into a decoder-only lan-\\\\nguage model as '\n",
      " 'prefix tokens and utilize next-token\\\\nprediction loss to align the two '\n",
      " 'modalities. Though\\\\nsuch models demonstrate impressive '\n",
      " \"capabilities\\\\nacross various vision-language tasks such as re-' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 1}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='prediction loss to align the two modalities. Though\\\\nsuch \"\n",
      " 'models demonstrate impressive capabilities\\\\nacross various vision-language '\n",
      " 'tasks such as re-\\\\ntrieval and visual question answering, they '\n",
      " 'perform\\\\nless desirably over compositional reasoning (Di-\\\\nwan et al., '\n",
      " '2022; Hsieh et al., 2023).\\\\n2.2 Compositional Reasoning\\\\nCompositional '\n",
      " 'reasoning (Diwan et al., 2022) in\\\\nvision and language tasks involves '\n",
      " 'evaluating mod-\\\\nels’ ability to understand and manipulate complex\\\\nideas '\n",
      " 'by breaking them down into simpler, con-\\\\nstituent components before '\n",
      " 'recombining them in\\\\nnovel ways. A typical example can be '\n",
      " 'distinguish-\\\\ning “blank pants and white shorts” versus “white\\\\npants and '\n",
      " 'blank shorts”. Winoground (Diwan et al.,\\\\n2022) is the pioneering benchmark '\n",
      " 'for composi-\\\\ntional reasoning, composed of 400 items, each with\\\\ntwo '\n",
      " 'images and two corresponding captions. Given\\\\nan image, multimodal models '\n",
      " \"are asked to find the\\\\nmatching caption from the provided two options,' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 1}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='two images and two corresponding captions. Given\\\\nan image, \"\n",
      " 'multimodal models are asked to find the\\\\nmatching caption from the provided '\n",
      " 'two options,\\\\nand vice versa. SugarCrepe (Hsieh et al., 2023)\\\\nand SeeTrue '\n",
      " '(Yarom et al., 2023) further scale the\\\\ndistractive captions by leveraging '\n",
      " 'language models\\\\nfor automated generation.\\\\nNegCLIP (Yuksekgonul et al., '\n",
      " '2023) attempts to\\\\nimprove compositional reasoning via fine-tuning\\\\nCLIP '\n",
      " \"with perturbed captions as the negative text\\\\n2' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 1}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='There are 3 cars and 4 personsThere are 2 cars and 3 \"\n",
      " 'persons(b) Flickr30k-Counting GroundedImage InpaintingNegative ImageDigit '\n",
      " 'ManipulationPositive CaptionNegative CaptionA man with orange hat and blue '\n",
      " 'vestA man with red hat and orange vestA woman with blue hatand orange '\n",
      " 'vest\\\\nA man with blue hat and orange vestPositive Image\\\\nPositive '\n",
      " 'Caption(a) Flickr30k-Positions HorizontalFlip\\\\nFace is to the left of '\n",
      " 'toysPositive ImagePositive Caption\\\\nPositionalKeyword FlipFace is to the '\n",
      " 'right of toysGPT-4V AssistedCaption '\n",
      " 'GenerationReplaceNounReplaceAdjectiveSwapAdjectivesNegative '\n",
      " 'CaptionDALLE-3T2I GenerationDALLE-3T2I Generation\\\\nDALLE-3T2I Generation(c) '\n",
      " 'Flickr30k-Attributes Negative ImageNegative ImageNegative CaptionPositive '\n",
      " 'ImageFigure 2: The data curation pipeline of CounterCurate. Given a positive '\n",
      " 'image-caption pair, we first generate the\\\\nnegative captions, based on '\n",
      " 'which we curate the negative images using the most suitable approach. '\n",
      " \"Specifically,' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '2}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='negative captions, based on which we curate the negative \"\n",
      " 'images using the most suitable approach. Specifically,\\\\n(a) for '\n",
      " 'Flickr30k-Positions (left/right), we flip the positional keyword before '\n",
      " 'conducting the horizontal flip for the\\\\nimage; (b) for Flickr30k-Counting, '\n",
      " 'we manipulate the digit before applying grounded image inpainting (Li et '\n",
      " 'al.,\\\\n2023) as the negative image; (c) for Flickr30k-Attributes, we first '\n",
      " 'leverage GPT-4V (OpenAI, 2023c) to generate\\\\nreasonable hard negative '\n",
      " 'captions for replacing the noun, replacing the adjective, and swapping the '\n",
      " 'adjectives. Then\\\\nwe leverage DALLE-3 (Betker et al., 2023) to generate '\n",
      " 'coherent images.\\\\nsamples in contrastive learning. However, such\\\\npermuted '\n",
      " 'captions can be easily detected by a text-\\\\nonly transformer (Lin et al., '\n",
      " '2023), demonstrating\\\\nthat a more effective approach is needed to '\n",
      " 'further\\\\nenhance compositional reasoning. DAC (Doveh\\\\net al., 2024) '\n",
      " 'leverages dense and aligned captions\\\\nfor better positive captions. SGVL '\n",
      " \"(Herzig et al.,' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 2}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='enhance compositional reasoning. DAC (Doveh\\\\net al., 2024) \"\n",
      " 'leverages dense and aligned captions\\\\nfor better positive captions. SGVL '\n",
      " '(Herzig et al.,\\\\n2023) utilizes scene graph to fine-tune the pre-\\\\ntrained '\n",
      " 'vision-language model. TSVLC (Doveh\\\\net al., 2023) leverages language '\n",
      " 'models to gen-\\\\nerate positive and negative captions. '\n",
      " 'COCO-\\\\nCounterfactual (Le et al., 2023) further utilizes both\\\\nnegative '\n",
      " 'images and negative captions to fine-tune\\\\nCLIP via a rule-based data '\n",
      " 'curation pipeline.\\\\nMoreover, previous works mainly consider '\n",
      " 'com-\\\\nposition reasoning with semantically different con-\\\\ncepts rather '\n",
      " 'than physically grounded relationships\\\\nsuch as object counting and '\n",
      " 'distinguishing left/right\\\\nand above/below. Though a recent work '\n",
      " '(Paiss\\\\net al., 2023) improved CLIP’s counting capabili-\\\\nties, they '\n",
      " 'neither addressed compositional reason-\\\\ning nor leveraged negative images. '\n",
      " 'In our paper, we\\\\nsystematically improve both semantic and physi-\\\\ncal '\n",
      " \"compositional reasoning by leveraging the most' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 2}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='systematically improve both semantic and physi-\\\\ncal \"\n",
      " 'compositional reasoning by leveraging the most\\\\ncapable generative models '\n",
      " 'including GPT-4V (Ope-\\\\nnAI, 2023c) and DALLE-3 (Betker et al., '\n",
      " '2023).\\\\n2.3 Counterfactual Reasoning\\\\nCounterfactual reasoning (Morgan and '\n",
      " 'Winship,\\\\n2015) refers to the process of imagining “what\\\\nif” scenarios by '\n",
      " 'modifying the input data. In the\\\\ncontext of visio-linguistics scenarios, '\n",
      " 'this involves\\\\ncurating negative images and captions by manip-\\\\nulating '\n",
      " 'the original data and observing how it af-fects the outcome. This helps '\n",
      " 'models understand\\\\ncause and effect and predict outcomes in '\n",
      " 'situations\\\\nthey’ve never seen before. The key to counter-\\\\nfactual '\n",
      " 'reasoning is curating meaningful and hard\\\\nenough examples. '\n",
      " 'COCO-Counterfactual (Le et al.,\\\\n2023) explores simple linguistic rules to '\n",
      " 'generate\\\\nnegative captions and uses an image editing model\\\\nPrompt2Prompt '\n",
      " '(Hertz et al., 2022) to produce neg-\\\\native images (Hertz et al., 2022). '\n",
      " \"Prompt2Prompt is' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 2}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Prompt2Prompt (Hertz et al., 2022) to produce neg-\\\\native \"\n",
      " 'images (Hertz et al., 2022). Prompt2Prompt is\\\\nless desirable in '\n",
      " 'understanding complex language\\\\ninstructions, making the generated '\n",
      " 'counterfactuals\\\\nless reliable and challenging.\\\\nIn this paper, we collect '\n",
      " 'the most challenging\\\\nnegative image and text examples by leveraging\\\\nthe '\n",
      " 'most capable language models GPT-4 (OpenAI,\\\\n2023b), text-to-image '\n",
      " 'generation model DALLE-\\\\n3 (Betker et al., 2023), and GLIGEN (Li et '\n",
      " 'al.,\\\\n2023), significantly improving our model’s reason-\\\\ning. Utilizing '\n",
      " 'these hard negatives, our model can\\\\nbetter grasp the nuances of language '\n",
      " 'and vision,\\\\nenhancing its performance on tasks that require '\n",
      " 'a\\\\nsophisticated understanding of the world.\\\\n3 Approach\\\\nIn Sec. 3.1, we '\n",
      " 'first justify our selecting Flickr30k\\\\nEntities dataset for both '\n",
      " 'benchmarking and fine-\\\\ntuning. Further in Sec. 3.2 and 3.3, we '\n",
      " 'demonstrate\\\\nthe near-chance performance of multimodal mod-\\\\nels like CLIP '\n",
      " \"and LLaV A on physically grounded' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 2}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='tuning. Further in Sec. 3.2 and 3.3, we demonstrate\\\\nthe \"\n",
      " 'near-chance performance of multimodal mod-\\\\nels like CLIP and LLaV A on '\n",
      " 'physically grounded\\\\ncompositional reasoning tasks, including '\n",
      " 'positional\\\\nunderstanding and counting, and improve their per-\\\\nformance '\n",
      " 'by generating negative image-text pairs\\\\nvia data augmentation and grounded '\n",
      " \"image inpaint-\\\\ning. Finally, we introduce using capable text and\\\\n3' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 2}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Positive CaptionsNegative Captions\\\\nNegative \"\n",
      " 'Images\\\\nPositiveImages\\\\nA woman with blue hat and orange vestA man with '\n",
      " 'blue hat and orange vestLarge Multimodal Model:Given the image, Choose the '\n",
      " 'correct caption:A. A man with blue hat and orange vest; B. A woman with blue '\n",
      " 'hat and orange vest.\\\\n: B. The correct caption is “A woman with blue hat '\n",
      " 'and orange vest”… \\\\n(a) Fine-tuning using contrastive learning (CLIP) (b) '\n",
      " 'Fine-tuning using generative models (LLaV A)Figure 3: Fine-tuning different '\n",
      " 'types of large multimodal models with CounterCurate. Our pipeline can '\n",
      " 'enhance\\\\nboth contrastive learning models and generative models by '\n",
      " 'augmenting vanilla image-caption pairs with curated\\\\nnegative images and '\n",
      " 'captions. Specifically, our counterfactual image-caption pairs (a) provide '\n",
      " 'auxiliary contrastive\\\\nloss for models like CLIP, where positive '\n",
      " \"contrastive units in the similarity matrix are painted as blue/red and' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 3}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='loss for models like CLIP, where positive contrastive units \"\n",
      " 'in the similarity matrix are painted as blue/red and\\\\nnegative ones are '\n",
      " 'painted as white, and (b) can be naturally integrated into the original '\n",
      " 'next-token prediction loss in\\\\ntext generation models such as LLaV '\n",
      " 'A.\\\\nimage generation models to improve models’ se-\\\\nmantic compositional '\n",
      " 'reasoning in Sec. 3.4.\\\\n3.1 Necessity of Grounded Image Captions\\\\nPrevious '\n",
      " 'approaches (Yuksekgonul et al., 2023;\\\\nHsieh et al., 2023) adopt global '\n",
      " 'image captioned\\\\ndatasets such as COCO-Captions (Chen et al.,\\\\n2015) and '\n",
      " 'craft negative captions to either bench-\\\\nmark or improve compositional '\n",
      " 'reasoning. This\\\\nmethod, however, lacks regional information im-\\\\nportant '\n",
      " 'to physically grounded reasoning, such as\\\\ndetermining the relative '\n",
      " 'position of two objects.\\\\nTherefore, we leverage Flickr30k Entities '\n",
      " '(Plum-\\\\nmer et al., 2016), a grounded image captioning\\\\ndataset to both '\n",
      " \"curate negative image-caption pairs\\\\nand benchmark models.' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 3}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='mer et al., 2016), a grounded image captioning\\\\ndataset to \"\n",
      " 'both curate negative image-caption pairs\\\\nand benchmark models.\\\\n3.2 '\n",
      " 'Improving Positional Understanding\\\\nCurrently, there is no benchmark that '\n",
      " 'directly eval-\\\\nuates the positional understanding of multimodal\\\\nmodels. '\n",
      " 'We build such a benchmark, Flickr30k-\\\\nPositions as in Figure 2 (a), that '\n",
      " 'is composed of\\\\npositive and negative image-caption pairs describ-\\\\ning '\n",
      " 'the same objects in opposite positions.\\\\nGenerating negative captions. '\n",
      " 'Specifically, we\\\\nutilize the bounding boxes\\\\nB={Bj= (xj1, yj1, xj2, yj2)| '\n",
      " '∀j∈I}\\\\nin Flickr30k Entities (Plummer et al., 2016) for\\\\neach object jin '\n",
      " 'Image Ithat only has one boundingbox, where box Bj’s upper-left and '\n",
      " 'lower-right cor-\\\\nners are at (xj1, yj1)and(xj2, yj2), respectively.\\\\nFor '\n",
      " 'each pair of objects {a, b} ⊆I, we identify the\\\\npositional object pairs '\n",
      " 'via the '\n",
      " 'formula:\\\\n\\\\uf8f1\\\\n\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f2\\\\n\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f3xa2≤xb1=⇒ais '\n",
      " 'to the leftofb\\\\nxa1≥xb2=⇒ais to the right ofb\\\\nya2≤yb1=⇒aisabove '\n",
      " \"b\\\\nya1≥yb2=⇒aisbelow b' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 3}\")\n",
      "('chunk num 1 : \\n'\n",
      " ' '\n",
      " \"page_content='\\\\uf8f1\\\\n\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f2\\\\n\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f4\\\\uf8f3xa2≤xb1=⇒ais \"\n",
      " 'to the leftofb\\\\nxa1≥xb2=⇒ais to the right ofb\\\\nya2≤yb1=⇒aisabove '\n",
      " 'b\\\\nya1≥yb2=⇒aisbelow b\\\\nWith the formula, we generate positive and '\n",
      " 'neg-\\\\native positional captions CandC′such as C=“a\\\\nbike is to the left of '\n",
      " 'a woman” vs.C′=“a bike is\\\\nto the right of a women” andC=“a table is '\n",
      " 'above\\\\na towel” vs.C′=“a table is below a towel” .\\\\nWe also prompt '\n",
      " 'text-only GPT4 (OpenAI,\\\\n2023b) to rewrite the vanilla negative prompt '\n",
      " '“a\\\\nis above/below b”to better guide image genera-\\\\ntion models. For '\n",
      " 'example, the caption “a street is\\\\nabove young child\" is very unnatural, '\n",
      " 'while GPT-\\\\nexpanded caption “the street stretches out above a\\\\nyoung '\n",
      " 'child, elevated by a bridge” is much better.\\\\nFor detailed prompt '\n",
      " 'engineering, see Appendix B.\\\\nGenerating negative images. Furthermore, '\n",
      " 'we\\\\ninclude negative images in our dataset to curate\\\\nFlickr30k-Positions. '\n",
      " 'For the left-right pairs, we\\\\napply a simple data augmentation method: '\n",
      " \"flip\\\\nthe original images horizontally. For the above-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 3}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Flickr30k-Positions. For the left-right pairs, we\\\\napply a \"\n",
      " 'simple data augmentation method: flip\\\\nthe original images horizontally. '\n",
      " \"For the above-\\\\nbelow pairs, however, flipping vertically without\\\\n4' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 3}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Figure 4: To generate the correct above-below \"\n",
      " 'negative\\\\nimage via GLIGEN with the original prompt \" the ball is\\\\nbelow '\n",
      " 'the sports outfit \", we recenter the bounding boxes\\\\nof \"ball\" and \" sports '\n",
      " 'outfit \" and feed them into GLIGEN\\\\ntogether with an expanded prompt from '\n",
      " 'GPT4.\\\\ncontext would lead to unreasonable images. To\\\\naddress this issue, '\n",
      " 'we utilize GLIGEN (Li et al.,\\\\n2023), an object-level text-to-image '\n",
      " 'generation\\\\nmodel that can ground text prompts with bounding\\\\nboxes. In '\n",
      " 'particular, for each above-below object\\\\npair(a, b)∈I, we swap the centers '\n",
      " 'of the bound-\\\\ning boxes Ba, Bbwhile maintaining their widths\\\\nand '\n",
      " 'heights, ensuring that the generated images\\\\nare natural while preserving '\n",
      " 'the objects’ sizes and\\\\naspect-ratios as demonstrated in Figure 4. The '\n",
      " 'new\\\\nbounding boxes and prompts are fed to GLIGEN\\\\nto generate the '\n",
      " 'corresponding negative image I′.\\\\n3.3 Improving Object Counting\\\\nWe '\n",
      " 'introduce Flickr30k-Counting to enhance\\\\nLMMs’ object counting '\n",
      " \"capabilities. We again cu-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 4}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='3.3 Improving Object Counting\\\\nWe introduce \"\n",
      " 'Flickr30k-Counting to enhance\\\\nLMMs’ object counting capabilities. We again '\n",
      " 'cu-\\\\nrate counterfactual image-caption pairs as in Figure\\\\n2 (b). We count '\n",
      " 'the number of bounding boxes for\\\\neach object category in '\n",
      " 'Flickr30k-Entities (Plum-\\\\nmer et al., 2016). For two distinct object '\n",
      " 'cate-\\\\ngories {P, Q} ⊆ I, we generate a positive cap-\\\\ntionC=“there are '\n",
      " 'nPP’s and nQQ’s”, such as\\\\n“there are three cats and four dogs” where '\n",
      " 'category\\\\nPis “cat” and category Qis “dog”.\\\\nGenerating negative captions. '\n",
      " 'If both objects\\\\ndo not spatially overlap with other objects, we '\n",
      " 'gen-\\\\nerate the corresponding negative caption by decre-\\\\nmenting the one '\n",
      " 'with more objects, Q, and in-\\\\ncrementing the one with fewer, P; for '\n",
      " 'example,C′=“there are four cats and three dogs” . This en-\\\\nforces a hard '\n",
      " 'counterfactual format as the numbers\\\\nin the positives are reversed in the '\n",
      " 'negatives.\\\\nIn most cases, however, objects in an image do\\\\noverlap, and '\n",
      " \"we cannot ascertain that the incre-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 4}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='in the positives are reversed in the negatives.\\\\nIn most \"\n",
      " 'cases, however, objects in an image do\\\\noverlap, and we cannot ascertain '\n",
      " 'that the incre-\\\\nment/decrement rule still generates hard '\n",
      " 'counter-\\\\nfactuals. For example, if a dog overlaps with a\\\\ncat, and we '\n",
      " 're-use the rule above to remove one\\\\ndog and add one cat, we end up with '\n",
      " '“three cats\\\\nand three dogs”. In this case, we can only remove\\\\neither one '\n",
      " 'of Por one of Q, for example, the dog,\\\\nand all objects it overlaps with. '\n",
      " 'An example of the\\\\nresulting negative caption is C′=“there are two\\\\ncats '\n",
      " 'and three dogs. ”\\\\nGenerating negative images. In the case where\\\\nobjects '\n",
      " 'do not overlap, we simply leverage GLI-\\\\nGEN (Li et al., 2023) to inpaint a '\n",
      " 'new object of\\\\nthe incremented category Pat the bounding box of\\\\none of '\n",
      " 'the objects from the decremented category\\\\nQ. Otherwise, we remove the '\n",
      " 'target objects using\\\\nGLIGEN to replace it with an object from a '\n",
      " \"generic\\\\ncategory. We simply use the plant category for' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 4}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Q. Otherwise, we remove the target objects using\\\\nGLIGEN to \"\n",
      " 'replace it with an object from a generic\\\\ncategory. We simply use the plant '\n",
      " 'category for\\\\nreplacement, as plants can appear in almost any nat-\\\\nural '\n",
      " 'setting, let it be outdoor or indoor, and we find\\\\nthat GLIGEN does well in '\n",
      " 'generating it according\\\\nto each image I’s background during '\n",
      " 'inpainting\\\\nfor each corresponding negative image I′. More\\\\ndetails in '\n",
      " 'Appendix C.\\\\n3.4 Improving Semantic Compositional\\\\nReasoning\\\\nExisting '\n",
      " 'works (Yuksekgonul et al., 2023; Le et al.,\\\\n2023) use a rule-based '\n",
      " 'approach to generate neg-\\\\native images/captions for semantic '\n",
      " 'compositional\\\\nreasoning, resulting in less desirable negatives such\\\\nas '\n",
      " 'uncommon negative captions including “A dog\\\\nwith blue fur\" from “A dog '\n",
      " 'with black fur\" . We\\\\nintroduce Flickr30k-Attributes as in Figure 2 '\n",
      " '(c),\\\\nwhich utilizes GPT-4V (OpenAI, 2023c) to gener-\\\\nate hard negative '\n",
      " 'captions and leverages DALLE-\\\\n3 (Betker et al., 2023) for generating '\n",
      " \"corresponding' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '4}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='which utilizes GPT-4V (OpenAI, 2023c) to gener-\\\\nate hard \"\n",
      " 'negative captions and leverages DALLE-\\\\n3 (Betker et al., 2023) for '\n",
      " 'generating corresponding\\\\nhigh-quality negative images. We extract the '\n",
      " 'first\\\\nand most detailed caption Cfor each image Ifrom\\\\nthe original '\n",
      " 'Flickr30k dataset (Young et al., 2014).\\\\nGenerating negative captions. To '\n",
      " 'generate high-\\\\nquality negative captions, we leverage GPT-4V and\\\\nfeed '\n",
      " 'both positive images Iand captions Cto gen-\\\\nerate negative captions C′. To '\n",
      " 'let GPT-4V know\\\\nthe objects’ locations, we overlay bounding boxes\\\\nBonto '\n",
      " 'the original positive image (Cai et al., 2023)\\\\nand also feed this '\n",
      " \"annotated image into GPT-4V .\\\\n5' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 4}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='We prompt GPT-4V to generate three kinds of cap-\\\\ntions, \"\n",
      " 'including (i) changing a noun, (ii) changing\\\\nan adjective, and (iii) '\n",
      " 'swapping any of the two\\\\nphrases, nouns, adjectives, etc. For example, '\n",
      " 'given\\\\nthe original caption C“a man wearing a black\\\\nshirt and blue jeans\" '\n",
      " ', GPT-4V can generate corre-\\\\nsponding negatives C′: (i)“a man wearing a '\n",
      " 'black\\\\njacket and blue jeans\" , (ii) “a man wearing a black\\\\nshirt and '\n",
      " 'redjeans\" , and (iii) “a man wearing a\\\\nblue shirt and black jeans\" . '\n",
      " 'Detailed prompts to\\\\nGPT-4V are shown in Appendix A.\\\\nGenerating negative '\n",
      " 'images. We simply feed\\\\nthe curated captions C′from GPT-4V directly '\n",
      " 'into\\\\nthe currently most capable text-to-image generation\\\\nmodel, DALLE-3 '\n",
      " '(Betker et al., 2023), to generate\\\\nhigh-quality negative images I′.\\\\n3.5 '\n",
      " 'Fine-tuning Methodology\\\\nAs demonstrated in Figure 3, CounterCurate can '\n",
      " 'be\\\\nused to fine-tune both contrastive (Radford et al.,\\\\n2021) and '\n",
      " 'generative (Liu et al., 2023b) multimodal\\\\nmodels, which significantly '\n",
      " \"improve their perfor-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 5}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='2021) and generative (Liu et al., 2023b) multimodal\\\\nmodels, \"\n",
      " 'which significantly improve their perfor-\\\\nmance. To fine-tune a '\n",
      " 'contrastive model like CLIP,\\\\nwe sample Npositive image-text pairs and '\n",
      " 'the\\\\ncorresponding negative image-text pairs to form\\\\na batch (C, I, C′, '\n",
      " 'I′)as shown in Figure 3 (a). We\\\\nrefer to this method as Grouping , where '\n",
      " 'our model\\\\nis forced to distinguish the positive pairs from '\n",
      " 'their\\\\ncorresponding negatives, which is demonstrated to\\\\nbe helpful in '\n",
      " 'experiments. We also conduct a de-\\\\ntailed analysis of the effectiveness of '\n",
      " 'this method\\\\nin Section 4.5. The training loss is the same as\\\\nthe '\n",
      " 'original, where cross-entropy loss is used to\\\\nmaximize the diagonal '\n",
      " 'elements and minimize all\\\\nother entries in the similarity matrix.\\\\nFor '\n",
      " 'the generative models like LLaV A (Liu et al.,\\\\n2023b,a), we reformat our '\n",
      " 'data into conversations\\\\nand follow their exact training paradigm to '\n",
      " 'fine-\\\\ntune, as shown in Figure 3 (b). The training loss is\\\\nthe original '\n",
      " \"next-token prediction loss.' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 5}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='and follow their exact training paradigm to fine-\\\\ntune, as \"\n",
      " 'shown in Figure 3 (b). The training loss is\\\\nthe original next-token '\n",
      " 'prediction loss.\\\\n4 Experiments\\\\nWe utilize the proposed three datasets, '\n",
      " 'Flickr30k-\\\\nPositions, Flickr30k-Counting, and Flickr30k-\\\\nAttributes, '\n",
      " 'created via CounterCurate, to fine-tune\\\\nboth contrastive and generative '\n",
      " 'models. Specifi-\\\\ncally, we select two common multimodal models,\\\\nViT-B/32 '\n",
      " 'from (Ilharco et al., 2021) OpenCLIP pre-\\\\ntrained on LAION-2B (Schuhmann '\n",
      " 'et al., 2022)\\\\nand LLaV A-1.5 (Liu et al., 2023a) as base models.\\\\nAn '\n",
      " 'overview of our results is attached in Figure 5.\\\\nX(b)CLIP√(a)+ '\n",
      " 'OursX(b)LLaVA√(a)+ Ours(a) With (b) Without\\\\nSugarCrepe(add)The boy is <?> '\n",
      " 'shoes\\\\nX(b)CLIP√(a)+Ours√(a)LLaVA√(a)+Ours(a) White/black (b) '\n",
      " 'Black/whiteSugarCrepe(Swap)<?> shirt <?> '\n",
      " 'shorts\\\\n√(b)CLIP√(b)+OursX(a)LLaVA√(b)+Ours(a) Far (b) '\n",
      " 'CloseSugarCrepe(replace)Zebras are <?> to each otherFigure 5: Qualitative '\n",
      " \"examples of models’ composi-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 5}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='√(b)CLIP√(b)+OursX(a)LLaVA√(b)+Ours(a) Far (b) \"\n",
      " 'CloseSugarCrepe(replace)Zebras are <?> to each otherFigure 5: Qualitative '\n",
      " 'examples of models’ composi-\\\\ntional reasoning capabilities before/after '\n",
      " 'being finetuned\\\\nvia our approach CounterCurate. Wrong answers are\\\\nmarked '\n",
      " 'in red. Our approach enhances both CLIP and\\\\nLLaV A’s reasoning '\n",
      " 'capabilities.\\\\n4.1 Implementation Details\\\\nWhen generating images for '\n",
      " 'Flickr30k-Attributes,\\\\nwe provide DALLE-3 (Betker et al., 2023) with '\n",
      " 'hd\\\\nquality and natural style. We train CLIP (Radford\\\\net al., 2021; '\n",
      " 'Ilharco et al., 2021) and LLaV A (Liu\\\\net al., 2023a) on 1 and 4 NVIDIA '\n",
      " 'A100 80GB\\\\nGPUs, respectively. When fine-tuning CLIP, we\\\\nset Adam '\n",
      " 'optimizer’s β1andβ2to be 0.9 and 0.98,\\\\nand weight decay to be 0.2.\\\\nFor '\n",
      " 'Flickr30k-Attributes, we trained the CLIP\\\\nmodel with a learning rate of '\n",
      " '1e-5, batch size of\\\\n256, and mixed precision for 5 epochs. For LLaV '\n",
      " 'A-\\\\n1.5, we used a learning rate of 2e-6 and batch size\\\\nof 16 for 1 '\n",
      " \"epoch. For Flickr30k-Positions, we used' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 5}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='1.5, we used a learning rate of 2e-6 and batch size\\\\nof 16 \"\n",
      " 'for 1 epoch. For Flickr30k-Positions, we used\\\\na learning rate of 2.56e-5 '\n",
      " 'and batch size of 1024\\\\nfor CLIP, and trained for 50 epochs without '\n",
      " 'group-\\\\ning and 15 epochs with grouping. For Flickr30k-\\\\nCounting, CLIP '\n",
      " 'was trained with a learning rate of\\\\n5e-5. The LLaV A parameters for both '\n",
      " 'Flickr30k-\\\\nPositions and Flickr30k-Counting were identical,\\\\nwith a '\n",
      " 'learning rate of 2.56e-5 and batch size of 16.\\\\n4.2 Evaluating Positional '\n",
      " 'Understanding\\\\nAgain, to the best of our knowledge, we are the\\\\nfirst to '\n",
      " 'comprehensively evaluate a multimodal\\\\nmodel’s ability in understanding the '\n",
      " 'left-and-right\\\\nand/or above-and-below positioning relations be-\\\\ntween '\n",
      " 'objects. We conduct a 4:1 train-test split\\\\non Flickr30k-Positions and use '\n",
      " 'the test subset as\\\\nthe benchmark. We further separate this dataset\\\\ninto '\n",
      " '3 subsets: left-and-right only, above-and-\\\\nbelow only, and both. We '\n",
      " \"evaluate our fine-tuned\\\\nCLIP models by comparing the CLIP similar-' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 5}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='into 3 subsets: left-and-right only, above-and-\\\\nbelow only, \"\n",
      " 'and both. We evaluate our fine-tuned\\\\nCLIP models by comparing the CLIP '\n",
      " \"similar-\\\\nity scores between the four image-caption pairs,\\\\n6' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 5}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Models LR AB Both\\\\nRandom 50.00 50.00 50.00\\\\nCLIP 50.55 \"\n",
      " '52.80 51.56\\\\n+ CounterCurate 75.88 91.52 84.90\\\\nLLaV A-1.5 61.84 56.69 '\n",
      " '59.17\\\\n+ CounterCurate 95.72 96.21 96.00\\\\nTable 1: Performance of CLIP and '\n",
      " 'LLaV A on Flickr30k-\\\\nPositions’s test dataset. “LR” means left-and-right, '\n",
      " 'and\\\\n“AB” means above-and-below.\\\\n(C, I),(C, I′),(C′, I),(C′, I′)from each '\n",
      " 'group of\\\\ndata in the dataset, where (C, I, C′, I′)denotes the\\\\npositive '\n",
      " 'and negative image/caption, respectively.\\\\nThe model receives a score of '\n",
      " '0.5 if s(C,I)> s (C,I′)\\\\nand another score of 0.5 if s(C′,I)< s (C′,I′), '\n",
      " 'where\\\\ns(C,I)is the CLIP cosine-similarity score between\\\\ncaption Cand '\n",
      " 'image I. For LLaV A-1.5, we simply\\\\nquery the model to choose between the '\n",
      " 'positive and\\\\nnegative captions when presented with the ground\\\\ntruth '\n",
      " 'image. The results are shown in Table 1.\\\\nAs we hypothesized, LMMs are '\n",
      " 'indeed largely\\\\noblivious to the objects’ positioning in the im-\\\\nage, '\n",
      " \"which is especially manifested in the vanilla' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 6}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='As we hypothesized, LMMs are indeed largely\\\\noblivious to \"\n",
      " 'the objects’ positioning in the im-\\\\nage, which is especially manifested in '\n",
      " 'the vanilla\\\\nCLIP’s performance, which is only marginally bet-\\\\nter than '\n",
      " 'random guessing. Vanilla LLaV A-1.5\\\\nshows slightly better '\n",
      " 'performance.\\\\nAfter fine-tuning with the training split '\n",
      " 'of\\\\nFlickr30k-Positions, both models perform signifi-\\\\ncantly better '\n",
      " 'across all subsets. Specifically, for the\\\\nmixed case, CLIP improves by '\n",
      " '33%, and LLaV A\\\\nachieves a high accuracy of 96%. These '\n",
      " 'results\\\\ndemonstrate that CounterCurate is highly effective\\\\nacross '\n",
      " 'different kinds of multimodal models.\\\\n4.3 Evaluating Object '\n",
      " 'Counting\\\\nHere we show the counting capability of our mod-\\\\nels fine-tuned '\n",
      " 'on Flickr30k-Counting. We select a\\\\ndifferent dataset, PointQA-LookTwice '\n",
      " '(Mani et al.,\\\\n2022), as the evaluation benchmark. '\n",
      " 'Specifically,\\\\nPointQA-LookTwice was designed to ask models\\\\nthe number '\n",
      " 'nJof occurrences of an object category\\\\nJin an image I. We reformat this '\n",
      " \"dataset such' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '6}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='PointQA-LookTwice was designed to ask models\\\\nthe number \"\n",
      " 'nJof occurrences of an object category\\\\nJin an image I. We reformat this '\n",
      " 'dataset such\\\\nthat for every J, we generate a positive caption\\\\nCJ=“There '\n",
      " 'are nJJ’s”and a negative caption\\\\nC′\\\\nJ=“There are nJ+ 1J’s”. For example, '\n",
      " 'given\\\\nCJ“there are 3 dogs” ,C′\\\\nJis“there are 4 dogs” .\\\\nSimilar to what '\n",
      " 'we did in Section 4.2, we mark\\\\nthat a model made a correct prediction if '\n",
      " '(i) CLIP\\\\nshows s(CJ,I)> s (C′\\\\nJ,I)and (ii) LLaV A-1.5 cor-\\\\nrectly '\n",
      " 'chooses the option for CJ. The results are\\\\nshown in Table 2.CLIP LLaV '\n",
      " 'A-1.5\\\\nVanilla 57.50 44.87\\\\n+ CounterCurate 68.51 50.74\\\\nTable 2: '\n",
      " 'Comparison between CLIP and LLaV A on the\\\\nbenchmark created out of '\n",
      " 'PointQA-LookTwice.\\\\nAs CLIP performs slightly better than '\n",
      " 'random\\\\nguessing, it is surprising that LLaV A-1.5 performs\\\\nworse than '\n",
      " 'random. Nevertheless, fine-tuning with\\\\nFlickr30k-Counting improves both '\n",
      " 'models’ count-\\\\ning capability. This shows the effectiveness of us-\\\\ning '\n",
      " \"GLIGEN-generated negative images in Coun-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 6}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Flickr30k-Counting improves both models’ count-\\\\ning \"\n",
      " 'capability. This shows the effectiveness of us-\\\\ning GLIGEN-generated '\n",
      " 'negative images in Coun-\\\\nterCurate to tackle the problem of counting. '\n",
      " 'We\\\\nconduct further ablation studies in Appendix F.\\\\n4.4 Evaluating '\n",
      " 'Semantic Compositional\\\\nReasoning\\\\nSimilar to the setup in Sec 4.3, we '\n",
      " 'fine-tune both\\\\nCLIP and LLaV A-1.5 under our curated '\n",
      " 'Flickr30k-\\\\nAttributes. We use another common benchmark\\\\nSugarCrepe (Hsieh '\n",
      " 'et al., 2023) to evaluate the\\\\nmodel’s semantic compositional reasoning '\n",
      " 'capa-\\\\nbilities. SugarCrepe has three major categories\\\\ndepicted in Table '\n",
      " '3. The evaluation protocol is\\\\nalso the same as Sec 4.3. We choose '\n",
      " 'NegCLIP\\\\n(Yuksekgonul et al., 2023) and GPT-4V (OpenAI,\\\\n2023c) as the '\n",
      " 'representative strong baselines for\\\\ncontrastive and generative models '\n",
      " 'respectively.\\\\nWe observe significant improvements for both\\\\nCLIP and LLaV '\n",
      " 'A-1.5, both on average and categor-\\\\nically. Summarized results are shown '\n",
      " \"in Table 3,' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '6}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='We observe significant improvements for both\\\\nCLIP and LLaV \"\n",
      " 'A-1.5, both on average and categor-\\\\nically. Summarized results are shown '\n",
      " 'in Table 3,\\\\nwhere Appendix G contains more detailed scores.\\\\nFor example, '\n",
      " 'CounterCurate fine-tuned CLIP\\\\nsurpasses NegCLIP on average as well as in '\n",
      " 'two\\\\nmain categories. Note that the source of fine-\\\\ntuning data, '\n",
      " 'Flickr30k-Entities, contains much\\\\nfewer image-caption pairs than '\n",
      " 'COCO-Captions\\\\n(around 100k image-caption pairs) where NegCLIP\\\\nis '\n",
      " 'trained. Furthermore, when prompting GPT-4V\\\\nto generate the negative '\n",
      " 'captions, we intentionally\\\\nprompted GPT-4V to produce None when there '\n",
      " 'is\\\\nno feasible condition to conduct “swapping\". Such\\\\ntwo factors lead to '\n",
      " 'the fact that our data curation\\\\npipeline results much fewer negative '\n",
      " 'samples for\\\\nthe “swap” category, which is around 2.5k. We\\\\nargue that '\n",
      " 'using our pipeline on other datasets that\\\\ncan generate more of the “swap” '\n",
      " \"attribute shall lead\\\\nto larger improvements in performance.' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 6}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='argue that using our pipeline on other datasets that\\\\ncan \"\n",
      " 'generate more of the “swap” attribute shall lead\\\\nto larger improvements in '\n",
      " 'performance.\\\\nLLaV A-1.5 also performed better in all three cat-\\\\negories '\n",
      " \"after fine-tuning. It is also surprising that\\\\n7' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 6}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Categories CLIP NegCLIP + CounterCurate LLaV A-1.5 GPT-4V + \"\n",
      " 'CounterCurate\\\\nAdd 84.82 87.28 89.44 (+4.62) 86.02 91.63 97.13 '\n",
      " '(+11.11)\\\\nReplace 82.40 85.36 87.10 (+4.70) 92.38 93.53 92.82 '\n",
      " '(+0.43)\\\\nSwap 65.42 75.31 72.22 (+6.80) 85.95 88.21 90.88 (+4.93)\\\\nAverage '\n",
      " '81.23 84.85 86.15 (+4.92) 89.27 92.19 94.17 (+4.90)\\\\nTable 3: Comparison '\n",
      " 'between performances of CLIP and LLaV A-1.5 on SugarCrepe before and after '\n",
      " 'fine-tuning.\\\\nWe also add NegCLIP (Yuksekgonul et al., 2023) and GPT-4V '\n",
      " '(OpenAI, 2023c) as strong baselines for contrastive\\\\nand generative '\n",
      " 'multimodal models. The best performances are bolded, and improvements '\n",
      " 'against CLIP and LLaV A\\\\nare measured in the parentheses. CounterCurate '\n",
      " 'shows significant performance boost compared to both vanilla\\\\nCLIP/LLaV '\n",
      " 'A-1.5 model and advanced models such as GPT-4V .\\\\nour fine-tuned model '\n",
      " 'outperforms the SOTA LMM\\\\nGPT-4V both on average and in two of the '\n",
      " 'cate-\\\\ngories, most significantly over the “add” category.\\\\nWe observe '\n",
      " \"improvements on other datasets (Di-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 7}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='GPT-4V both on average and in two of the cate-\\\\ngories, most \"\n",
      " 'significantly over the “add” category.\\\\nWe observe improvements on other '\n",
      " 'datasets (Di-\\\\nwan et al., 2022) as well in Appendix E.\\\\nCounterCurate '\n",
      " 'shows better performance com-\\\\npared to the prior rule-based methods or '\n",
      " 'less desir-\\\\nable models that generates negatives. These signifi-\\\\ncant '\n",
      " 'improvements show that fine-tuning with accu-\\\\nrate and hard negative '\n",
      " 'samples is important, again\\\\ndemonstrating the effectiveness of our data '\n",
      " 'cura-\\\\ntion and fine-tuning pipeline for both contrastive\\\\nmodels and text '\n",
      " 'generation models for improving\\\\nsemantic counterfactual '\n",
      " 'understanding.\\\\n4.5 In-Depth Analysis\\\\nEffectiveness of Negative Images, '\n",
      " 'Negative Cap-\\\\ntions, and Grouping The core of CounterCurate\\\\nis to (i) '\n",
      " 'fine-tune models with both negative images\\\\nand negative captions and (ii) '\n",
      " 'use grouping to help\\\\nthe model better distinguish the positive pairs '\n",
      " \"from\\\\nthe negatives. Here we conduct rigorous ablation' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 7}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='the model better distinguish the positive pairs from\\\\nthe \"\n",
      " 'negatives. Here we conduct rigorous ablation\\\\nstudies to demonstrate the '\n",
      " 'necessity of each compo-\\\\nnent. We use the same training parameters and '\n",
      " 'test\\\\nwith the same methods as we did for fine-tuning\\\\nCLIP and evaluating '\n",
      " 'it on '\n",
      " 'Flickr30k-Positions.\\\\nNegative\\\\nImagesNegative\\\\nCaptionsGroup\\\\n-ingLR AB '\n",
      " 'Both\\\\n× × × 50.55 52.80 51.56\\\\n× ✓ × 55.86 62.25 58.68\\\\n✓ × × 54.35 56.79 '\n",
      " '54.95\\\\n✓ ✓ × 69.99 91.24 76.88\\\\n✓ ✓ ✓ 75.88 91.52 84.90\\\\nTable 4: '\n",
      " 'Ablation study demonstrating the necessity of\\\\nusing (i) negative images, '\n",
      " '(ii) negative captions, and\\\\n(iii) grouping strategies. Models are '\n",
      " 'fine-tuned and\\\\nevaluated on the Flickr30k-Positions dataset.\\\\nAs shown in '\n",
      " 'Table 4, even though using either\\\\nthe negative caption or the negative '\n",
      " 'image canimprove performance, the scores are significantly\\\\nworse than when '\n",
      " 'using both elements to fine-tune.\\\\nThis demonstrates that CounterCurate, '\n",
      " \"which incor-\\\\nporates both negative captions and negative images,' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 7}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='This demonstrates that CounterCurate, which incor-\\\\nporates \"\n",
      " 'both negative captions and negative images,\\\\nis necessary to achieve '\n",
      " 'desirable improvements.\\\\nGrouping also delivers further improvements '\n",
      " 'com-\\\\npared to not using this strategy.\\\\nCorrectness of DALLE-3 generated '\n",
      " 'images\\\\nWe randomly sample 300 DALLE-3 images gen-\\\\nerated from the '\n",
      " 'negative captions in Flickr30k-\\\\nAttributes for human annotators to check '\n",
      " 'whether\\\\nthe generated image is consistent (matches) with\\\\nthe negative '\n",
      " 'captions. We obtain a consistency\\\\nscore of 84.67%, which demonstrates the '\n",
      " 'high qual-\\\\nity of the DALLE-3 generated images.\\\\nPerformance on zero-shot '\n",
      " 'vision-language tasks\\\\nTo evaluate whether fine-tuning on the '\n",
      " 'counter-\\\\nfactual data hurts the original zero-shot vision-\\\\nlanguage '\n",
      " 'performance, we compare the image and\\\\ntext retrieval performance on MSCOCO '\n",
      " '(Lin et al.,\\\\n2015) of the original CLIP model and Counter-\\\\nCurate '\n",
      " 'fine-tuned model on Flickr30k-Attributes.\\\\nThe results in Table 5 show that '\n",
      " \"on average, the' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 7}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='2015) of the original CLIP model and Counter-\\\\nCurate \"\n",
      " 'fine-tuned model on Flickr30k-Attributes.\\\\nThe results in Table 5 show that '\n",
      " 'on average, the\\\\nmodel fine-tuned via CounterCurate maintains '\n",
      " 'per-\\\\nformance with marginal difference compared to the\\\\noriginal CLIP '\n",
      " 'model.\\\\nScores CLIP + CounterCurate\\\\nImage @1 39.44 37.81\\\\nImage @5 65.43 '\n",
      " '64.24\\\\nText @1 56.48 56.96\\\\nText @5 79.74 80.12\\\\nAverage @1 47.96 '\n",
      " '47.39\\\\nAverage @5 72.59 72.18\\\\nTable 5: Comparison of image and text '\n",
      " 'retrieval preci-\\\\nsion scores on the MSCOCO dataset between the '\n",
      " 'orig-\\\\ninal CLIP model and CounterCurate fine-tuned CLIP.\\\\nThe latter is '\n",
      " 'able to maintain overall performance with\\\\nminor improvements in text '\n",
      " \"retrieval precision.\\\\n8' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 7}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='5 Conclusion\\\\nIn conclusion, CounterCurate significantly \"\n",
      " 'en-\\\\nhances the visio-linguistic compositional reasoning\\\\ncapabilities of '\n",
      " 'multimodal contrastive and genera-\\\\ntive models. This is achieved by '\n",
      " 'addressing the ne-\\\\nglect of physically grounded reasoning and '\n",
      " 'exploit-\\\\ning the potential of using text and image generation\\\\nmodels for '\n",
      " 'semantic counterfactual fine-tuning. We\\\\nbelieve our contributions can pave '\n",
      " 'the way for fur-\\\\nther research in compositional '\n",
      " 'reasoning.\\\\nAcknowledgement\\\\nThis work was supported in part by NSF '\n",
      " 'CA-\\\\nREER IIS2150012, and Institute of Information\\\\n& communications '\n",
      " 'Technology Planning & Eval-\\\\nuation(IITP) grants funded by the Korea '\n",
      " 'govern-\\\\nment(MSIT) (No. 2022-0-00871, Development of\\\\nAI Autonomy and '\n",
      " 'Knowledge Enhancement for AI\\\\nAgent Collaboration) and (No. '\n",
      " 'RS2022-00187238,\\\\nDevelopment of Large Korean Language Model\\\\nTechnology '\n",
      " 'for Efficient Pre-training), and Mi-\\\\ncrosoft Accelerate Foundation Models '\n",
      " \"Research\\\\nProgram.\\\\nThis research is funded in part by the Univer-' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 8}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Technology for Efficient Pre-training), and Mi-\\\\ncrosoft \"\n",
      " 'Accelerate Foundation Models Research\\\\nProgram.\\\\nThis research is funded '\n",
      " 'in part by the Univer-\\\\nsity of Wisconsin–Madison L&S Honors '\n",
      " 'Program\\\\nthrough a Trewartha Senior Thesis Research '\n",
      " 'Grant.\\\\nLimitations\\\\nWe acknowledge that DALLE-3, one of the '\n",
      " 'gen-\\\\nerative models we use, is a closed-source text-to-\\\\nimage generation '\n",
      " 'model, which makes it difficult\\\\nto systematically analyze its '\n",
      " 'behavior.\\\\nReferences\\\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline '\n",
      " 'Luc,\\\\nAntoine Miech, Iain Barr, Yana Hasson, Karel\\\\nLenc, Arthur Mensch, '\n",
      " 'Katherine Millican, Malcolm\\\\nReynolds, et al. 2022. Flamingo: a visual '\n",
      " 'language\\\\nmodel for few-shot learning. NeurIPS , 35:23716–\\\\n23736.\\\\nJames '\n",
      " 'Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-\\\\nfeng Wang, Linjie Li, Long '\n",
      " 'Ouyang, Juntang Zhuang,\\\\nJoyce Lee, Yufei Guo, Wesam Manassra, '\n",
      " 'Prafulla\\\\nDhariwal, Casey Chu, Yunxin Jiao, and Aditya\\\\nRamesh. 2023. '\n",
      " \"Improving image generation with\\\\nbetter captions.' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 8}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Dhariwal, Casey Chu, Yunxin Jiao, and Aditya\\\\nRamesh. 2023. \"\n",
      " 'Improving image generation with\\\\nbetter captions.\\\\nMu Cai, Haotian Liu, '\n",
      " 'Siva Karthik Mustikovela, Gre-\\\\ngory P. Meyer, Yuning Chai, Dennis Park, '\n",
      " 'and\\\\nYong Jae Lee. 2023. Making large multimodal\\\\nmodels understand '\n",
      " 'arbitrary visual prompts. In\\\\narXiv:2312.00784 .Xi Chen, Xiao Wang, Soravit '\n",
      " 'Changpinyo, AJ Pier-\\\\ngiovanni, Piotr Padlewski, Daniel Salz, '\n",
      " 'Sebastian\\\\nGoodman, Adam Grycner, Basil Mustafa, Lucas\\\\nBeyer, Alexander '\n",
      " 'Kolesnikov, Joan Puigcerver, Nan\\\\nDing, Keran Rong, Hassan Akbari, Gaurav '\n",
      " 'Mishra,\\\\nLinting Xue, Ashish V Thapliyal, James Bradbury,\\\\nWeicheng Kuo, '\n",
      " 'Mojtaba Seyedhosseini, Chao Jia,\\\\nBurcu Karagol Ayan, Carlos Riquelme Ruiz, '\n",
      " 'An-\\\\ndreas Peter Steiner, Anelia Angelova, Xiaohua Zhai,\\\\nNeil Houlsby, '\n",
      " 'and Radu Soricut. 2023. PaLI: A\\\\njointly-scaled multilingual language-image '\n",
      " 'model.\\\\nInThe Eleventh International Conference on Learn-\\\\ning '\n",
      " \"Representations .\\\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 8}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='InThe Eleventh International Conference on Learn-\\\\ning \"\n",
      " 'Representations .\\\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\\\nishna '\n",
      " 'Vedantam, Saurabh Gupta, Piotr Dollár, and\\\\nC Lawrence Zitnick. 2015. '\n",
      " 'Microsoft coco captions:\\\\nData collection and evaluation server. arXiv '\n",
      " 'preprint\\\\narXiv:1504.00325 .\\\\nYen-Chun Chen, Linjie Li, Licheng Yu, '\n",
      " 'Ahmed\\\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\\\nJingjing Liu. 2020. '\n",
      " 'Uniter: Universal image-text\\\\nrepresentation learning. In European '\n",
      " 'conference on\\\\ncomputer vision , pages 104–120. Springer.\\\\nAnuj Diwan, '\n",
      " 'Layne Berry, Eunsol Choi, David Harwath,\\\\nand Kyle Mahowald. 2022. Why is '\n",
      " 'winoground\\\\nhard? investigating failures in visuolinguistic '\n",
      " 'compo-\\\\nsitionality. arXiv preprint arXiv:2211.00768 .\\\\nSivan Doveh, Assaf '\n",
      " 'Arbelle, Sivan Harary, Roei Herzig,\\\\nDonghyun Kim, Paola Cascante-Bonilla, '\n",
      " 'Amit Al-\\\\nfassy, Rameswar Panda, Raja Giryes, Rogerio Feris,\\\\net al. 2024. '\n",
      " 'Dense and aligned captions (dac) pro-\\\\nmote compositional reasoning in vl '\n",
      " \"models. Ad-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '8}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='fassy, Rameswar Panda, Raja Giryes, Rogerio Feris,\\\\net al. \"\n",
      " '2024. Dense and aligned captions (dac) pro-\\\\nmote compositional reasoning '\n",
      " 'in vl models. Ad-\\\\nvances in Neural Information Processing Systems '\n",
      " ',\\\\n36.\\\\nSivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz,\\\\nRoei '\n",
      " 'Herzig, Raja Giryes, Rogerio Feris, Rameswar\\\\nPanda, Shimon Ullman, and '\n",
      " 'Leonid Karlinsky. 2023.\\\\nTeaching structured vision & language concepts '\n",
      " 'to\\\\nvision & language models. In Proceedings of the\\\\nIEEE/CVF Conference '\n",
      " 'on Computer Vision and Pat-\\\\ntern Recognition , pages 2657–2668.\\\\nAmir '\n",
      " 'Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aber-\\\\nman, Yael Pritch, and Daniel '\n",
      " 'Cohen-Or. 2022.\\\\nPrompt-to-prompt image editing with cross '\n",
      " 'attention\\\\ncontrol. arXiv preprint arXiv:2208.01626 .\\\\nRoei Herzig, Alon '\n",
      " 'Mendelson, Leonid Karlinsky, As-\\\\nsaf Arbelle, Rogerio Feris, Trevor '\n",
      " 'Darrell, and Amir\\\\nGloberson. 2023. Incorporating structured '\n",
      " 'represen-\\\\ntations into pretrained vision & language models us-\\\\ning scene '\n",
      " \"graphs. EMNLP .' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 8}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Globerson. 2023. Incorporating structured represen-\\\\ntations \"\n",
      " 'into pretrained vision & language models us-\\\\ning scene graphs. EMNLP '\n",
      " '.\\\\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha\\\\nKembhavi, and Ranjay '\n",
      " 'Krishna. 2023. Sugarcrepe:\\\\nFixing hackable benchmarks for '\n",
      " 'vision-language\\\\ncompositionality.\\\\nGabriel Ilharco, Mitchell Wortsman, '\n",
      " 'Ross Wightman,\\\\nCade Gordon, Nicholas Carlini, Rohan Taori, Achal\\\\nDave, '\n",
      " \"Vaishaal Shankar, Hongseok Namkoong, John\\\\n9' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 8}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Miller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-\\\\nwig \"\n",
      " 'Schmidt. 2021. Openclip.\\\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, '\n",
      " 'Zarana\\\\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\\\\nLi, and Tom '\n",
      " 'Duerig. 2021. Scaling up visual and\\\\nvision-language representation '\n",
      " 'learning with noisy\\\\ntext supervision. In International conference on '\n",
      " 'ma-\\\\nchine learning , pages 4904–4916. PMLR.\\\\nTiep Le, Vasudev Lal, and '\n",
      " 'Phillip Howard. 2023. Coco-\\\\ncounterfactuals: Automatically constructed '\n",
      " 'counter-\\\\nfactual examples for image-text pairs.\\\\nYuheng Li, Haotian Liu, '\n",
      " 'Qingyang Wu, Fangzhou\\\\nMu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, '\n",
      " 'and\\\\nYong Jae Lee. 2023. Gligen: Open-set grounded\\\\ntext-to-image '\n",
      " 'generation.\\\\nTsung-Yi Lin, Michael Maire, Serge Belongie, '\n",
      " 'Lubomir\\\\nBourdev, Ross Girshick, James Hays, Pietro Perona,\\\\nDeva Ramanan, '\n",
      " 'C. Lawrence Zitnick, and Piotr Dol-\\\\nlár. 2015. Microsoft coco: Common '\n",
      " 'objects in con-\\\\ntext.\\\\nZhiqiu Lin, Xinyue Chen, Deepak Pathak, '\n",
      " \"Pengchuan\\\\nZhang, and Deva Ramanan. 2023. Visual-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 9}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='lár. 2015. Microsoft coco: Common objects in \"\n",
      " 'con-\\\\ntext.\\\\nZhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan\\\\nZhang, '\n",
      " 'and Deva Ramanan. 2023. Visual-\\\\ngptscore: Visio-linguistic reasoning with '\n",
      " 'multi-\\\\nmodal generative pre-training scores. arXiv '\n",
      " 'preprint\\\\narXiv:2306.01879 .\\\\nHaotian Liu, Chunyuan Li, Yuheng Li, and '\n",
      " 'Yong Jae\\\\nLee. 2023a. Improved baselines with visual instruc-\\\\ntion '\n",
      " 'tuning.\\\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and\\\\nYong Jae Lee. 2023b. '\n",
      " 'Visual instruction tuning.\\\\narXiv:2304.08485 .\\\\nArjun Mani, Nobline Yoo, '\n",
      " 'Will Hinthorn, and Olga\\\\nRussakovsky. 2022. Point and ask: '\n",
      " 'Incorporating\\\\npointing into visual question answering.\\\\nStephen L Morgan '\n",
      " 'and Christopher Winship. 2015.\\\\nCounterfactuals and causal inference . '\n",
      " 'Cambridge\\\\nUniversity Press.\\\\nOpenAI. 2023a. Chatgpt. '\n",
      " 'https://openai.com/\\\\nblog/chatgpt/ .\\\\nOpenAI. 2023b. Gpt-4 technical '\n",
      " 'report.\\\\nOpenAI. 2023c. Gpt-4v(ision) system card.\\\\nRoni Paiss, Ariel '\n",
      " 'Ephrat, Omer Tov, Shiran Zada, Inbar\\\\nMosseri, Michal Irani, and Tali '\n",
      " \"Dekel. 2023. Teach-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 9}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='OpenAI. 2023c. Gpt-4v(ision) system card.\\\\nRoni Paiss, Ariel \"\n",
      " 'Ephrat, Omer Tov, Shiran Zada, Inbar\\\\nMosseri, Michal Irani, and Tali '\n",
      " 'Dekel. 2023. Teach-\\\\ning clip to count to ten. ICCV .\\\\nBryan A. Plummer, '\n",
      " 'Liwei Wang, Chris M. Cervantes,\\\\nJuan C. Caicedo, Julia Hockenmaier, and '\n",
      " 'Svetlana\\\\nLazebnik. 2016. Flickr30k entities: Collecting\\\\nregion-to-phrase '\n",
      " 'correspondences for richer image-\\\\nto-sentence models.Alec Radford, Jong '\n",
      " 'Wook Kim, Chris Hallacy, Aditya\\\\nRamesh, Gabriel Goh, Sandhini Agarwal, '\n",
      " 'Girish Sas-\\\\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\\\\nGretchen '\n",
      " 'Krueger, and Ilya Sutskever. 2021. Learn-\\\\ning transferable visual models '\n",
      " 'from natural language\\\\nsupervision.\\\\nChristoph Schuhmann, Romain Beaumont, '\n",
      " 'Richard\\\\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\\\\nTheo Coombes, '\n",
      " 'Aarush Katta, Clayton Mullis,\\\\nMitchell Wortsman, Patrick Schramowski, '\n",
      " 'Srivatsa\\\\nKundurthy, Katherine Crowson, Ludwig Schmidt,\\\\nRobert '\n",
      " \"Kaczmarczyk, and Jenia Jitsev. 2022. Laion-' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 9}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Mitchell Wortsman, Patrick Schramowski, Srivatsa\\\\nKundurthy, \"\n",
      " 'Katherine Crowson, Ludwig Schmidt,\\\\nRobert Kaczmarczyk, and Jenia Jitsev. '\n",
      " '2022. Laion-\\\\n5b: An open large-scale dataset for training '\n",
      " 'next\\\\ngeneration image-text models.\\\\nHugo Touvron, Thibaut Lavril, Gautier '\n",
      " 'Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\\\nBaptiste '\n",
      " 'Rozière, Naman Goyal, Eric Hambro,\\\\nFaisal Azhar, et al. 2023. Llama: Open '\n",
      " 'and effi-\\\\ncient foundation language models. arXiv '\n",
      " 'preprint\\\\narXiv:2302.13971 .\\\\nMichal Yarom, Yonatan Bitton, Soravit '\n",
      " 'Changpinyo,\\\\nRoee Aharoni, Jonathan Herzig, Oran Lang, Eran\\\\nOfek, and '\n",
      " 'Idan Szpektor. 2023. What you see is what\\\\nyou read? improving text-image '\n",
      " 'alignment evalua-\\\\ntion. In Thirty-seventh Conference on Neural '\n",
      " 'Infor-\\\\nmation Processing Systems .\\\\nPeter Young, Alice Lai, Micah Hodosh, '\n",
      " 'and Julia Hock-\\\\nenmaier. 2014. From image descriptions to '\n",
      " 'visual\\\\ndenotations: New similarity metrics for semantic in-\\\\nference over '\n",
      " \"event descriptions.' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 9}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='enmaier. 2014. From image descriptions to \"\n",
      " 'visual\\\\ndenotations: New similarity metrics for semantic in-\\\\nference over '\n",
      " 'event descriptions.\\\\nMert Yuksekgonul, Federico Bianchi, Pratyusha '\n",
      " 'Kalluri,\\\\nDan Jurafsky, and James Zou. 2023. When and why\\\\nvision-language '\n",
      " 'models behave like bags-of-words,\\\\nand what to do about it? In '\n",
      " 'International Conference\\\\non Learning Representations .\\\\nPengchuan Zhang, '\n",
      " 'Xiujun Li, Xiaowei Hu, Jianwei\\\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, '\n",
      " 'and Jian-\\\\nfeng Gao. 2021. Vinvl: Revisiting visual represen-\\\\ntations in '\n",
      " 'vision-language models. In Proceedings\\\\nof the IEEE/CVF conference on '\n",
      " 'computer vision and\\\\npattern recognition , pages 5579–5588.\\\\nAppendix\\\\nA '\n",
      " 'Prompt guidance to GPT-4V for\\\\nFlickr30k-Attributes\\\\nAs a minor detail '\n",
      " 'specific to the dataset itself, we\\\\nextracted the objectified caption '\n",
      " 'C′\\\\nIfrom the an-\\\\nnotated Flickr30k-Entities dataset (Plummer et '\n",
      " 'al.,\\\\n2016). The objectified captions have square brack-\\\\nets added to the '\n",
      " \"original caption that allow us and' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 9}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='2016). The objectified captions have square brack-\\\\nets \"\n",
      " 'added to the original caption that allow us and\\\\nthe LMMs to refer to each '\n",
      " 'phrase for the object j\\\\nin the caption with a unique ID #j.\\\\nThe exact '\n",
      " 'prompt guidance we provided to GPT-\\\\n4V (OpenAI, 2023c) is as '\n",
      " \"follows:\\\\n10' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '9}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Categories CLIP NegCLIP + CounterCurate LLaV A-1.5 GPT-4V + \"\n",
      " 'CounterCurate\\\\nAdd Object 87.15 88.78 90.35 (+3.20) 87.83 91.59 97.82 '\n",
      " '(+9.99)\\\\nAdd Attribute 77.89 82.80 86.71 (+8.82) 80.64 91.76 95.09 '\n",
      " '(+14.45)\\\\nReplace Object 93.77 92.68 95.94 (+2.17) 96.73 96.31 98.37 '\n",
      " '(+1.64)\\\\nReplace Attribute 82.61 85.91 87.94 (+5.33) 92.64 93.53 93.53 '\n",
      " '(+0.89)\\\\nReplace Relations 68.92 76.46 76.24 (+7.32) 87.13 90.26 85.92 '\n",
      " '(-1.21)\\\\nSwap Object 60.00 75.51 68.57 (+8.57) 84.90 83.13 85.72 '\n",
      " '(+0.82)\\\\nSwap Attribute 67.42 75.23 73.57 (+6.15) 86.34 90.09 92.79 '\n",
      " '(+6.45)\\\\nAverage 81.23 84.85 86.15 (+4.92) 89.27 92.19 94.17 '\n",
      " '(+4.90)\\\\nTable 6: Detailed version of Table 3: comparison of performance '\n",
      " 'over each sub-category.\\\\nYou are given an image, the same image but\\\\nwith '\n",
      " 'bounding boxes, its corresponding caption\\\\nand an enhanced form of the '\n",
      " 'caption. Their\\\\nformat is as follows: Original Caption: A\\\\nchild in a pink '\n",
      " 'dress is helping a baby in a\\\\nblue dress climb up a set of stairs in an '\n",
      " \"en-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 10}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='format is as follows: Original Caption: A\\\\nchild in a pink \"\n",
      " 'dress is helping a baby in a\\\\nblue dress climb up a set of stairs in an '\n",
      " 'en-\\\\ntry way. Enhanced Caption: [/EN#1/people A\\\\nchild] in [/EN#2/clothing '\n",
      " 'a pink dress] helping\\\\n[/EN#3/people a baby] in [/EN#4/clothing a '\n",
      " 'blue\\\\ndress] climb up [/EN#5/other a set of stairs] in\\\\n[/EN#6/scene an '\n",
      " 'entry way]. In the enhanced cap-\\\\ntion, there is no new data, but that each '\n",
      " '“entity” is\\\\nmarked by a pair of square brackets. Most entities\\\\neach '\n",
      " 'correspond to one or more bounding boxes,\\\\nwhich will be specified. For '\n",
      " 'example, entity 1 in\\\\nthe sentence is “A child”, which is marked by a\\\\ntag '\n",
      " '[/EN#1/people . . . ]. “people” states the type\\\\nof the entity. If entity '\n",
      " 'is “other”, then there are no\\\\nrestrictions applied.\\\\nYou are tasked '\n",
      " 'to:\\\\nGenerate a caption that changes the object being\\\\ndiscussed in '\n",
      " 'exactly one of the entities. You MUST\\\\nensure that the new object is the '\n",
      " 'same type of entity\\\\nas the original object as specified in the tag. For '\n",
      " \"ex-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 10}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='ensure that the new object is the same type of entity\\\\nas \"\n",
      " 'the original object as specified in the tag. For ex-\\\\nample: [/EN#1/people '\n",
      " 'A child] => [/EN#1/people\\\\nAn adult] is allowed, but [/EN#1/people A '\n",
      " 'child]\\\\n=> [/EN#1/people A cat] is not allowed because a\\\\ncat is not '\n",
      " '“people”;\\\\nGenerate a caption that changes the qualifier\\\\n(such as an '\n",
      " 'adjective of a quantifier) that de-\\\\nscribes the object in exactly one of '\n",
      " 'the entities.\\\\nFor example: [/EN#2/clothing a pink dress] '\n",
      " '=>\\\\n[/EN#2/clothing a green dress].\\\\nGenerate, if possible, a caption that '\n",
      " 'reverses two\\\\nof the entities or their qualifiers such that the '\n",
      " 'origi-\\\\nnal sentence structure is not changed, but produces\\\\na negative '\n",
      " 'prompt. For example, given two entities\\\\n“a green dress” and “a blue '\n",
      " 'blouse”, you can either\\\\nswap the two entities’ order or swap the '\n",
      " 'adjectives\\\\nand produce “a blue dress” and “a green blouse”.\\\\nIf you '\n",
      " 'cannot generate one, report None.All in all, the new description must meet '\n",
      " \"all of\\\\nthese requirements:' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 10}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='If you cannot generate one, report None.All in all, the new \"\n",
      " 'description must meet all of\\\\nthese requirements:\\\\n1. The change of '\n",
      " 'attribute must be sufficiently\\\\ndifferent to make the new description '\n",
      " 'inaccurate,\\\\nbut it should also be somewhat related to be chal-\\\\nlenging '\n",
      " 'to an AI model.\\\\n2. Compared to the original description, the '\n",
      " 'new\\\\ndescription must differ in only one attribute. All\\\\nother details '\n",
      " 'must be kept the same.\\\\n3. The new description must mimic the '\n",
      " 'sentence\\\\nstructure of the original description.\\\\n4. The new description '\n",
      " 'must be fluent, logical,\\\\nand grammatically correct.\\\\n5. Carefully look at '\n",
      " 'the image, and give negative\\\\ncaptions that are reasonable given the '\n",
      " 'objects’ po-\\\\nsition, size, and relationship to the overall setting.\\\\n6. '\n",
      " 'Pose challenging(difficult enough) negative\\\\ncaptions so that a large '\n",
      " 'multimodal text generation\\\\nmodel should struggle to distinguish the '\n",
      " 'original\\\\ncaption v.s. negative caption.\\\\nHere are some examples whose '\n",
      " \"output for-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': \"\n",
      " '10}')\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='model should struggle to distinguish the original\\\\ncaption \"\n",
      " 'v.s. negative caption.\\\\nHere are some examples whose output for-\\\\nmat you '\n",
      " 'should follow: Original Caption: A\\\\nchild in a pink dress is helping a baby '\n",
      " 'in a\\\\nblue dress climb up a set of stairs in an en-\\\\ntry way. Enhanced '\n",
      " 'Caption: [/EN#1/people A\\\\nchild] in [/EN#2/clothing a pink blouse] '\n",
      " 'helping\\\\n[/EN#3/people a baby] in [/EN#4/clothing a blue\\\\ndress] climb up '\n",
      " '[/EN#5/other a set of stairs] in\\\\n[/EN#6/scene an entry way]. Bounding '\n",
      " 'Boxes: #1:\\\\npurple Your answer: {“noun”: {“action”: (1, “a\\\\nchild”, “an '\n",
      " 'adult”), “caption”: “An adult in a green\\\\ndress is helping a baby in a blue '\n",
      " 'dress climb up a set\\\\nof stairs in an entry way.”]}, “adjective”: '\n",
      " '{“action”:\\\\n(2, “a pink dress”, “a green dress”), “caption”: “A\\\\nchild in '\n",
      " 'a green dress is helping a baby in a blue\\\\ndress climb up a set of stairs '\n",
      " 'in an entry way.”},\\\\n“reverse”: {“action”: (2, 4), “caption”: “A child\\\\nin '\n",
      " \"a blue blouse is helping a baby in a pink dress' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 10}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='“reverse”: {“action”: (2, 4), “caption”: “A child\\\\nin a blue \"\n",
      " 'blouse is helping a baby in a pink dress\\\\nclimb up a set of stairs in an '\n",
      " \"entry way.”}}\\\\n11' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 10}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='B Prompt guidance to GPT-4V for\\\\nFlickr30k-Positions \"\n",
      " '(above-and-below)\\\\nTo generate high-quality counterfactual images for\\\\nthe '\n",
      " 'above-and-below subset of Flickr30k-Positions,\\\\nwe use the re-writing '\n",
      " 'technique to generate convinc-\\\\ning and detailed captions for GLIGEN (Li et '\n",
      " 'al.,\\\\n2023). Specifically, we leverage GPT-4V (OpenAI,\\\\n2023c) with the '\n",
      " 'following prompt:\\\\nI will give you a caption in the format \"A is\\\\nabove '\n",
      " 'B.\" You need to expand the sentence such\\\\nthat the meaning \"A is above B\" '\n",
      " 'is preserved and\\\\nyour answer is reasonable for a human to under-\\\\nstand '\n",
      " 'what you’re describing. Do not make the\\\\nanswer too long; one long sentence '\n",
      " 'is enough. For\\\\nexample, if i give you \"a man is under a dog\", a\\\\ngood '\n",
      " 'answer would be \"there is a man resting on\\\\nthe ground, and there is a dog '\n",
      " 'lying above him.\"\\\\nOne restriction: A and B do not overlap. This\\\\nmeans '\n",
      " 'that if I ask you to expand \"a hat is below\\\\nwater\", you must not assume '\n",
      " \"that the hat is below\\\\nwater. Remember that you MUST include both A' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 11}\")\n",
      "('chunk num 1 : \\n'\n",
      " ' page_content=\\'means that if I ask you to expand \"a hat is below\\\\nwater\", '\n",
      " 'you must not assume that the hat is below\\\\nwater. Remember that you MUST '\n",
      " 'include both A\\\\nand B in your answer, like my example did.\\\\nC Object '\n",
      " 'Removal vs Inpainting Plants\\\\nWe explain here as for why we do not use '\n",
      " 'object\\\\nremoval tools to curate our data for Flickr30k-\\\\nCounting as we '\n",
      " 'make one example in Figure\\\\n6. We used a tool from a public '\n",
      " 'Github\\\\nRepository https://github.com/treeebooor/\\\\nobject-remove and '\n",
      " 'compared it with GLIGEN\\\\n(Li et al., 2023)’s inpainting results. We found '\n",
      " 'a\\\\nsignificant pattern that when object removal failed,\\\\nmost of GLIGEN’s '\n",
      " 'inpainting succeeded.\\\\nD Overfitting in Above-Below Subset?\\\\nIn section '\n",
      " '4.2, we observed that both models per-\\\\nformed better on the '\n",
      " 'above-and-below subset, with\\\\nCLIP (Radford et al., 2021; Ilharco et al., '\n",
      " '2021)\\\\nperforming significantly better. However, we argue\\\\nthat this is '\n",
      " 'not caused by overfitting because there\\\\nis an even number of both \"A is '\n",
      " 'above B\" captions\\' metadata={\\'source\\': '\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 11}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='that this is not caused by overfitting because there\\\\nis an \"\n",
      " 'even number of both \"A is above B\" captions\\\\nvs. \"A is below B\" captions, '\n",
      " 'and both captions are\\\\nnot always matched to a generated image by '\n",
      " 'GLI-\\\\nGEN (Li et al., 2023) or the original image from\\\\nFlickr30k (Young '\n",
      " 'et al., 2014). Since the training\\\\nand testing datasets are shuffled and '\n",
      " 'separated from\\\\nthe same dataset, this ensures that neither model\\\\ncan '\n",
      " 'find a pattern in, for example, recognizing the\\\\ngenerated image apart from '\n",
      " 'the original image and\\\\nchoosing an option on that basis. Another proof '\n",
      " 'is\\\\n(a) Original Image\\\\n(b) Object Removal\\\\n (c) GLIGEN Inpainted '\n",
      " 'Plant\\\\nFigure 6: We want to remove/cover the person on the\\\\nright with '\n",
      " 'their skateboard in image (a) completely. Ob-\\\\nject removal only achieves '\n",
      " 'the goal partially and makes\\\\nthe image much less natural, while GLIGEN '\n",
      " 'inpainted\\\\na plant perfectly into the image as it also preserved the\\\\nrest '\n",
      " 'of the information in the image.\\\\nthat since LLaV A-1.5 (Liu et al., 2023a) '\n",
      " \"can per-' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 11}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='a plant perfectly into the image as it also preserved \"\n",
      " 'the\\\\nrest of the information in the image.\\\\nthat since LLaV A-1.5 (Liu et '\n",
      " 'al., 2023a) can per-\\\\nform equally well on both subsets after '\n",
      " 'fine-tuning,\\\\nthe two subsets are shown to be at least as well-\\\\ndefined '\n",
      " 'as the other. We also welcome others to\\\\nuse our dataset as a benchmark to '\n",
      " 'test their model’s\\\\nability to understand object positioning in images.\\\\nE '\n",
      " 'Results on the Winoground Dataset\\\\nWe evaluated our LLaV A-1.5 (Liu et al., '\n",
      " '2023a) on\\\\nWinoground (Diwan et al., 2022), a model specifi-\\\\ncally made '\n",
      " 'to test a model’s limit of visio-linguistic\\\\ncapabilities via human-annoted '\n",
      " 'difficult image-\\\\ncaption pairing questions. Our model, after '\n",
      " 'fine-\\\\ntuning on Flickr30k-Attributes, showed significant\\\\nimprovements on '\n",
      " 'the text score of Winoground.\\\\nFMore Ablations on Flickr30k-Counting\\\\nWe '\n",
      " 'also test not using any of the generated negative\\\\nimage-caption pairs and '\n",
      " \"not using grouping while\\\\nfine-tuning CLIP with Flickr30k-Counting. Eval-' \"\n",
      " \"metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 11}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='We also test not using any of the generated \"\n",
      " 'negative\\\\nimage-caption pairs and not using grouping while\\\\nfine-tuning '\n",
      " 'CLIP with Flickr30k-Counting. Eval-\\\\nuation results on PointQA-LookTwice '\n",
      " 'are shown\\\\nin Table 8. Both ablations showed much less im-\\\\nprovement, '\n",
      " 'showcasing that using grouping and the\\\\nnegative image-caption pairs is '\n",
      " \"more powerful in\\\\n12' metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', \"\n",
      " \"'page': 11}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Model Text Score\\\\nCLIP (ViT-B/32) (Radford et al., 2021) \"\n",
      " '25.25\\\\nUNITER base(Chen et al., 2020) 32.25\\\\nUNITER large(Chen et al., '\n",
      " '2020) 38.00\\\\nVinVL (Zhang et al., 2021) 37.75\\\\nBLIP2 (Zhang et al., 2021) '\n",
      " '44.00\\\\nPALI (Chen et al., 2023) 46.50\\\\nLLaV A-1.5 (Liu et al., 2023a) '\n",
      " '65.85\\\\nLLaV A-1.5 + CounterCurate 69.15 (+3.30)\\\\nTable 7: Our fine-tuned '\n",
      " 'model of LLaV A-1.5 with\\\\nFlickr30k-Attributes shows significant '\n",
      " 'improvements\\\\non the difficult visio-linguistic reasoning '\n",
      " 'dataset\\\\nWinoground.\\\\nimproving models’ counting abilities.\\\\nModel '\n",
      " 'Setting Accuracy\\\\nVanilla 57.50\\\\n+ CounterCurate (No Neg) 60.85\\\\n+ '\n",
      " 'CounterCurate (No Group) 65.70\\\\n+ CounterCurate 68.51\\\\nTable 8: More '\n",
      " 'ablations on the CLIP model trained with\\\\nFlickr30k-Counting. the score '\n",
      " 'without any negatives\\\\nand the score without grouping.\\\\nG More Results on '\n",
      " 'SugarCrepe\\\\nHere we show the performance improvements for\\\\nevery '\n",
      " 'sub-category of SugarCrepe in Table 6. Over-\\\\nall, CounterCurate shows '\n",
      " \"clear performance gain' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 12}\")\n",
      "('chunk num 1 : \\n'\n",
      " \" page_content='Here we show the performance improvements for\\\\nevery \"\n",
      " 'sub-category of SugarCrepe in Table 6. Over-\\\\nall, CounterCurate shows '\n",
      " 'clear performance gain\\\\nover the two base models, CLIP (Radford et '\n",
      " 'al.,\\\\n2021) and LLaV A (Liu et al., 2023b). We also\\\\npoint out that the '\n",
      " '“swap object” category in Sug-\\\\narCrepe (Hsieh et al., 2023) only has 246 '\n",
      " 'items as\\\\ncompared to other categories’ 500+, 1000+ items;\\\\nthis means '\n",
      " 'that the performance on this specific\\\\ncategory could show more '\n",
      " \"fluctuations caused by\\\\nthe training process.\\\\n13' metadata={'source': \"\n",
      " \"'pdfs/2402_13254/2402_13254.pdf', 'page': 12}\")\n",
      "Original source document: pdfs/2402_13254/2402_13254.pdf\n",
      "Original source document page: 8\n",
      "--------------------------------------\n",
      "Globerson. 2023. Incorporating structured represen-\n",
      "tations into pretrained vision & language models us-\n",
      "ing scene graphs. EMNLP .\n",
      "Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha\n",
      "Kembhavi, and Ranjay Krishna. 2023. Sugarcrepe:\n",
      "Fixing hackable benchmarks for vision-language\n",
      "compositionality.\n",
      "Gabriel Ilharco, Mitchell Wortsman, Ross Wightman,\n",
      "Cade Gordon, Nicholas Carlini, Rohan Taori, Achal\n",
      "Dave, Vaishaal Shankar, Hongseok Namkoong, John\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://arxiv.org/pdf/2402.13254.pdf\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 1000\n",
    "port=\"19533\"\n",
    "host=\"127.0.0.1\"\n",
    "\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "HF_EOS_TOKEN_LENGTH = 3\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0.1\n",
    "RANDOM_SEED = 415\n",
    "M=16\n",
    "\n",
    "folder_pdfs = \"pdfs\"\n",
    "folder_images = \"images\"\n",
    "folder_final_videos = \"final_videos\"\n",
    "folder_audio = \"audio_voiceovers\"\n",
    "folder_transcripts = \"transcripts\"\n",
    "\n",
    "# Call the function to create the folder\n",
    "# create_folder(folder_pdfs)\n",
    "# create_folder(folder_images)\n",
    "# create_folder(folder_audio)\n",
    "# create_folder(folder_final_videos)\n",
    "# create_folder(folder_transcripts)\n",
    "\n",
    "# Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "arxiv_id = download_and_save_pdf(url, folder_pdfs)\n",
    "arxiv_id\n",
    "\n",
    "arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "pdf_file_path = os.path.join(pdf_path, f\"{arxiv_name}.pdf\")\n",
    "image_folder = f\"{folder_images}/{arxiv_name}_pngs\" \n",
    "mp3_path = f\"{folder_audio}/output_{arxiv_name}.mp3\"\n",
    "output_path = f\"{folder_final_videos}/{arxiv_name}.mp4\" \n",
    "\n",
    "# # Download open source embedding model \"WhereIsAI/UAE-Large-V1\" via Huggingface's Sentence Transformers\n",
    "# encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "\n",
    "# Create a no-schema milvus collection and define the database index\n",
    "# milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M, my_uri)\n",
    "\n",
    "# Load PDF's into a PDF object using LangChain's PyPDFLoader\n",
    "loader = PyPDFLoader(f\"{pdf_path}/{arxiv_name}.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Cut text from PDF's into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "\n",
    "\n",
    "print(\"chunks: \\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    pprint.pprint(f\"chunk num {1} : \\n {chunk}\")\n",
    "\n",
    "# # Convert chunks to a list of dictionaries.\n",
    "# chunk_list = []\n",
    "# for chunk in chunks:\n",
    "\n",
    "# Generate embeddings using encoder from HuggingFace.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    # Assemble embedding vector, original text chunk, metadata.\n",
    "    # chunk_dict = {\n",
    "    #     'vector': converted_values,\n",
    "    #     'chunk': chunk.page_content,\n",
    "    #     'source': chunk.metadata['page']\n",
    "    # }\n",
    "    # chunk_list = embeddings.embed_query(chunk)\n",
    "\n",
    "# Creating new collection in Milvus vector store\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": host, \"port\": port},\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    drop_old=True,\n",
    ").from_documents(\n",
    "    chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_args={\"host\": host, \"port\": port},\n",
    ")\n",
    "    \n",
    "\n",
    "# Retrieving stored collection from Milvus vector store\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,    \n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_args={\"host\": host, \"port\": port}\n",
    ")\n",
    "\n",
    "query = \"What is the title of the paper?\"\n",
    "\n",
    "# Similarity search\n",
    "docs_result = vector_store.similarity_search(query)\n",
    "\n",
    "print(f\"Original source document: {docs_result[0].metadata['source']}\")\n",
    "print(f\"Original source document page: {docs_result[0].metadata['page']}\")\n",
    "print(\"--------------------------------------\")\n",
    "print(docs_result[0].page_content)\n",
    "\n",
    "# # Insert data into the Milvus collection.\n",
    "# print(\"Start inserting entities\\n\")\n",
    "\n",
    "# inserted_chunks = milvus_client.insert(\n",
    "#     COLLECTION_NAME,\n",
    "#     data=chunk_list,\n",
    "#     progress_bar=True\n",
    "# )\n",
    "# print(\"Finished inserting entities\\n\")\n",
    "# print(\"inserted_chunks\")\n",
    "# print(inserted_chunks)\n",
    "\n",
    "# # After the final entity is inserted, call flush to stop growing segments left in memory.\n",
    "# utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d41591-b1b3-45ff-9f06-de2bcab5afdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='If you cannot generate one, report None.All in all, the new description must meet all of\\nthese requirements:\\n1. The change of attribute must be sufficiently\\ndifferent to make the new description inaccurate,\\nbut it should also be somewhat related to be chal-\\nlenging to an AI model.\\n2. Compared to the original description, the new\\ndescription must differ in only one attribute. All\\nother details must be kept the same.\\n3. The new description must mimic the sentence\\nstructure of the original description.\\n4. The new description must be fluent, logical,\\nand grammatically correct.\\n5. Carefully look at the image, and give negative\\ncaptions that are reasonable given the objects’ po-\\nsition, size, and relationship to the overall setting.\\n6. Pose challenging(difficult enough) negative\\ncaptions so that a large multimodal text generation\\nmodel should struggle to distinguish the original\\ncaption v.s. negative caption.\\nHere are some examples whose output for-', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 10, 'pk': 448364140949406497}),\n",
       " Document(page_content='B Prompt guidance to GPT-4V for\\nFlickr30k-Positions (above-and-below)\\nTo generate high-quality counterfactual images for\\nthe above-and-below subset of Flickr30k-Positions,\\nwe use the re-writing technique to generate convinc-\\ning and detailed captions for GLIGEN (Li et al.,\\n2023). Specifically, we leverage GPT-4V (OpenAI,\\n2023c) with the following prompt:\\nI will give you a caption in the format \"A is\\nabove B.\" You need to expand the sentence such\\nthat the meaning \"A is above B\" is preserved and\\nyour answer is reasonable for a human to under-\\nstand what you’re describing. Do not make the\\nanswer too long; one long sentence is enough. For\\nexample, if i give you \"a man is under a dog\", a\\ngood answer would be \"there is a man resting on\\nthe ground, and there is a dog lying above him.\"\\nOne restriction: A and B do not overlap. This\\nmeans that if I ask you to expand \"a hat is below\\nwater\", you must not assume that the hat is below\\nwater. Remember that you MUST include both A', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 11, 'pk': 448364140949406500}),\n",
       " Document(page_content='2016). The objectified captions have square brack-\\nets added to the original caption that allow us and\\nthe LMMs to refer to each phrase for the object j\\nin the caption with a unique ID #j.\\nThe exact prompt guidance we provided to GPT-\\n4V (OpenAI, 2023c) is as follows:\\n10', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 9, 'pk': 448364140949406493}),\n",
       " Document(page_content='5 Conclusion\\nIn conclusion, CounterCurate significantly en-\\nhances the visio-linguistic compositional reasoning\\ncapabilities of multimodal contrastive and genera-\\ntive models. This is achieved by addressing the ne-\\nglect of physically grounded reasoning and exploit-\\ning the potential of using text and image generation\\nmodels for semantic counterfactual fine-tuning. We\\nbelieve our contributions can pave the way for fur-\\nther research in compositional reasoning.\\nAcknowledgement\\nThis work was supported in part by NSF CA-\\nREER IIS2150012, and Institute of Information\\n& communications Technology Planning & Eval-\\nuation(IITP) grants funded by the Korea govern-\\nment(MSIT) (No. 2022-0-00871, Development of\\nAI Autonomy and Knowledge Enhancement for AI\\nAgent Collaboration) and (No. RS2022-00187238,\\nDevelopment of Large Korean Language Model\\nTechnology for Efficient Pre-training), and Mi-\\ncrosoft Accelerate Foundation Models Research\\nProgram.\\nThis research is funded in part by the Univer-', metadata={'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 8, 'pk': 448364140949406482})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_QUESTION = f\"\"\"Answer in no less than 4000 characters. \n",
    "    - It is of utmost importance to use only the information from the Context to answer the user's question. Be clear, factual, complete, concise. Answer the question and follow the instructions to the best of your ability.You will be provided a research paper and your task is to summarize the research paper into a 5 minute video as follows:\n",
    "    - Create an outline the key points of the paper\n",
    "    - Clearly state in your outline why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Do not write any fact which is not present in the paper\n",
    "    \n",
    "    - Write a clearly organized and to-the-point outline summary of the following research:,\n",
    "    - The outline should have 3000 words and objectives should be clearly defined for each section of the paper while preserving the specifics address in the technology used or methods tried that have advanced the particular field.\n",
    "    - Introduce the research scientists involved and the institutions involved if known.\n",
    "    - Every single line in the outline should be in complete sentences, talk with dignity and sophistication. \n",
    "    - Use phrases such as \"Our research presents\", \"This paper details the\", do not use words such as realm, or start the sentence with \"In the\"\n",
    "    - Assume the audience is asking why and how about the reasoning and logic of the content. \n",
    "    - Use present tense and do not use past tense.\n",
    "    - Do not use phrases such as \"x has been discussed, x has been highlighted\", be as specific on the details as possible.\n",
    "    - Make sure to answer clearly what is the major contribution of this body of work.\n",
    "    - The outline should answer to the point and in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    \n",
    "    - After you have produced the outline, next convert each point in the outline to be one or more complete sentences in third person point of view, going into detail especially regarding the technicalities and key concepts of the research. Make sure that it is absolutely clear in specific detail why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead.\n",
    "    - Always start by stating the title of the paper as the first few words.\n",
    "    - First, assume the role of a research scientist who has won accolates for being able to explain expert information to a high-schooler and is giving an overview briefing of a research project.\n",
    "    - Assume the role of the editor of the best ranking tv production company in the world. \n",
    "    - Format into a script but not screenplay to be broadcasted publicly in a 5 minute production of 4000 words for higher education consumption.\n",
    "    - Introduce yourself to assume the role of a third party and do not assume the time of day, do not say good evening you are not the researcher but you represent\n",
    "    the researcher in advocating for their work. Provide the narration only, do not format as a screenplay.\n",
    "    - Spend at least 6 sentences delving deep into the research key findings and evaluation.\n",
    "    - Do not start a paragraph with \"Good day, esteemed viewers.\"\n",
    "    \n",
    "    - Lastly edit the entire script to make sure that it is obviously stated to the video viewer why was the research done, what are the technologies that were previously known involved,\n",
    "    how is the technique or actions performed advancing the field, what are the key metrics that define the success of the work \n",
    "    and what are future directions that lie ahead. Cite the grounding sources.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "docs_result = vector_store.similarity_search(SAMPLE_QUESTION)\n",
    "docs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4e52c0-964b-4a31-8127-850db234958a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'milvus_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m SAMPLE_QUESTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the title of the paper\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Search Milvus collection\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m retrieved_top_k \u001b[38;5;241m=\u001b[39m \u001b[43mmilvus_client\u001b[49m\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     18\u001b[0m     COLLECTION_NAME,\n\u001b[1;32m     19\u001b[0m     data\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mencode([SAMPLE_QUESTION]),\n\u001b[1;32m     20\u001b[0m     search_params\u001b[38;5;241m=\u001b[39msearch_params,\n\u001b[1;32m     21\u001b[0m     output_fields\u001b[38;5;241m=\u001b[39moutput_fields,\n\u001b[1;32m     22\u001b[0m     limit\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m     23\u001b[0m     consistency_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEventually\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m distances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m context \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'milvus_client' is not defined"
     ]
    }
   ],
   "source": [
    "top_k = 16384\n",
    "M = 16\n",
    "num_shot_answers = 9999999\n",
    "efConstruction = M * 2\n",
    "\n",
    "# Return top k results with HNSW index.\n",
    "search_params = {\"ef\": efConstruction}\n",
    "\n",
    "# Define output fields to return.\n",
    "output_fields = [\"source\", \"chunk\"]\n",
    "\n",
    "metadata_fields = [f for f in output_fields if f != 'chunk']\n",
    "\n",
    "SAMPLE_QUESTION = \"What is the title of the paper\"\n",
    "\n",
    "# Search Milvus collection\n",
    "retrieved_top_k = milvus_client.search(\n",
    "    COLLECTION_NAME,\n",
    "    data=encoder.encode([SAMPLE_QUESTION]),\n",
    "    search_params=search_params,\n",
    "    output_fields=output_fields,\n",
    "    limit=top_k,\n",
    "    consistency_level=\"Eventually\"\n",
    ")\n",
    "\n",
    "distances = []\n",
    "context = []\n",
    "context_metadata = []\n",
    "i = 1\n",
    "for r in retrieved_top_k[0]:\n",
    "    distances.append(r['distance'])\n",
    "    if i <= num_shot_answers:\n",
    "        if len(metadata_fields) > 0:\n",
    "            metadata = {}\n",
    "            for field in metadata_fields:\n",
    "                metadata[field] = r['entity'][field]\n",
    "            context_metadata.append(metadata)\n",
    "        context.append(r['entity']['chunk'])\n",
    "    i += 1\n",
    "\n",
    "# Assemble formatted results in a zipped list.\n",
    "formatted_results = list(zip(distances, context, context_metadata))\n",
    "# Return all the things for convenience.\n",
    "# formatted_results\n",
    "# , context, context_metadata\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a5201b9-74ad-41b0-b521-68bbd14baf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model Text Score CLIP (ViT-B/32) (Radford et al., 2021) 25.25 UNITER base(Chen et al., 2020) 32.25 UNITER large(Chen et al., 2020) 38.00 VinVL (Zhang et al., 2021) 37.75 BLIP2 (Zhang et al., 2021) 44.00 PALI (Chen et al., 2023) 46.50 LLaV A-1.5 (Liu et al., 2023a) 65.85 LLaV A-1.5 + CounterCurate 69.15 (+3.30) Table 7: Our fine-tuned model of LLaV A-1.5 with Flickr30k-Attributes shows significant improvements on the difficult visio-linguistic reasoning dataset Winoground. improving models’ counting abilities. Model Setting Accuracy Vanilla 57.50 + CounterCurate (No Neg) 60.85 + CounterCurate (No Group) 65.70 + CounterCurate 68.51 Table 8: More ablations on the CLIP model trained with Flickr30k-Counting. the score without any negatives and the score without grouping. G More Results on SugarCrepe Here we show the performance improvements for every sub-category of SugarCrepe in Table 6. Over- all, CounterCurate shows clear performance gain over the two base models, CLIP (Radford et al., 2021) and LLaV A (Liu et al., 2023b). We also point out that the “swap object” category in Sug- arCrepe (Hsieh et al., 2023) only has 246 items as compared to other categories’ 500+, 1000+ items; this means that the performance on this specific category could show more fluctuations caused by the training process. 13'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[-1].page_content.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "033b23d2-68b1-47e4-bb14-a0de9993b15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pdfs/2402_13254/2402_13254.pdf', 'page': 12}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce22d3c-34bf-49d3-a26c-0f25e94342ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.schema of <class 'langchain_core.documents.base.Document'>>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c6321b-abd9-447c-b48e-bdc9047f02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(url):\n",
    "\n",
    "    COLLECTION_NAME = \"MilvusDocs\"\n",
    "    HF_EOS_TOKEN_LENGTH = 3\n",
    "    LLM_NAME = \"gpt-3.5-turbo\"\n",
    "    TEMPERATURE = 0.1\n",
    "    RANDOM_SEED = 415\n",
    "    M=16\n",
    "    \n",
    "    folder_pdfs = \"pdfs\"\n",
    "    folder_images = \"images\"\n",
    "    folder_final_videos = \"final_videos\"\n",
    "    folder_audio = \"audio_voiceovers\"\n",
    "    folder_transcripts = \"transcripts\"\n",
    "    \n",
    "    # Call the function to create the folder\n",
    "    create_folder(folder_pdfs)\n",
    "    create_folder(folder_images)\n",
    "    create_folder(folder_audio)\n",
    "    create_folder(folder_final_videos)\n",
    "    create_folder(folder_transcripts)\n",
    "    \n",
    "    # Download and save a PDF file from an arXiv.org URL into local directory.\n",
    "    arxiv_id = download_and_save_pdf(url, folder_pdfs)\n",
    "    \n",
    "    arxiv_name = arxiv_id.replace(\".\", \"_\")\n",
    "    pdf_path = os.path.join(folder_pdfs, arxiv_name)\n",
    "    pdf_file_path = os.path.join(pdf_path, f\"{arxiv_name}.pdf\")\n",
    "    image_folder = f\"{folder_images}/{arxiv_name}_pngs\" \n",
    "    mp3_path = f\"{folder_audio}/output_{arxiv_name}.mp3\"\n",
    "    output_path = f\"{folder_final_videos}/{arxiv_name}.mp4\" \n",
    "    \n",
    "    # Download open source embedding model \"WhereIsAI/UAE-Large-V1\" via Huggingface's Sentence Transformers\n",
    "    encoder, EMBEDDING_DIM, MAX_SEQ_LENGTH = download_and_initialize_embedding_model()\n",
    "    \n",
    "    # Create a no-schema milvus collection and define the database index\n",
    "    milvus_client = create_milvus_collection(COLLECTION_NAME, EMBEDDING_DIM, M, my_uri)\n",
    "    \n",
    "    # Load PDF's into a PDF object using LangChain's PyPDFLoader\n",
    "    loader = PyPDFLoader(f\"{pdf_path}/{arxiv_name}.pdf\")\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Cut text from PDF's into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "    chunks = split_documents_to_chunks(docs, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \n",
    "    # Insert text chunks into Milvus vector database using index type HNSW Indexing and Cosine Distance\n",
    "    insert_chunks_into_milvus(chunks, COLLECTION_NAME, encoder, milvus_client, MAX_SEQ_LENGTH, HF_EOS_TOKEN_LENGTH)\n",
    "    \n",
    "    # Generate transcript using OpenAI based on the cosine distance search of the document then using gpt-3.5-turbo's chat completions\n",
    "    text_for_TTS_list = search_and_generate_response(\n",
    "        milvus_client, # Running session via docker container on port http://localhost:19531\n",
    "        encoder, # Sentence Transformer WhereIsAI/UAE-Large-V1\n",
    "        COLLECTION_NAME, # MilvusDocs by default\n",
    "        LLM_NAME,\n",
    "        TEMPERATURE,\n",
    "        RANDOM_SEED,\n",
    "        M)\n",
    "    \n",
    "    # Save LLM-generated voiceover script to directory\n",
    "    save_transcript(text_for_TTS_list, folder_transcripts, arxiv_name)\n",
    "    print(\"transcript saved\")\n",
    "    \n",
    "    # # convert text to speech with Elevenlabs\n",
    "    # audio_path = text_to_speech(text_for_TTS_list[0], arxiv_name, folder_audio)\n",
    "    \n",
    "    # # convert each pdf to a png\n",
    "    # convert_pdf_to_png(folder_images, pdf_file_path, arxiv_name)\n",
    "    \n",
    "    # # cut png's in half\n",
    "    # cut_pngs_in_half(image_folder)\n",
    "\n",
    "    # move_uncropped_files(image_folder)\n",
    "    \n",
    "    # # combine png's with audio to generate an mp4\n",
    "    # create_video(mp3_path, image_folder, output_path)\n",
    "    # milvus_client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "    # return folder_final_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b87f57-84ff-43ba-ae44-7b887466b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_list = [\"2402.13254\", \"2403.07874\", \"2403.07872\", \"2403.07870\",\"2403.07869\"]\n",
    "paper_list = [\"2402.13254\",\"2308.08079\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75ab254-dc96-47fe-82cc-5533180a82ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdfs' already exists.\n",
      "The folder 'images' already exists.\n",
      "The folder 'audio_voiceovers' already exists.\n",
      "The folder 'final_videos' already exists.\n",
      "The folder 'transcripts' already exists.\n",
      "The folder 'pdfs/2402_13254' already exists.\n",
      "\n",
      "PDF downloaded and saved as 2402_13254.pdf\n",
      "\n",
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name WhereIsAI/UAE-Large-V1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datatype of SentenceTransformer encoded object<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "\n",
      "\n",
      "What the encoder object looks like: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "\n",
      "\n",
      "model_name: WhereIsAI/UAE-Large-V1\n",
      "\n",
      "EMBEDDING_DIM: 1024\n",
      "\n",
      "MAX_SEQ_LENGTH: 512\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 1024}, 'element_type': 0}], 'aliases': [], 'collection_id': 448362022341446377, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "Start inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of chunks inserted into Milvus database: 122 with chunk id starting at number: 448362022341446549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'Answer 1: Title: \"Llama: Open and efficient foundation language models\"\\n'\n",
      " '\\n'\n",
      " 'Introduction:\\n'\n",
      " '- The research paper titled \"Llama: Open and efficient foundation language '\n",
      " 'models\" is authored by Marie-Anne Martinet, Timothée Lachaux, Baptiste '\n",
      " 'Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and others in 2023.\\n'\n",
      " '- The paper aims to introduce Llama, a new language model that is open and '\n",
      " 'efficient, contributing to the advancement of language models in the field '\n",
      " 'of natural language processing.\\n'\n",
      " '\\n'\n",
      " 'Key Contributions:\\n'\n",
      " '1. Introduction of Llama Language Model:\\n'\n",
      " '   - The paper introduces the Llama language model, which is designed to be '\n",
      " 'open and efficient, addressing the need for improved language models in the '\n",
      " 'field of natural language processing.\\n'\n",
      " '   - Llama is developed to be a foundational language model that can be '\n",
      " 'utilized for various NLP tasks, offering a new approach to language '\n",
      " 'modeling.\\n'\n",
      " '\\n'\n",
      " '2. Efficiency and Performance:\\n'\n",
      " '   - Llama is designed to be efficient in terms of computational resources '\n",
      " 'while maintaining high performance in language processing tasks.\\n'\n",
      " '   - The model aims to provide a balance between efficiency and '\n",
      " 'effectiveness, making it a valuable addition to the existing landscape of '\n",
      " 'language models.\\n'\n",
      " '\\n'\n",
      " '3. Openness and Accessibility:\\n'\n",
      " '   - Llama is presented as an open language model, emphasizing transparency '\n",
      " 'and accessibility for researchers and developers in the NLP community.\\n'\n",
      " '   - The open nature of Llama encourages collaboration, innovation, and '\n",
      " 'further advancements in the field of natural language processing.\\n'\n",
      " '\\n'\n",
      " 'Evaluation Metrics:\\n'\n",
      " '- The success of the Llama language model is evaluated based on key metrics '\n",
      " 'that demonstrate its advancements over previously known information in the '\n",
      " 'field of language models.\\n'\n",
      " '- Metrics such as model performance, efficiency in computation, and '\n",
      " 'accessibility contribute to proving the effectiveness and significance of '\n",
      " 'Llama in advancing the field of natural language processing.\\n'\n",
      " '\\n'\n",
      " 'Evaluation Findings:\\n'\n",
      " '- The evaluation of Llama language model showcases its efficiency and '\n",
      " 'performance in various NLP tasks, highlighting its ability to outperform '\n",
      " 'existing models in terms of computational resources and task accuracy.\\n'\n",
      " '- By comparing Llama to other established language models, the paper '\n",
      " 'demonstrates the superior performance and efficiency of Llama, validating '\n",
      " 'its contributions to the field of natural language processing.\\n'\n",
      " '\\n'\n",
      " 'Future Directions:\\n'\n",
      " '- The paper sets the stage for future directions in the development and '\n",
      " 'utilization of language models, suggesting potential areas for further '\n",
      " 'research and improvement.\\n'\n",
      " '- Future directions may include enhancing the capabilities of Llama through '\n",
      " 'additional training data, fine-tuning techniques, and optimization '\n",
      " 'strategies to further advance its performance and efficiency in NLP tasks.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, the research paper on Llama language model presents a '\n",
      " 'significant contribution to the field of natural language processing by '\n",
      " 'introducing an open and efficient foundational language model. Through its '\n",
      " 'innovative approach, performance metrics, and emphasis on openness, Llama '\n",
      " 'demonstrates advancements over previously known information in language '\n",
      " 'modeling, paving the way for future developments in NLP research and '\n",
      " 'applications.\\n')\n",
      "\n",
      "Transcript saved in: transcripts/2402_13254.txt\n",
      "transcript saved\n",
      "The folder 'pdfs' already exists.\n",
      "The folder 'images' already exists.\n",
      "The folder 'audio_voiceovers' already exists.\n",
      "The folder 'final_videos' already exists.\n",
      "The folder 'transcripts' already exists.\n",
      "The folder 'pdfs/2308_08079' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name WhereIsAI/UAE-Large-V1. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PDF downloaded and saved as 2308_08079.pdf\n",
      "\n",
      "device: cpu\n",
      "\n",
      "Datatype of SentenceTransformer encoded object<class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "\n",
      "\n",
      "What the encoder object looks like: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "\n",
      "\n",
      "model_name: WhereIsAI/UAE-Large-V1\n",
      "\n",
      "EMBEDDING_DIM: 1024\n",
      "\n",
      "MAX_SEQ_LENGTH: 512\n",
      "\n",
      "Collection had previously been created, dropping previous collection to initialize anew: `MilvusDocs`\n",
      "\n",
      "Successfully created collection: `MilvusDocs`\n",
      "{'collection_name': 'MilvusDocs', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': 5, 'params': {}, 'element_type': 0, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': 101, 'params': {'dim': 1024}, 'element_type': 0}], 'aliases': [], 'collection_id': 448362022341846654, 'consistency_level': 3, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "Start inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 31.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished inserting entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of chunks inserted into Milvus database: 156 with chunk id starting at number: 448362022341446672\n",
      "\n",
      "('\\n'\n",
      " 'Answer 1: Title: Advancing Lower-Dimensional Space Stabilization through '\n",
      " 'Rigid Transformations: A Novel Workflow\\n'\n",
      " '\\n'\n",
      " 'Introduction:\\n'\n",
      " '- Introduce the research scientists involved: The research conducted by a '\n",
      " 'team of scientists led by the corresponding author, whose workflow is '\n",
      " 'publicly available on GitHub.\\n'\n",
      " '- Briefly mention the institutions involved: The research was conducted '\n",
      " 'independently by the team of scientists.\\n'\n",
      " '\\n'\n",
      " 'Key Contributions:\\n'\n",
      " '1. Development of a novel workflow: \\n'\n",
      " '   - The research presents a novel workflow for stabilizing '\n",
      " 'lower-dimensional spaces through rigid transformations.\\n'\n",
      " '   - The workflow is designed to address stability issues in '\n",
      " 'lower-dimensional spaces and ensure Euclidean transformation invariance.\\n'\n",
      " '\\n'\n",
      " '2. Application of rigid transformations:\\n'\n",
      " '   - The research applies rigid transformations to multiple MDS realizations '\n",
      " 'for both batch and sequential models.\\n'\n",
      " '   - Rigid transformations are used to stabilize the lower-dimensional space '\n",
      " 'and ensure consistency in the representation of the data.\\n'\n",
      " '\\n'\n",
      " '3. Evaluation of stability:\\n'\n",
      " '   - The research evaluates the stability of the lower-dimensional space by '\n",
      " 'analyzing the movements of out-of-sample points (OOSP) within the space.\\n'\n",
      " '   - The stability is assessed by observing the movements of OOSP under '\n",
      " 'different scenarios and conditions.\\n'\n",
      " '\\n'\n",
      " '4. Uncertainty space visualization:\\n'\n",
      " '   - The research provides visualizations of uncertainty spaces to make '\n",
      " 'appropriate inferences about the data.\\n'\n",
      " '   - The visualization aids in understanding the stability and consistency '\n",
      " 'of the lower-dimensional space representation.\\n'\n",
      " '\\n'\n",
      " 'Evaluation Metrics:\\n'\n",
      " '1. Movement of out-of-sample points (OOSP):\\n'\n",
      " '   - The success of the workflow is measured by analyzing the movements of '\n",
      " 'OOSP within the lower-dimensional space.\\n'\n",
      " '   - Positive, negative, or constant movements of OOSP indicate the '\n",
      " 'stability and consistency achieved through rigid transformations.\\n'\n",
      " '\\n'\n",
      " '2. Multiplier analysis:\\n'\n",
      " '   - The research conducts a multiplier analysis to observe the effects of '\n",
      " 'varying multipliers on the movements of OOSP.\\n'\n",
      " '   - The analysis helps in understanding how different factors influence the '\n",
      " 'stability of the lower-dimensional space.\\n'\n",
      " '\\n'\n",
      " '3. Single-predictor and multi-predictor scenarios:\\n'\n",
      " '   - The stability of the lower-dimensional space is evaluated under both '\n",
      " 'single-predictor and multi-predictor scenarios.\\n'\n",
      " '   - Comparing the movements of OOSP in different scenarios provides '\n",
      " 'insights into the effectiveness of the workflow.\\n'\n",
      " '\\n'\n",
      " '4. Consistency in representations:\\n'\n",
      " '   - The consistency in the representation of data points in the '\n",
      " 'lower-dimensional space is a key metric for evaluating the success of the '\n",
      " 'workflow.\\n'\n",
      " '   - Ensuring that similar input vectors are mapped to nearby points on the '\n",
      " 'manifold demonstrates the effectiveness of the rigid transformations.\\n'\n",
      " '\\n'\n",
      " 'Future Directions:\\n'\n",
      " '- Further research can explore the application of the proposed workflow in '\n",
      " 'different domains and datasets.\\n'\n",
      " '- Investigating the scalability and efficiency of the workflow for larger '\n",
      " 'datasets and complex models.\\n'\n",
      " '- Enhancing the visualization techniques for uncertainty spaces to provide '\n",
      " 'more detailed insights into the data representation.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, the research paper \"Advancing Lower-Dimensional Space '\n",
      " 'Stabilization through Rigid Transformations: A Novel Workflow\" makes '\n",
      " 'significant contributions to the field by introducing a novel workflow for '\n",
      " 'stabilizing lower-dimensional spaces using rigid transformations. The '\n",
      " 'evaluation metrics, including the movement of out-of-sample points, '\n",
      " 'multiplier analysis, and consistency in representations, demonstrate the '\n",
      " 'effectiveness of the workflow in achieving stability and consistency in the '\n",
      " 'data representation. The future directions of the research point towards '\n",
      " 'further exploration of the workflow in different domains and datasets, as '\n",
      " 'well as enhancing visualization techniques for uncertainty spaces.\\n')\n",
      "\n",
      "Transcript saved in: transcripts/2308_08079.txt\n",
      "transcript saved\n"
     ]
    }
   ],
   "source": [
    "for paper in paper_list:\n",
    "\n",
    "    url = f\"https://arxiv.org/pdf/{paper}.pdf\"\n",
    "    process_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d92e0-24b3-4dd3-834b-cf42ef7396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_url,\n",
    "    inputs=gr.Textbox(placeholder=\"Enter arXiv PDF URL\"),\n",
    "    outputs=gr.Video(),\n",
    "    live=True,\n",
    "    theme=\"sky\",\n",
    "    flagging_options=None,  # Disable the flag button\n",
    "    title=\"Arxiv2Video\",\n",
    ")\n",
    "\n",
    "# Add a submit button\n",
    "submit_button = gr.Button()\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23e133-4b96-4ddb-ab6f-b50851a71a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18507a8-242a-4f68-b551-a7212c325155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
